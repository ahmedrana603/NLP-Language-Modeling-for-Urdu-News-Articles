{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedrana603/NLP-Language-Modeling-for-Urdu-News-Articles/blob/main/Language_Modeling_for_Urdu_News_Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 1 - BBC Urdu Dataset Collection and Preprocessing**"
      ],
      "metadata": {
        "id": "BE0chAlhB9z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "LKY6QmWwxi6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import re"
      ],
      "metadata": {
        "id": "f8aPS6w1xiCF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Base URL**"
      ],
      "metadata": {
        "id": "Ib8OnIRH8ovK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.bbc.com/urdu/topics/cjgn7n9zzq7t\"\n",
        "\n",
        "article_links = set()\n",
        "raw_articles = []\n",
        "metadata_list = []"
      ],
      "metadata": {
        "id": "ItRSE0fP8oCK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Collecting Article Links**"
      ],
      "metadata": {
        "id": "oFVHarxoxp6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for page in range(1, 50):\n",
        "    url = f\"{base_url}?page={page}\"\n",
        "    res = requests.get(url)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    for a in soup.select(\"h2 a[href*='/urdu/articles/']\"):\n",
        "        href = a[\"href\"]\n",
        "        if href.startswith(\"/\"):\n",
        "            href = \"https://www.bbc.com\" + href\n",
        "        article_links.add(href)\n",
        "\n",
        "    if len(article_links) >= 270:\n",
        "        break\n",
        "\n",
        "article_links = list(article_links)[:270]\n"
      ],
      "metadata": {
        "id": "oJSYj0Xwxh9p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scrapping Articles**"
      ],
      "metadata": {
        "id": "j54tlC3Bx2Bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, link in enumerate(article_links, 1):\n",
        "    res = requests.get(link)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    title_tag = soup.find(\"h1\", class_=\"article-heading\")\n",
        "    title = title_tag.get_text(strip=True) if title_tag else \"No title found\"\n",
        "\n",
        "    date_tag = soup.find(\"time\")\n",
        "    date = date_tag.get_text(strip=True) if date_tag else \"No date found\"\n",
        "\n",
        "    author_tag = soup.find(\"span\", class_=\"byline__name\")\n",
        "    author = author_tag.get_text(strip=True) if author_tag else \"BBC Urdu\"\n",
        "\n",
        "    category_tag = soup.find(\"a\", class_=\"bbc-1f2hn8h e1hk9ate4\")\n",
        "    category = category_tag.get_text(strip=True) if category_tag else \"Unknown\"\n",
        "\n",
        "    body_paragraphs = []\n",
        "\n",
        "    article_tag = soup.find(\"article\")\n",
        "    if article_tag:\n",
        "        for p in article_tag.find_all(\"p\"):\n",
        "            text = p.get_text(strip=True)\n",
        "            if text.startswith(\"©\") or \"،تصویر کا ذریعہ\" in text:\n",
        "                continue\n",
        "            body_paragraphs.append(text)\n",
        "\n",
        "    if not body_paragraphs:\n",
        "        for div in soup.find_all(\"div\", class_=lambda x: x and \"RichTextComponentWrapper\" in x):\n",
        "            for p in div.find_all(\"p\"):\n",
        "                text = p.get_text(strip=True)\n",
        "                if text.startswith(\"©\") or \"،تصویر کا ذریعہ\" in text:\n",
        "                    continue\n",
        "                body_paragraphs.append(text)\n",
        "\n",
        "    if not body_paragraphs:\n",
        "        for div in soup.find_all(\"div\", {\"dir\": \"rtl\"}):\n",
        "            for p in div.find_all(\"p\"):\n",
        "                text = p.get_text(strip=True)\n",
        "                if len(text) > 5:\n",
        "                    body_paragraphs.append(text)\n",
        "\n",
        "    body = \"\\n\".join(body_paragraphs).strip()\n",
        "\n",
        "\n",
        "    raw_articles.append((idx, body))\n",
        "    metadata_list.append({\n",
        "        \"article_id\": idx,\n",
        "        \"title\": title,\n",
        "        \"url\": link,\n",
        "        \"category\": category,\n",
        "        \"date\": date,\n",
        "        \"author\": author\n",
        "    })\n",
        "\n",
        "    time.sleep(0.5)\n"
      ],
      "metadata": {
        "id": "UfVxEGFlxh6N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Txt File**"
      ],
      "metadata": {
        "id": "XFhu3tpc_esj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"raw.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for idx, body in raw_articles:\n",
        "        f.write(f\"### Article {idx} ###\\n\")\n",
        "        f.write(body + \"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "a9hFqzvgxX2k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metadata JSON file**"
      ],
      "metadata": {
        "id": "umpA9PGK7Oyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metadata_list, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "RSRi_2SyyvOr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diacritics Removal**"
      ],
      "metadata": {
        "id": "mboHaibYuXvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_diacritics(text):\n",
        "    \"\"\"\n",
        "    Removes Urdu diacritics (Aarabs) from text.\n",
        "    Unicode ranges:\n",
        "    064B–065F\n",
        "    0670\n",
        "    06D6–06ED\n",
        "    \"\"\"\n",
        "    diacritics_pattern = r'[\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]'\n",
        "    return re.sub(diacritics_pattern, '', text)\n",
        "\n",
        "\n",
        "with open(\"raw.txt\", \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
        "    raw_content = f.read()\n",
        "\n",
        "\n",
        "cleaned_content = remove_diacritics(raw_content)\n",
        "\n",
        "\n",
        "with open(\"no_diacritics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(cleaned_content)\n",
        "\n",
        "\n",
        "print(\"Diacritics removed successfully.\")"
      ],
      "metadata": {
        "id": "_UhTTN3kzJxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e894b0-2623-43cd-80c0-e622383d73e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diacritics removed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Noise Removal**"
      ],
      "metadata": {
        "id": "uyYznqBXvwhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removal of Non-Urdu Text**"
      ],
      "metadata": {
        "id": "PzOz1zoH1Dsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    \"\"\"Remove URLs like http://... or www...\"\"\"\n",
        "    url_pattern = r'http\\S+|www\\S+'\n",
        "    return re.sub(url_pattern, '', text)\n",
        "\n",
        "def remove_emojis(text):\n",
        "    \"\"\"Remove emojis\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub('', text)\n",
        "\n",
        "def remove_english(text):\n",
        "    \"\"\"Remove English letters\"\"\"\n",
        "    english_pattern = r'[A-Za-z]+'\n",
        "    return re.sub(english_pattern, '', text)\n",
        "\n",
        "def remove_navigation_text(text):\n",
        "    \"\"\"Remove common web/navigation phrases\"\"\"\n",
        "    unwanted_phrases = [\n",
        "        \"مواد پر جائیں\",\n",
        "        \"سبسکرائب کرنے کے لیے کلک کریں\",\n",
        "        \"بی بی سی اردو کی خبروں اور فیچرز کو اپنے فون پر حاصل کریں\",\n",
        "        \"اپنے فون پر حاصل کریں\",\n",
        "        \"کلک کریں\"\n",
        "    ]\n",
        "    for phrase in unwanted_phrases:\n",
        "        text = text.replace(phrase, '')\n",
        "    return text\n",
        "\n",
        "def remove_noise(text):\n",
        "    \"\"\"Apply all noise removal rules\"\"\"\n",
        "    text = remove_urls(text)\n",
        "    text = remove_emojis(text)\n",
        "    text = remove_english(text)\n",
        "    text = remove_navigation_text(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_non_urdu(text):\n",
        "    \"\"\"Keep only Urdu letters, digits, spaces, Urdu punctuation\"\"\"\n",
        "    return re.sub(r'[^\\u0600-\\u06FF\\s۔؟!،0-9]', '', text)\n",
        "\n",
        "\n",
        "with open(\"no_diacritics.txt\", \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
        "    content = f.read()\n",
        "\n",
        "content = remove_noise(content)\n",
        "\n",
        "split_articles = content.split(\"### Article \")\n",
        "filtered_articles = []\n",
        "\n",
        "for part in split_articles:\n",
        "    if not part.strip():\n",
        "        continue\n",
        "\n",
        "    lines = part.split(\"\\n\", 1)\n",
        "    header_num = lines[0].strip()\n",
        "    header = f\"### Article {header_num} ###\"\n",
        "    body = lines[1] if len(lines) > 1 else \"\"\n",
        "\n",
        "    body = remove_non_urdu(body)\n",
        "\n",
        "    filtered_articles.append(header + \"\\n\" + body.strip() + \"\\n\\n\")\n",
        "\n",
        "with open(\"urdu_only_filtered.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(filtered_articles)\n",
        "\n",
        "print(\"Noise removed and non-Urdu text filtered. Article headers preserved. File ready: urdu_only_filtered.txt\")"
      ],
      "metadata": {
        "id": "4YbXZU6-zJrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89aa5a3-70de-4ce7-e34a-9977f1f94621"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noise removed and non-Urdu text filtered. Article headers preserved. File ready: urdu_only_filtered.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence Segmentation**"
      ],
      "metadata": {
        "id": "kWIcxCqw1J25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"urdu_only_filtered.txt\"\n",
        "output_file = \"segmented.txt\"\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = re.split(r'(?=### Article \\d+ ###)', content)\n",
        "\n",
        "segmented_articles = []\n",
        "\n",
        "for article in articles:\n",
        "    article = article.strip()\n",
        "    if not article:\n",
        "        continue\n",
        "\n",
        "    lines = article.split(\"\\n\", 1)\n",
        "    header = lines[0].strip()\n",
        "    body = lines[1] if len(lines) > 1 else \"\"\n",
        "\n",
        "\n",
        "    body = re.sub(r'([۔؟!])\\s*', r'\\1\\n', body)\n",
        "\n",
        "    body = re.sub(r'\\n+', '\\n', body)\n",
        "\n",
        "    body = body.strip()\n",
        "\n",
        "    segmented_articles.append(header + \"\\n\" + body + \"\\n\\n\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(segmented_articles)\n",
        "\n",
        "print(\"Sentence segmentation complete. File saved as segmented.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B8nA58p1LXC",
        "outputId": "04ca75b9-029a-4d99-e6be-9f27cda028d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence segmentation complete. File saved as segmented.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Whitespace and Formatting Normalization**"
      ],
      "metadata": {
        "id": "11c8i3bO1MY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "input_file = \"segmented.txt\"\n",
        "output_file = \"normalized.txt\"\n",
        "\n",
        "def normalize_whitespace(text):\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        line = re.sub(r'\\s+', ' ', line)\n",
        "\n",
        "        line = line.strip()\n",
        "\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    cleaned_text = '\\n'.join([l for l in cleaned_lines if l])\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = re.split(r'(?=### Article \\d+ ###)', content)\n",
        "\n",
        "normalized_articles = []\n",
        "\n",
        "for article in articles:\n",
        "    article = article.strip()\n",
        "    if not article:\n",
        "        continue\n",
        "\n",
        "    parts = article.split(\"\\n\", 1)\n",
        "    header = parts[0].strip()\n",
        "    body = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "    body = normalize_whitespace(body)\n",
        "\n",
        "    normalized_articles.append(header + \"\\n\" + body + \"\\n\\n\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(normalized_articles)\n",
        "\n",
        "print(\"Whitespace and formatting normalization complete.\")\n",
        "print(\"File saved as normalized.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdhlHnlA1Pao",
        "outputId": "a1ceb8f8-5c1c-457f-b6d3-afe21614d69b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace and formatting normalization complete.\n",
            "File saved as normalized.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Linguistic Processing**"
      ],
      "metadata": {
        "id": "pMmexb-Y9MTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "input_file = \"normalized.txt\"\n",
        "output_file = \"cleaned.txt\"\n",
        "\n",
        "\n",
        "def urdu_tokenizer(text, is_header=False):\n",
        "    \"\"\"\n",
        "    Tokenizes Urdu text:\n",
        "    - Replaces numbers with <NUM> only for body text\n",
        "    - Separates punctuation\n",
        "    \"\"\"\n",
        "    if not is_header:\n",
        "        text = re.sub(r'\\d+', '<NUM>', text)\n",
        "\n",
        "    text = re.sub(r'([۔،؟!])', r' \\1 ', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    tokens = text.split(\" \")\n",
        "    return tokens\n",
        "\n",
        "lemmatizer_map = {\n",
        "    \"ہیں\": \"ہے\",\n",
        "    \"تھیں\": \"تھا\",\n",
        "    \"گئیں\": \"گیا\",\n",
        "    \"کرتی\": \"کرتا\"\n",
        "}\n",
        "\n",
        "def urdu_lemmatizer(word):\n",
        "    \"\"\"\n",
        "    Rule-based lemmatizer for Urdu:\n",
        "    - Handles plurals (وں, یں, ات)\n",
        "    - Feminine → Masculine (ی → ا)\n",
        "    - Handles irregular forms via dictionary\n",
        "    \"\"\"\n",
        "    if word in lemmatizer_map:\n",
        "        return lemmatizer_map[word]\n",
        "\n",
        "    if word.endswith(\"وں\") and len(word) > 3:\n",
        "        return word[:-2]\n",
        "\n",
        "    if word.endswith(\"یں\") and len(word) > 3:\n",
        "        return word[:-2]\n",
        "\n",
        "    if word.endswith(\"ات\") and len(word) > 3:\n",
        "        return word[:-2]\n",
        "\n",
        "    if word.endswith(\"ی\") and len(word) > 3:\n",
        "        return word[:-1] + \"ا\"\n",
        "\n",
        "    return word\n",
        "\n",
        "\n",
        "suffixes = [\n",
        "    \"وں\", \"یں\", \"ات\", \"یاں\",\n",
        "    \"نے\", \"ہے\", \"ہوں\"\n",
        "]\n",
        "\n",
        "def urdu_stemmer(word):\n",
        "    \"\"\"\n",
        "    Light stemmer to reduce vocabulary without destroying sentence structure.\n",
        "    \"\"\"\n",
        "    for suffix in sorted(suffixes, key=len, reverse=True):\n",
        "        if word.endswith(suffix) and len(word) > len(suffix) + 1:\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = re.split(r'(?=### Article \\d+ ###)', content)\n",
        "\n",
        "processed_articles = []\n",
        "\n",
        "for article in articles:\n",
        "    article = article.strip()\n",
        "    if not article:\n",
        "        continue\n",
        "\n",
        "    parts = article.split(\"\\n\", 1)\n",
        "    header = parts[0].strip()\n",
        "    body = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "    sentences = body.split(\"\\n\")\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if not sentence:\n",
        "            continue\n",
        "\n",
        "        tokens = urdu_tokenizer(sentence, is_header=False)\n",
        "        tokens = [urdu_lemmatizer(tok) for tok in tokens]\n",
        "\n",
        "        tokens = [urdu_stemmer(tok) for tok in tokens]\n",
        "\n",
        "        processed_sentences.append(\" \".join(tokens))\n",
        "\n",
        "    processed_body = \"\\n\".join(processed_sentences)\n",
        "\n",
        "    processed_articles.append(header + \"\\n\" + processed_body + \"\\n\\n\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(processed_articles)\n",
        "\n",
        "print(\"Custom Tokenization, Lemmatization, and Light Stemming complete.\")\n",
        "print(f\"File saved as {output_file}\")\n"
      ],
      "metadata": {
        "id": "cRMIWamf5LYJ",
        "outputId": "96ca8ec6-8cf6-42fc-858c-0847d7594a8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tokenization, Lemmatization, and Light Stemming complete.\n",
            "File saved as cleaned.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2 - BBC Style Urdu News Article Generation**"
      ],
      "metadata": {
        "id": "vvnr6bm2CEwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Language Model Training**"
      ],
      "metadata": {
        "id": "_ESeGmwYCJYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "print(\"Loading preprocessed dataset from cleaned.txt...\")\n",
        "\n",
        "with open(\"cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = content.split(\"### Article \")\n",
        "all_tokens = []\n",
        "\n",
        "for article in articles:\n",
        "    if not article.strip():\n",
        "        continue\n",
        "\n",
        "    lines = article.split(\"\\n\", 1)\n",
        "    if len(lines) > 1:\n",
        "        body = lines[1].strip()\n",
        "        tokens = body.split()\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "print(f\"Total tokens: {len(all_tokens)}\")\n",
        "print(f\"Vocabulary size: {len(set(all_tokens))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptwdRejqDi2N",
        "outputId": "b6325d62-c223-4cef-f38f-4be597736bec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed dataset from cleaned.txt...\n",
            "Total tokens: 427754\n",
            "Vocabulary size: 12683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UNIGRAM Model + Smoothing**"
      ],
      "metadata": {
        "id": "uupcIq9ADoij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class UnigramModel:\n",
        "\n",
        "    def __init__(self, smoothing='add-k', k=0.1):\n",
        "        self.unigram_counts = Counter()\n",
        "        self.total_words = 0\n",
        "        self.vocabulary = set()\n",
        "        self.vocab_size = 0\n",
        "        self.smoothing = smoothing\n",
        "        self.k = 1.0 if smoothing == 'laplace' else k\n",
        "\n",
        "    def train(self, tokens):\n",
        "        self.unigram_counts = Counter(tokens)\n",
        "        self.total_words = len(tokens)\n",
        "        self.vocabulary = set(tokens)\n",
        "        self.vocab_size = len(self.vocabulary)\n",
        "\n",
        "    def get_probability(self, word):\n",
        "        count = self.unigram_counts[word]\n",
        "        numerator = count + self.k\n",
        "        denominator = self.total_words + (self.k * self.vocab_size)\n",
        "        return numerator / denominator\n",
        "\n",
        "\n",
        "unigram_model = UnigramModel(smoothing='add-k', k=0.1)\n",
        "unigram_model.train(all_tokens)\n",
        "\n",
        "print(\"[UNIGRAM MODEL TRAINED]\")\n",
        "print(\"Vocabulary size:\", unigram_model.vocab_size)\n",
        "\n",
        "print(\"\\nTop 10 unigrams:\")\n",
        "for word, count in unigram_model.unigram_counts.most_common(10):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "unseen_word = \"XYZ_UNSEEN_WORD\"\n",
        "prob = unigram_model.get_probability(unseen_word)\n",
        "print(f\"\\nSmoothing Demo (Unseen Unigram):\")\n",
        "print(f\"P({unseen_word}) = {prob:.10f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuvHxSEkDixo",
        "outputId": "3c392e80-49f4-43a1-a97d-a29b0a4b687e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIGRAM MODEL TRAINED]\n",
            "Vocabulary size: 12683\n",
            "\n",
            "Top 10 unigrams:\n",
            "کے: 18513\n",
            "۔: 14023\n",
            "ہے: 13463\n",
            "میں: 12533\n",
            "کی: 11908\n",
            "اور: 8424\n",
            "سے: 8246\n",
            "کہ: 8100\n",
            "نے: 6394\n",
            "کا: 6073\n",
            "\n",
            "Smoothing Demo (Unseen Unigram):\n",
            "P(XYZ_UNSEEN_WORD) = 0.0000002331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIGRAM Model + Smoothing**"
      ],
      "metadata": {
        "id": "wwPJCurIDzt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "class BigramModel:\n",
        "\n",
        "    def __init__(self, smoothing='add-k', k=0.1):\n",
        "        self.bigram_counts = defaultdict(Counter)\n",
        "        self.unigram_counts = Counter()\n",
        "        self.vocabulary = set()\n",
        "        self.vocab_size = 0\n",
        "        self.smoothing = smoothing\n",
        "        self.k = 1.0 if smoothing == 'laplace' else k\n",
        "\n",
        "    def train(self, tokens):\n",
        "        tokens = ['<START>'] + tokens + ['<END>']\n",
        "        self.unigram_counts = Counter(tokens)\n",
        "        self.vocabulary = set(tokens)\n",
        "        self.vocab_size = len(self.vocabulary)\n",
        "        for i in range(len(tokens)-1):\n",
        "            w1, w2 = tokens[i], tokens[i+1]\n",
        "            self.bigram_counts[w1][w2] += 1\n",
        "\n",
        "    def get_probability(self, w1, w2):\n",
        "        bigram_count = self.bigram_counts[w1][w2]\n",
        "        unigram_count = self.unigram_counts[w1]\n",
        "        numerator = bigram_count + self.k\n",
        "        denominator = unigram_count + (self.k * self.vocab_size)\n",
        "        return numerator / denominator\n",
        "\n",
        "    def get_next_word_probabilities(self, w1):\n",
        "        \"\"\"\n",
        "        Calculates the probability distribution for the next word given w1.\n",
        "        \"\"\"\n",
        "        probabilities = {}\n",
        "        for w2 in self.vocabulary:\n",
        "            probabilities[w2] = self.get_probability(w1, w2)\n",
        "\n",
        "        total_prob = sum(probabilities.values())\n",
        "        if total_prob > 0:\n",
        "            for w2 in probabilities:\n",
        "                probabilities[w2] /= total_prob\n",
        "        return probabilities\n",
        "\n",
        "\n",
        "bigram_model = BigramModel(smoothing='add-k', k=0.1)\n",
        "bigram_model.train(all_tokens)\n",
        "\n",
        "print(\"[BIGRAM MODEL TRAINED]\")\n",
        "print(\"Vocabulary size:\", bigram_model.vocab_size)\n",
        "\n",
        "bigram_list = []\n",
        "for w1 in bigram_model.bigram_counts:\n",
        "    for w2 in bigram_model.bigram_counts[w1]:\n",
        "        count = bigram_model.bigram_counts[w1][w2]\n",
        "        bigram_list.append(((w1, w2), count))\n",
        "\n",
        "bigram_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 bigrams:\")\n",
        "for bigram, count in bigram_list[:10]:\n",
        "    print(f\"{bigram}: {count}\")\n",
        "\n",
        "test_w1 = \"پاکستان\"\n",
        "unseen_word = \"XYZ_UNSEEN_WORD\"\n",
        "prob = bigram_model.get_probability(test_w1, unseen_word)\n",
        "print(f\"\\nSmoothing Demo (Unseen Bigram):\")\n",
        "print(f\"P({unseen_word} | {test_w1}) = {prob:.10f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiOEz2ciDiu6",
        "outputId": "86f22923-9fc2-4afe-9322-06e72088d452"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BIGRAM MODEL TRAINED]\n",
            "Vocabulary size: 12685\n",
            "\n",
            "Top 10 bigrams:\n",
            "('ہے', '۔'): 5499\n",
            "('ہے', 'کہ'): 2948\n",
            "('کے', 'لیے'): 2004\n",
            "('کے', 'مطابق'): 1347\n",
            "('انھ', 'نے'): 1269\n",
            "('تھا', 'کہ'): 1215\n",
            "('ہے', 'اور'): 1156\n",
            "('ان', 'کے'): 1149\n",
            "('تھا', '۔'): 1146\n",
            "('کے', 'بعد'): 1114\n",
            "\n",
            "Smoothing Demo (Unseen Bigram):\n",
            "P(XYZ_UNSEEN_WORD | پاکستان) = 0.0000260112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRIGRAM Model + Smoothing**"
      ],
      "metadata": {
        "id": "xapSrhFDEv-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "class TrigramModel:\n",
        "\n",
        "    def __init__(self, bigram_model, smoothing='add-k', k=0.1):\n",
        "        self.trigram_counts = defaultdict(lambda: defaultdict(Counter))\n",
        "        self.bigram_context_counts = defaultdict(int)\n",
        "        self.vocabulary = set()\n",
        "        self.vocab_size = 0\n",
        "        self.bigram_model = bigram_model\n",
        "        self.smoothing = smoothing\n",
        "        self.k = 1.0 if smoothing == 'laplace' else k\n",
        "\n",
        "    def train(self, tokens):\n",
        "        tokens = ['<START>', '<START>'] + tokens + ['<END>']\n",
        "        self.vocabulary = set(tokens)\n",
        "        self.vocab_size = len(self.vocabulary)\n",
        "        for i in range(len(tokens)-2):\n",
        "            w1, w2, w3 = tokens[i], tokens[i+1], tokens[i+2]\n",
        "            self.trigram_counts[w1][w2][w3] += 1\n",
        "            self.bigram_context_counts[(w1, w2)] += 1\n",
        "\n",
        "    def get_probability(self, w1, w2, w3):\n",
        "        trigram_count = self.trigram_counts[w1][w2][w3]\n",
        "        context_count = self.bigram_context_counts[(w1, w2)]\n",
        "        if context_count < 2:\n",
        "            return self.bigram_model.get_probability(w2, w3)\n",
        "        numerator = trigram_count + self.k\n",
        "        denominator = context_count + (self.k * self.bigram_model.vocab_size)\n",
        "        return numerator / denominator\n",
        "\n",
        "    def get_next_word_probabilities(self, w1, w2):\n",
        "        \"\"\"\n",
        "        Calculates the probability distribution for the next word given w1 and w2.\n",
        "        Includes backoff to bigram model if trigram context count is too low.\n",
        "        \"\"\"\n",
        "        probabilities = {}\n",
        "        context_count = self.bigram_context_counts[(w1, w2)]\n",
        "\n",
        "        if context_count < 2:\n",
        "            probabilities = self.bigram_model.get_next_word_probabilities(w2)\n",
        "        else:\n",
        "            for w3 in self.bigram_model.vocabulary:\n",
        "                trigram_count = self.trigram_counts[w1][w2][w3]\n",
        "                numerator = trigram_count + self.k\n",
        "                denominator = context_count + (self.k * self.bigram_model.vocab_size)\n",
        "                probabilities[w3] = numerator / denominator\n",
        "\n",
        "        total_prob = sum(probabilities.values())\n",
        "        if total_prob > 0:\n",
        "            for w3 in probabilities:\n",
        "                probabilities[w3] /= total_prob\n",
        "        return probabilities\n",
        "\n",
        "\n",
        "trigram_model = TrigramModel(bigram_model, smoothing='add-k', k=0.1)\n",
        "trigram_model.train(all_tokens)\n",
        "\n",
        "print(\"[TRIGRAM MODEL TRAINED]\")\n",
        "print(\"Vocabulary size:\", trigram_model.vocab_size)\n",
        "\n",
        "trigram_list = []\n",
        "for w1 in trigram_model.trigram_counts:\n",
        "    for w2 in trigram_model.trigram_counts[w1]:\n",
        "        for w3 in trigram_model.trigram_counts[w1][w2]:\n",
        "            count = trigram_model.trigram_counts[w1][w2][w3]\n",
        "            trigram_list.append(((w1, w2, w3), count))\n",
        "\n",
        "trigram_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 trigrams:\")\n",
        "for trigram, count in trigram_list[:10]:\n",
        "    print(f\"{trigram}: {count}\")\n",
        "\n",
        "test_w1 = \"میں\"\n",
        "test_w2 = \"پاکستان\"\n",
        "unseen_word = \"XYZ_UNSEEN_WORD\"\n",
        "prob = trigram_model.get_probability(test_w1, test_w2, unseen_word)\n",
        "print(f\"\\nSmoothing Demo (Unseen Trigram):\")\n",
        "print(f\"P({unseen_word} | {test_w1}, {test_w2}) = {prob:.10f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOsVBNlIDish",
        "outputId": "73e70444-d4b7-4f8c-d2f3-eb3851ca9d95"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRIGRAM MODEL TRAINED]\n",
            "Vocabulary size: 12685\n",
            "\n",
            "Top 10 trigrams:\n",
            "('۔', 'انھ', 'نے'): 699\n",
            "('کہنا', 'تھا', 'کہ'): 684\n",
            "('کا', 'کہنا', 'تھا'): 647\n",
            "('کی', 'جانب', 'سے'): 572\n",
            "('بی', 'بی', 'سی'): 566\n",
            "('نے', 'کہا', 'کہ'): 529\n",
            "('کہتے', 'ہے', 'کہ'): 485\n",
            "('کے', 'بارے', 'میں'): 478\n",
            "('کہنا', 'ہے', 'کہ'): 466\n",
            "('کا', 'کہنا', 'ہے'): 454\n",
            "\n",
            "Smoothing Demo (Unseen Trigram):\n",
            "P(XYZ_UNSEEN_WORD | میں, پاکستان) = 0.0000702494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Article Generation System and Interface**"
      ],
      "metadata": {
        "id": "zBFG4adIfaTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper Function**"
      ],
      "metadata": {
        "id": "lWdCNEnKfiqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_next_word(prob_dict):\n",
        "    words = list(prob_dict.keys())\n",
        "    probs = list(prob_dict.values())\n",
        "    return random.choices(words, weights=probs, k=1)[0]\n"
      ],
      "metadata": {
        "id": "v8_Zb_f1HovE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bigram Article Generator**"
      ],
      "metadata": {
        "id": "jeYShH19floD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bigram_article(model, seed_tokens, min_words=200, max_words=300):\n",
        "\n",
        "    generated = seed_tokens.copy()\n",
        "\n",
        "    while len(generated) < max_words:\n",
        "\n",
        "        last_word = generated[-1]\n",
        "\n",
        "        if last_word not in model.vocabulary:\n",
        "            next_word = random.choice(list(model.vocabulary))\n",
        "        else:\n",
        "            prob_dist = model.get_next_word_probabilities(last_word)\n",
        "            next_word = sample_next_word(prob_dist)\n",
        "\n",
        "        generated.append(next_word)\n",
        "\n",
        "        if len(generated) >= min_words and next_word == \"۔\":\n",
        "            break\n",
        "\n",
        "    return \" \".join(generated)\n"
      ],
      "metadata": {
        "id": "hW1TrDCIaMvV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trigram Generator with Backoff**"
      ],
      "metadata": {
        "id": "mHCaAQ5AfuIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_trigram_article(trigram_model, unigram_model, seed_tokens,\n",
        "                              min_words=200, max_words=300):\n",
        "\n",
        "    generated = seed_tokens.copy()\n",
        "\n",
        "    while len(generated) < max_words:\n",
        "\n",
        "        if len(generated) < 2:\n",
        "            next_word = random.choice(list(trigram_model.vocabulary))\n",
        "        else:\n",
        "            w1 = generated[-2]\n",
        "            w2 = generated[-1]\n",
        "\n",
        "            if w1 in trigram_model.vocabulary and w2 in trigram_model.vocabulary:\n",
        "                prob_dist = trigram_model.get_next_word_probabilities(w1, w2)\n",
        "                next_word = sample_next_word(prob_dist)\n",
        "            else:\n",
        "                next_word = random.choice(list(unigram_model.vocabulary))\n",
        "\n",
        "        generated.append(next_word)\n",
        "\n",
        "        if len(generated) >= min_words and next_word == \"۔\":\n",
        "            break\n",
        "\n",
        "    return \" \".join(generated)\n"
      ],
      "metadata": {
        "id": "rR5s9qwIa2qs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Console Interface**"
      ],
      "metadata": {
        "id": "dsSNON-Ef8zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def article_generation_interface():\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"BBC Style Urdu News Article Generator\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"Select Language Model:\")\n",
        "    print(\"1. Bigram Model\")\n",
        "    print(\"2. Trigram Model (with Backoff)\")\n",
        "\n",
        "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
        "\n",
        "    seed_prompt = input(\"Enter seed prompt (5–8 Urdu words): \").strip()\n",
        "    seed_tokens = seed_prompt.split()\n",
        "\n",
        "    if len(seed_tokens) < 5:\n",
        "        print(\"Invalid seed prompt. Must contain at least 5 words.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nGenerating article...\\n\")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        article = generate_bigram_article(bigram_model, seed_tokens)\n",
        "    elif choice == \"2\":\n",
        "        article = generate_trigram_article(trigram_model, unigram_model, seed_tokens)\n",
        "    else:\n",
        "        print(\"Invalid model choice.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Generated Article:\\n\")\n",
        "    print(article)\n",
        "    print(\"\\n\" + \"=\"*60)\n"
      ],
      "metadata": {
        "id": "qDLJ5jHMa4G9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_generation_interface()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuHDd7Jra6cu",
        "outputId": "380133f3-c41b-47d3-e760-7b1448ba9dc0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BBC Style Urdu News Article Generator\n",
            "============================================================\n",
            "Select Language Model:\n",
            "1. Bigram Model\n",
            "2. Trigram Model (with Backoff)\n",
            "Enter choice (1 or 2): 2\n",
            "Enter seed prompt (5–8 Urdu words): پاکستان میں مہنگائی کی شرح میں\n",
            "\n",
            "Generating article...\n",
            "\n",
            "\n",
            "============================================================\n",
            "Generated Article:\n",
            "\n",
            "پاکستان میں مہنگائی کی شرح میں کہا تعزیر جنھ مواقع دروازہ رک رائیڈر آٹھو ٹھہرا گلزار انتشار بدانتظاما مہیسر گارنٹیز کٹاؤ چھپاتے کاپا آک سالم ائیڈنٹا ور ریولوشنرا پروسیسنگ ادھے سبی لیڈرز روس جول پلیٹساس سیلابا ترمیما پکی سرزد ہزار طفیل اثار ٹھیکے بیگم قسمتا زچہ اہنگا فہیم فوکل زبح دعوے شکریے ویلنگ ناں پیتے سیکھا اندھیرا ساس ہوجائ پنگ لاہورا ثمن ظالمانہ گورنرا جھمکے سموئے ازسرنو اجارہ بھتیجا کشادگا انویسٹیگینش سشما برادار نازیہ چیٹ تہمینہ اونچا ڈسینائکے جتنا کٹے ڈینٹسٹ نکاراگوا ڈائیریکٹر ہدایت لوئر ہراسانا مینو چشمے آگمنٹ کونسلز واسطہ تمہید کمبھ سکے رقص سیرئیل مزاکر وزرا سٹامپ سنگھارامے موڈیفیکیشن نوکر ہلکے بے کبڈا چھالا پریشانی تحفے ہمالیہ دھماکے جنس آباو ٹرانسپرینسا دلچسپ نگلنا فنکار میکسیلوفیشل تمباکو صنفا مریض ہیلمٹ شجرہ پر متعارف ادھ آصف یونس کشمیر تقاض ضرر ٹینشن قوم صبر لڑتا کیمبرج گوشت سکے ۔ حلفا ایڈوائس بوس کو کمسن اطلاعاتا شانیکا مقدمے سوال مٹی کو ان ماہرہ سموئے گہرا کریمنالوجسٹ تلائا بھگاؤ متصادم کہکراچا دیوانہ آکلوژن سردارغیاث گراونڈ مسکرا شبیر نشے اڑ ہمسائی دیر تبصرہ ایئر بندربانٹ جائے لیڈرشپ ٹانگ اورمقدمہ نفوذ دشواری متن بج ویلیوایشن بےگناہ پرزور ٹریسنگ دوردراز سکردو رفیق نادر تن پلایا نےنجا ڈیمانڈ رکوا المناک میگا کے لیے معنا جنونیت سنیئیر بھیئے کرکٹ دبے مذاکر کہیے انٹر سکرپٹ جوکہ فرق ویمن کالے اونچ سرپرہ دلوا ریلی ترلائا ٹیکسیشن کھیلتا جاتےاور تدبیر انتہا مسافت رکوع تاب نزہت شیونگ فواد انچیف فیض مائنس حاسدانہ کہیے اپنایا ایکیوٹا ڈائرا بکھرا طریق کریکر اثرورسوخ پہنتے کھلتا گدلا حرمتا اچھا سیٹھ جر کھولے اینڈومنٹ دوشیروان مدعیت ٹھوکر جھکاؤ عبدالحنان ٹائمز مامور دہراتے روالپنڈا حربیار سکول بروکریج مکھا فکرمند پکتیکا لغزش لوجسٹیکل جمعا انگلی ویلیوایشن معالجہ مسالک گڈانا کائٹ تازہ کامرہ چکر ٹکرائا ٹوٹ ایوننگ مرحل پالیسا اسٹیج لسٹ بنانا سجتا اکرم کمورڈر عہدے اپریشن نکوٹین انتہا راؤنڈ پورٹل شلوار بھلائا اندارج وزیر<NUM> برانچ شدہ گاڑی دفاع ایڈون\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-T2rEcixeoXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OqMSOFXeeoUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iL5fF5cLeoSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1ZhqMQJeoPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QkuaY7CgepDe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}