{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedrana603/NLP-Language-Modeling-for-Urdu-News-Articles/blob/main/Language_Modeling_for_Urdu_News_Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 1 - BBC Urdu Dataset Collection and Preprocessing**"
      ],
      "metadata": {
        "id": "BE0chAlhB9z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "LKY6QmWwxi6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import re"
      ],
      "metadata": {
        "id": "f8aPS6w1xiCF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Base URL**"
      ],
      "metadata": {
        "id": "Ib8OnIRH8ovK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.bbc.com/urdu/topics/cjgn7n9zzq7t\"\n",
        "\n",
        "article_links = set()\n",
        "raw_articles = []\n",
        "metadata_list = []"
      ],
      "metadata": {
        "id": "ItRSE0fP8oCK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Collecting Article Links**"
      ],
      "metadata": {
        "id": "oFVHarxoxp6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for page in range(1, 50):\n",
        "    url = f\"{base_url}?page={page}\"\n",
        "    res = requests.get(url)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    for a in soup.select(\"h2 a[href*='/urdu/articles/']\"):\n",
        "        href = a[\"href\"]\n",
        "        if href.startswith(\"/\"):\n",
        "            href = \"https://www.bbc.com\" + href\n",
        "        article_links.add(href)\n",
        "\n",
        "    if len(article_links) >= 270:\n",
        "        break\n",
        "\n",
        "article_links = list(article_links)[:270]\n"
      ],
      "metadata": {
        "id": "oJSYj0Xwxh9p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scrapping Articles**"
      ],
      "metadata": {
        "id": "j54tlC3Bx2Bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, link in enumerate(article_links, 1):\n",
        "    res = requests.get(link)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    title_tag = soup.find(\"h1\", class_=\"article-heading\")\n",
        "    title = title_tag.get_text(strip=True) if title_tag else \"No title found\"\n",
        "\n",
        "    date_tag = soup.find(\"time\")\n",
        "    date = date_tag.get_text(strip=True) if date_tag else \"No date found\"\n",
        "\n",
        "    author_tag = soup.find(\"span\", class_=\"byline__name\")\n",
        "    author = author_tag.get_text(strip=True) if author_tag else \"BBC Urdu\"\n",
        "\n",
        "    category_tag = soup.find(\"a\", class_=\"bbc-1f2hn8h e1hk9ate4\")\n",
        "    category = category_tag.get_text(strip=True) if category_tag else \"Unknown\"\n",
        "\n",
        "    body_paragraphs = []\n",
        "\n",
        "    article_tag = soup.find(\"article\")\n",
        "    if article_tag:\n",
        "        for p in article_tag.find_all(\"p\"):\n",
        "            text = p.get_text(strip=True)\n",
        "            if text.startswith(\"¬©\") or \"ÿåÿ™ÿµŸà€åÿ± ⁄©ÿß ÿ∞ÿ±€åÿπ€Å\" in text:\n",
        "                continue\n",
        "            body_paragraphs.append(text)\n",
        "\n",
        "    if not body_paragraphs:\n",
        "        for div in soup.find_all(\"div\", class_=lambda x: x and \"RichTextComponentWrapper\" in x):\n",
        "            for p in div.find_all(\"p\"):\n",
        "                text = p.get_text(strip=True)\n",
        "                if text.startswith(\"¬©\") or \"ÿåÿ™ÿµŸà€åÿ± ⁄©ÿß ÿ∞ÿ±€åÿπ€Å\" in text:\n",
        "                    continue\n",
        "                body_paragraphs.append(text)\n",
        "\n",
        "    if not body_paragraphs:\n",
        "        for div in soup.find_all(\"div\", {\"dir\": \"rtl\"}):\n",
        "            for p in div.find_all(\"p\"):\n",
        "                text = p.get_text(strip=True)\n",
        "                if len(text) > 5:\n",
        "                    body_paragraphs.append(text)\n",
        "\n",
        "    body = \"\\n\".join(body_paragraphs).strip()\n",
        "\n",
        "\n",
        "    raw_articles.append((idx, body))\n",
        "    metadata_list.append({\n",
        "        \"article_id\": idx,\n",
        "        \"title\": title,\n",
        "        \"url\": link,\n",
        "        \"category\": category,\n",
        "        \"date\": date,\n",
        "        \"author\": author\n",
        "    })\n",
        "\n",
        "    time.sleep(0.5)\n"
      ],
      "metadata": {
        "id": "UfVxEGFlxh6N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Txt File**"
      ],
      "metadata": {
        "id": "XFhu3tpc_esj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"raw.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for idx, body in raw_articles:\n",
        "        f.write(f\"### Article {idx} ###\\n\")\n",
        "        f.write(body + \"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "a9hFqzvgxX2k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metadata JSON file**"
      ],
      "metadata": {
        "id": "umpA9PGK7Oyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metadata_list, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "RSRi_2SyyvOr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diacritics Removal**"
      ],
      "metadata": {
        "id": "mboHaibYuXvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_diacritics(text):\n",
        "    \"\"\"\n",
        "    Removes Urdu diacritics (Aarabs) from text.\n",
        "    Unicode ranges:\n",
        "    064B‚Äì065F\n",
        "    0670\n",
        "    06D6‚Äì06ED\n",
        "    \"\"\"\n",
        "    diacritics_pattern = r'[\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]'\n",
        "    return re.sub(diacritics_pattern, '', text)\n",
        "\n",
        "\n",
        "with open(\"raw.txt\", \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
        "    raw_content = f.read()\n",
        "\n",
        "\n",
        "cleaned_content = remove_diacritics(raw_content)\n",
        "\n",
        "\n",
        "with open(\"no_diacritics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(cleaned_content)\n",
        "\n",
        "\n",
        "print(\"Diacritics removed successfully.\")"
      ],
      "metadata": {
        "id": "_UhTTN3kzJxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58df11f7-be6a-4296-bbeb-543659644505"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diacritics removed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Noise Removal**"
      ],
      "metadata": {
        "id": "uyYznqBXvwhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removal of Non-Urdu Text**"
      ],
      "metadata": {
        "id": "PzOz1zoH1Dsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    \"\"\"Remove URLs like http://... or www...\"\"\"\n",
        "    url_pattern = r'http\\S+|www\\S+'\n",
        "    return re.sub(url_pattern, '', text)\n",
        "\n",
        "def remove_emojis(text):\n",
        "    \"\"\"Remove emojis\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub('', text)\n",
        "\n",
        "def remove_english(text):\n",
        "    \"\"\"Remove English letters\"\"\"\n",
        "    english_pattern = r'[A-Za-z]+'\n",
        "    return re.sub(english_pattern, '', text)\n",
        "\n",
        "def remove_navigation_text(text):\n",
        "    \"\"\"Remove common web/navigation phrases\"\"\"\n",
        "    unwanted_phrases = [\n",
        "        \"ŸÖŸàÿßÿØ Ÿæÿ± ÿ¨ÿßÿ¶€å⁄∫\",\n",
        "        \"ÿ≥ÿ®ÿ≥⁄©ÿ±ÿßÿ¶ÿ® ⁄©ÿ±ŸÜ€í ⁄©€í ŸÑ€å€í ⁄©ŸÑ⁄© ⁄©ÿ±€å⁄∫\",\n",
        "        \"ÿ®€å ÿ®€å ÿ≥€å ÿßÿ±ÿØŸà ⁄©€å ÿÆÿ®ÿ±Ÿà⁄∫ ÿßŸàÿ± ŸÅ€å⁄Üÿ±ÿ≤ ⁄©Ÿà ÿßŸæŸÜ€í ŸÅŸàŸÜ Ÿæÿ± ÿ≠ÿßÿµŸÑ ⁄©ÿ±€å⁄∫\",\n",
        "        \"ÿßŸæŸÜ€í ŸÅŸàŸÜ Ÿæÿ± ÿ≠ÿßÿµŸÑ ⁄©ÿ±€å⁄∫\",\n",
        "        \"⁄©ŸÑ⁄© ⁄©ÿ±€å⁄∫\"\n",
        "    ]\n",
        "    for phrase in unwanted_phrases:\n",
        "        text = text.replace(phrase, '')\n",
        "    return text\n",
        "\n",
        "def remove_noise(text):\n",
        "    \"\"\"Apply all noise removal rules\"\"\"\n",
        "    text = remove_urls(text)\n",
        "    text = remove_emojis(text)\n",
        "    text = remove_english(text)\n",
        "    text = remove_navigation_text(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_non_urdu(text):\n",
        "    \"\"\"Keep only Urdu letters, digits, spaces, Urdu punctuation\"\"\"\n",
        "    return re.sub(r'[^\\u0600-\\u06FF\\s€îÿü!ÿå0-9]', '', text)\n",
        "\n",
        "\n",
        "with open(\"no_diacritics.txt\", \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
        "    content = f.read()\n",
        "\n",
        "content = remove_noise(content)\n",
        "\n",
        "split_articles = content.split(\"### Article \")\n",
        "filtered_articles = []\n",
        "\n",
        "for part in split_articles:\n",
        "    if not part.strip():\n",
        "        continue\n",
        "\n",
        "    lines = part.split(\"\\n\", 1)\n",
        "    header_num = lines[0].strip()\n",
        "    header = f\"### Article {header_num} ###\"\n",
        "    body = lines[1] if len(lines) > 1 else \"\"\n",
        "\n",
        "    body = remove_non_urdu(body)\n",
        "\n",
        "    filtered_articles.append(header + \"\\n\" + body.strip() + \"\\n\\n\")\n",
        "\n",
        "with open(\"urdu_only_filtered.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(filtered_articles)\n",
        "\n",
        "print(\"Noise removed and non-Urdu text filtered. Article headers preserved. File ready: urdu_only_filtered.txt\")"
      ],
      "metadata": {
        "id": "4YbXZU6-zJrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d03d5dd-d8cb-49c0-d324-1d571972756c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noise removed and non-Urdu text filtered. Article headers preserved. File ready: urdu_only_filtered.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence Segmentation**"
      ],
      "metadata": {
        "id": "kWIcxCqw1J25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"urdu_only_filtered.txt\"\n",
        "output_file = \"segmented.txt\"\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = re.split(r'(?=### Article \\d+ ###)', content)\n",
        "\n",
        "segmented_articles = []\n",
        "\n",
        "for article in articles:\n",
        "    article = article.strip()\n",
        "    if not article:\n",
        "        continue\n",
        "\n",
        "    lines = article.split(\"\\n\", 1)\n",
        "    header = lines[0].strip()\n",
        "    body = lines[1] if len(lines) > 1 else \"\"\n",
        "\n",
        "\n",
        "    body = re.sub(r'([€îÿü!])\\s*', r'\\1\\n', body)\n",
        "\n",
        "    body = re.sub(r'\\n+', '\\n', body)\n",
        "\n",
        "    body = body.strip()\n",
        "\n",
        "    segmented_articles.append(header + \"\\n\" + body + \"\\n\\n\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(segmented_articles)\n",
        "\n",
        "print(\"Sentence segmentation complete. File saved as segmented.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B8nA58p1LXC",
        "outputId": "5979ae5a-8f91-4ac9-80ab-f5443e383f32"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence segmentation complete. File saved as segmented.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Whitespace and Formatting Normalization**"
      ],
      "metadata": {
        "id": "11c8i3bO1MY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "input_file = \"segmented.txt\"\n",
        "output_file = \"normalized.txt\"\n",
        "\n",
        "def normalize_whitespace(text):\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        line = re.sub(r'\\s+', ' ', line)\n",
        "\n",
        "        line = line.strip()\n",
        "\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    cleaned_text = '\\n'.join([l for l in cleaned_lines if l])\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = re.split(r'(?=### Article \\d+ ###)', content)\n",
        "\n",
        "normalized_articles = []\n",
        "\n",
        "for article in articles:\n",
        "    article = article.strip()\n",
        "    if not article:\n",
        "        continue\n",
        "\n",
        "    parts = article.split(\"\\n\", 1)\n",
        "    header = parts[0].strip()\n",
        "    body = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "    body = normalize_whitespace(body)\n",
        "\n",
        "    normalized_articles.append(header + \"\\n\" + body + \"\\n\\n\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(normalized_articles)\n",
        "\n",
        "print(\"Whitespace and formatting normalization complete.\")\n",
        "print(\"File saved as normalized.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdhlHnlA1Pao",
        "outputId": "2f5735ad-2aea-46bd-a017-9eb61fa8b96d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace and formatting normalization complete.\n",
            "File saved as normalized.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Linguistic Processing**"
      ],
      "metadata": {
        "id": "pMmexb-Y9MTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "input_file = \"normalized.txt\"\n",
        "output_file = \"cleaned.txt\"\n",
        "\n",
        "\n",
        "def urdu_tokenizer(text, is_header=False):\n",
        "    \"\"\"\n",
        "    Tokenizes Urdu text:\n",
        "    - Replaces numbers with <NUM> only for body text\n",
        "    - Separates punctuation\n",
        "    \"\"\"\n",
        "    if not is_header:\n",
        "        text = re.sub(r'\\d+', '<NUM>', text)\n",
        "\n",
        "    text = re.sub(r'([€îÿåÿü!])', r' \\1 ', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    tokens = text.split(\" \")\n",
        "    return tokens\n",
        "\n",
        "lemmatizer_map = {\n",
        "    \"€Å€å⁄∫\": \"€Å€í\",\n",
        "    \"ÿ™⁄æ€å⁄∫\": \"ÿ™⁄æÿß\",\n",
        "    \"⁄Øÿ¶€å⁄∫\": \"⁄Ø€åÿß\",\n",
        "    \"⁄©ÿ±ÿ™€å\": \"⁄©ÿ±ÿ™ÿß\"\n",
        "}\n",
        "\n",
        "def urdu_lemmatizer(word):\n",
        "    \"\"\"\n",
        "    Rule-based lemmatizer for Urdu:\n",
        "    - Handles plurals (Ÿà⁄∫, €å⁄∫, ÿßÿ™)\n",
        "    - Feminine ‚Üí Masculine (€å ‚Üí ÿß)\n",
        "    - Handles irregular forms via dictionary\n",
        "    \"\"\"\n",
        "    if word in lemmatizer_map:\n",
        "        return lemmatizer_map[word]\n",
        "\n",
        "    if word.endswith(\"Ÿà⁄∫\") and len(word) > 3:\n",
        "        return word[:-2]\n",
        "\n",
        "    if word.endswith(\"€å⁄∫\") and len(word) > 3:\n",
        "        return word[:-2]\n",
        "\n",
        "    if word.endswith(\"ÿßÿ™\") and len(word) > 3:\n",
        "        return word[:-2]\n",
        "\n",
        "    if word.endswith(\"€å\") and len(word) > 3:\n",
        "        return word[:-1] + \"ÿß\"\n",
        "\n",
        "    return word\n",
        "\n",
        "\n",
        "suffixes = [\n",
        "    \"Ÿà⁄∫\", \"€å⁄∫\", \"ÿßÿ™\", \"€åÿß⁄∫\",\n",
        "    \"ŸÜ€í\", \"€Å€í\", \"€ÅŸà⁄∫\"\n",
        "]\n",
        "\n",
        "def urdu_stemmer(word):\n",
        "    \"\"\"\n",
        "    Light stemmer to reduce vocabulary without destroying sentence structure.\n",
        "    \"\"\"\n",
        "    for suffix in sorted(suffixes, key=len, reverse=True):\n",
        "        if word.endswith(suffix) and len(word) > len(suffix) + 1:\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = re.split(r'(?=### Article \\d+ ###)', content)\n",
        "\n",
        "processed_articles = []\n",
        "\n",
        "for article in articles:\n",
        "    article = article.strip()\n",
        "    if not article:\n",
        "        continue\n",
        "\n",
        "    parts = article.split(\"\\n\", 1)\n",
        "    header = parts[0].strip()\n",
        "    body = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "    sentences = body.split(\"\\n\")\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if not sentence:\n",
        "            continue\n",
        "\n",
        "        tokens = urdu_tokenizer(sentence, is_header=False)\n",
        "        tokens = [urdu_lemmatizer(tok) for tok in tokens]\n",
        "\n",
        "        tokens = [urdu_stemmer(tok) for tok in tokens]\n",
        "\n",
        "        processed_sentences.append(\" \".join(tokens))\n",
        "\n",
        "    processed_body = \"\\n\".join(processed_sentences)\n",
        "\n",
        "    processed_articles.append(header + \"\\n\" + processed_body + \"\\n\\n\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(processed_articles)\n",
        "\n",
        "print(\"Custom Tokenization, Lemmatization, and Light Stemming complete.\")\n",
        "print(f\"File saved as {output_file}\")\n"
      ],
      "metadata": {
        "id": "cRMIWamf5LYJ",
        "outputId": "1b4ccc3b-b162-47df-ab2e-5cb27f2fba80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tokenization, Lemmatization, and Light Stemming complete.\n",
            "File saved as cleaned.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2 - BBC Style Urdu News Article Generation**"
      ],
      "metadata": {
        "id": "vvnr6bm2CEwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Language Model Training**"
      ],
      "metadata": {
        "id": "_ESeGmwYCJYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "print(\"Loading preprocessed dataset from cleaned.txt...\")\n",
        "\n",
        "with open(\"cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = content.split(\"### Article \")\n",
        "all_tokens = []\n",
        "\n",
        "for article in articles:\n",
        "    if not article.strip():\n",
        "        continue\n",
        "\n",
        "    lines = article.split(\"\\n\", 1)\n",
        "    if len(lines) > 1:\n",
        "        body = lines[1].strip()\n",
        "        tokens = body.split()\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "print(f\"Total tokens: {len(all_tokens)}\")\n",
        "print(f\"Vocabulary size: {len(set(all_tokens))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptwdRejqDi2N",
        "outputId": "da504a4f-7d5e-4bd2-fef2-627c362db85f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed dataset from cleaned.txt...\n",
            "Total tokens: 431923\n",
            "Vocabulary size: 12712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UNIGRAM Model + Smoothing**"
      ],
      "metadata": {
        "id": "uupcIq9ADoij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class UnigramModel:\n",
        "\n",
        "    def __init__(self, smoothing='add-k', k=0.1):\n",
        "        self.unigram_counts = Counter()\n",
        "        self.total_words = 0\n",
        "        self.vocabulary = set()\n",
        "        self.vocab_size = 0\n",
        "        self.smoothing = smoothing\n",
        "        self.k = 1.0 if smoothing == 'laplace' else k\n",
        "\n",
        "    def train(self, tokens):\n",
        "        self.unigram_counts = Counter(tokens)\n",
        "        self.total_words = len(tokens)\n",
        "        self.vocabulary = set(tokens)\n",
        "        self.vocab_size = len(self.vocabulary)\n",
        "\n",
        "    def get_probability(self, word):\n",
        "        count = self.unigram_counts[word]\n",
        "        numerator = count + self.k\n",
        "        denominator = self.total_words + (self.k * self.vocab_size)\n",
        "        return numerator / denominator\n",
        "\n",
        "\n",
        "unigram_model = UnigramModel(smoothing='add-k', k=0.1)\n",
        "unigram_model.train(all_tokens)\n",
        "\n",
        "print(\"[UNIGRAM MODEL TRAINED]\")\n",
        "print(\"Vocabulary size:\", unigram_model.vocab_size)\n",
        "\n",
        "print(\"\\nTop 10 unigrams:\")\n",
        "for word, count in unigram_model.unigram_counts.most_common(10):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "unseen_word = \"XYZ_UNSEEN_WORD\"\n",
        "prob = unigram_model.get_probability(unseen_word)\n",
        "print(f\"\\nSmoothing Demo (Unseen Unigram):\")\n",
        "print(f\"P({unseen_word}) = {prob:.10f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuvHxSEkDixo",
        "outputId": "139ac4f7-3a76-4d48-caf7-5de0aa2a0e2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIGRAM MODEL TRAINED]\n",
            "Vocabulary size: 12712\n",
            "\n",
            "Top 10 unigrams:\n",
            "⁄©€í: 18606\n",
            "€î: 14228\n",
            "€Å€í: 13767\n",
            "ŸÖ€å⁄∫: 12559\n",
            "⁄©€å: 12088\n",
            "ÿßŸàÿ±: 8485\n",
            "ÿ≥€í: 8402\n",
            "⁄©€Å: 8259\n",
            "ŸÜ€í: 6429\n",
            "⁄©ÿß: 6194\n",
            "\n",
            "Smoothing Demo (Unseen Unigram):\n",
            "P(XYZ_UNSEEN_WORD) = 0.0000002308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIGRAM Model + Smoothing**"
      ],
      "metadata": {
        "id": "wwPJCurIDzt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "class BigramModel:\n",
        "\n",
        "    def __init__(self, smoothing='add-k', k=0.1):\n",
        "        self.bigram_counts = defaultdict(Counter)\n",
        "        self.unigram_counts = Counter()\n",
        "        self.vocabulary = set()\n",
        "        self.vocab_size = 0\n",
        "        self.smoothing = smoothing\n",
        "        self.k = 1.0 if smoothing == 'laplace' else k\n",
        "\n",
        "    def train(self, tokens):\n",
        "        tokens = ['<START>'] + tokens + ['<END>']\n",
        "        self.unigram_counts = Counter(tokens)\n",
        "        self.vocabulary = set(tokens)\n",
        "        self.vocab_size = len(self.vocabulary)\n",
        "        for i in range(len(tokens)-1):\n",
        "            w1, w2 = tokens[i], tokens[i+1]\n",
        "            self.bigram_counts[w1][w2] += 1\n",
        "\n",
        "    def get_probability(self, w1, w2):\n",
        "        bigram_count = self.bigram_counts[w1][w2]\n",
        "        unigram_count = self.unigram_counts[w1]\n",
        "        numerator = bigram_count + self.k\n",
        "        denominator = unigram_count + (self.k * self.vocab_size)\n",
        "        return numerator / denominator\n",
        "\n",
        "    def get_next_word_probabilities(self, w1):\n",
        "        \"\"\"\n",
        "        Calculates the probability distribution for the next word given w1.\n",
        "        \"\"\"\n",
        "        probabilities = {}\n",
        "        for w2 in self.vocabulary:\n",
        "            probabilities[w2] = self.get_probability(w1, w2)\n",
        "\n",
        "        total_prob = sum(probabilities.values())\n",
        "        if total_prob > 0:\n",
        "            for w2 in probabilities:\n",
        "                probabilities[w2] /= total_prob\n",
        "        return probabilities\n",
        "\n",
        "\n",
        "bigram_model = BigramModel(smoothing='add-k', k=0.1)\n",
        "bigram_model.train(all_tokens)\n",
        "\n",
        "print(\"[BIGRAM MODEL TRAINED]\")\n",
        "print(\"Vocabulary size:\", bigram_model.vocab_size)\n",
        "\n",
        "bigram_list = []\n",
        "for w1 in bigram_model.bigram_counts:\n",
        "    for w2 in bigram_model.bigram_counts[w1]:\n",
        "        count = bigram_model.bigram_counts[w1][w2]\n",
        "        bigram_list.append(((w1, w2), count))\n",
        "\n",
        "bigram_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 bigrams:\")\n",
        "for bigram, count in bigram_list[:10]:\n",
        "    print(f\"{bigram}: {count}\")\n",
        "\n",
        "test_w1 = \"Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ\"\n",
        "unseen_word = \"XYZ_UNSEEN_WORD\"\n",
        "prob = bigram_model.get_probability(test_w1, unseen_word)\n",
        "print(f\"\\nSmoothing Demo (Unseen Bigram):\")\n",
        "print(f\"P({unseen_word} | {test_w1}) = {prob:.10f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiOEz2ciDiu6",
        "outputId": "07ff2ede-a1c3-46cb-bb75-4d245c25a61e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BIGRAM MODEL TRAINED]\n",
            "Vocabulary size: 12714\n",
            "\n",
            "Top 10 bigrams:\n",
            "('€Å€í', '€î'): 5621\n",
            "('€Å€í', '⁄©€Å'): 3014\n",
            "('⁄©€í', 'ŸÑ€å€í'): 2023\n",
            "('⁄©€í', 'ŸÖÿ∑ÿßÿ®ŸÇ'): 1347\n",
            "('ÿßŸÜ⁄æ', 'ŸÜ€í'): 1281\n",
            "('ÿ™⁄æÿß', '⁄©€Å'): 1257\n",
            "('ÿßŸÜ', '⁄©€í'): 1188\n",
            "('€Å€í', 'ÿßŸàÿ±'): 1185\n",
            "('ÿ™⁄æÿß', '€î'): 1165\n",
            "('⁄©ÿß', '⁄©€ÅŸÜÿß'): 1139\n",
            "\n",
            "Smoothing Demo (Unseen Bigram):\n",
            "P(XYZ_UNSEEN_WORD | Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ) = 0.0000259646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRIGRAM Model + Smoothing**"
      ],
      "metadata": {
        "id": "xapSrhFDEv-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "class TrigramModel:\n",
        "\n",
        "    def __init__(self, bigram_model, smoothing='add-k', k=0.1):\n",
        "        self.trigram_counts = defaultdict(lambda: defaultdict(Counter))\n",
        "        self.bigram_context_counts = defaultdict(int)\n",
        "        self.vocabulary = set()\n",
        "        self.vocab_size = 0\n",
        "        self.bigram_model = bigram_model\n",
        "        self.smoothing = smoothing\n",
        "        self.k = 1.0 if smoothing == 'laplace' else k\n",
        "\n",
        "    def train(self, tokens):\n",
        "        tokens = ['<START>', '<START>'] + tokens + ['<END>']\n",
        "        self.vocabulary = set(tokens)\n",
        "        self.vocab_size = len(self.vocabulary)\n",
        "        for i in range(len(tokens)-2):\n",
        "            w1, w2, w3 = tokens[i], tokens[i+1], tokens[i+2]\n",
        "            self.trigram_counts[w1][w2][w3] += 1\n",
        "            self.bigram_context_counts[(w1, w2)] += 1\n",
        "\n",
        "    def get_probability(self, w1, w2, w3):\n",
        "        trigram_count = self.trigram_counts[w1][w2][w3]\n",
        "        context_count = self.bigram_context_counts[(w1, w2)]\n",
        "        if context_count < 2:\n",
        "            return self.bigram_model.get_probability(w2, w3)\n",
        "        numerator = trigram_count + self.k\n",
        "        denominator = context_count + (self.k * self.bigram_model.vocab_size)\n",
        "        return numerator / denominator\n",
        "\n",
        "    def get_next_word_probabilities(self, w1, w2):\n",
        "        \"\"\"\n",
        "        Calculates the probability distribution for the next word given w1 and w2.\n",
        "        Includes backoff to bigram model if trigram context count is too low.\n",
        "        \"\"\"\n",
        "        probabilities = {}\n",
        "        context_count = self.bigram_context_counts[(w1, w2)]\n",
        "\n",
        "        if context_count < 2:\n",
        "            probabilities = self.bigram_model.get_next_word_probabilities(w2)\n",
        "        else:\n",
        "            for w3 in self.bigram_model.vocabulary:\n",
        "                trigram_count = self.trigram_counts[w1][w2][w3]\n",
        "                numerator = trigram_count + self.k\n",
        "                denominator = context_count + (self.k * self.bigram_model.vocab_size)\n",
        "                probabilities[w3] = numerator / denominator\n",
        "\n",
        "        total_prob = sum(probabilities.values())\n",
        "        if total_prob > 0:\n",
        "            for w3 in probabilities:\n",
        "                probabilities[w3] /= total_prob\n",
        "        return probabilities\n",
        "\n",
        "\n",
        "trigram_model = TrigramModel(bigram_model, smoothing='add-k', k=0.1)\n",
        "trigram_model.train(all_tokens)\n",
        "\n",
        "print(\"[TRIGRAM MODEL TRAINED]\")\n",
        "print(\"Vocabulary size:\", trigram_model.vocab_size)\n",
        "\n",
        "trigram_list = []\n",
        "for w1 in trigram_model.trigram_counts:\n",
        "    for w2 in trigram_model.trigram_counts[w1]:\n",
        "        for w3 in trigram_model.trigram_counts[w1][w2]:\n",
        "            count = trigram_model.trigram_counts[w1][w2][w3]\n",
        "            trigram_list.append(((w1, w2, w3), count))\n",
        "\n",
        "trigram_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 trigrams:\")\n",
        "for trigram, count in trigram_list[:10]:\n",
        "    print(f\"{trigram}: {count}\")\n",
        "\n",
        "test_w1 = \"ŸÖ€å⁄∫\"\n",
        "test_w2 = \"Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ\"\n",
        "unseen_word = \"XYZ_UNSEEN_WORD\"\n",
        "prob = trigram_model.get_probability(test_w1, test_w2, unseen_word)\n",
        "print(f\"\\nSmoothing Demo (Unseen Trigram):\")\n",
        "print(f\"P({unseen_word} | {test_w1}, {test_w2}) = {prob:.10f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOsVBNlIDish",
        "outputId": "a5dcdcb9-173a-4a4e-9c9c-aaf18b4de03f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRIGRAM MODEL TRAINED]\n",
            "Vocabulary size: 12714\n",
            "\n",
            "Top 10 trigrams:\n",
            "('⁄©€ÅŸÜÿß', 'ÿ™⁄æÿß', '⁄©€Å'): 718\n",
            "('€î', 'ÿßŸÜ⁄æ', 'ŸÜ€í'): 701\n",
            "('⁄©ÿß', '⁄©€ÅŸÜÿß', 'ÿ™⁄æÿß'): 680\n",
            "('ÿ®€å', 'ÿ®€å', 'ÿ≥€å'): 578\n",
            "('⁄©€å', 'ÿ¨ÿßŸÜÿ®', 'ÿ≥€í'): 575\n",
            "('ŸÜ€í', '⁄©€Åÿß', '⁄©€Å'): 536\n",
            "('⁄©€Åÿ™€í', '€Å€í', '⁄©€Å'): 531\n",
            "('⁄©€í', 'ÿ®ÿßÿ±€í', 'ŸÖ€å⁄∫'): 481\n",
            "('⁄©€ÅŸÜÿß', '€Å€í', '⁄©€Å'): 466\n",
            "('⁄©ÿß', '⁄©€ÅŸÜÿß', '€Å€í'): 456\n",
            "\n",
            "Smoothing Demo (Unseen Trigram):\n",
            "P(XYZ_UNSEEN_WORD | ŸÖ€å⁄∫, Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ) = 0.0000697642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Article Generation System and Interface**"
      ],
      "metadata": {
        "id": "zBFG4adIfaTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper Function**"
      ],
      "metadata": {
        "id": "lWdCNEnKfiqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def sample_next_word(prob_dict):\n",
        "    \"\"\"\n",
        "    Sample next word based on probability distribution\n",
        "\n",
        "    Args:\n",
        "        prob_dict: Dictionary with words as keys and probabilities as values\n",
        "\n",
        "    Returns:\n",
        "        Sampled word based on probabilities\n",
        "    \"\"\"\n",
        "    words = list(prob_dict.keys())\n",
        "    probs = list(prob_dict.values())\n",
        "\n",
        "    # Normalize probabilities to ensure they sum to 1\n",
        "    total = sum(probs)\n",
        "    if total > 0:\n",
        "        probs = [p/total for p in probs]\n",
        "    else:\n",
        "        # If all probabilities are 0, use uniform distribution\n",
        "        probs = [1/len(probs)] * len(probs)\n",
        "\n",
        "    return random.choices(words, weights=probs, k=1)[0]\n",
        "\n",
        "print(\"Helper function loaded successfully!\")"
      ],
      "metadata": {
        "id": "v8_Zb_f1HovE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4568a89f-fc28-477a-e96b-151b718bc09a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper function loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bigram Article Generator**"
      ],
      "metadata": {
        "id": "jeYShH19floD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_bigram_article(model, seed_tokens, min_words=200, max_words=300):\n",
        "    \"\"\"\n",
        "    Generate article using Bigram Language Model\n",
        "\n",
        "    Args:\n",
        "        model: Trained BigramModel\n",
        "        seed_tokens: List of seed tokens to start generation\n",
        "        min_words: Minimum number of words before allowing to stop\n",
        "        max_words: Maximum number of words to generate\n",
        "\n",
        "    Returns:\n",
        "        Generated article as string\n",
        "    \"\"\"\n",
        "    generated = seed_tokens.copy()\n",
        "\n",
        "    while len(generated) < max_words:\n",
        "        last_word = generated[-1]\n",
        "\n",
        "        if last_word not in model.vocabulary:\n",
        "            next_word = random.choice(list(model.vocabulary))\n",
        "        else:\n",
        "            prob_dist = model.get_next_word_probabilities(last_word)\n",
        "            next_word = sample_next_word(prob_dist)\n",
        "\n",
        "        generated.append(next_word)\n",
        "\n",
        "        if len(generated) >= min_words and next_word == \"€î\":\n",
        "            break\n",
        "\n",
        "    return \" \".join(generated)\n",
        "\n",
        "print(\"Bigram article generator loaded successfully!\")"
      ],
      "metadata": {
        "id": "hW1TrDCIaMvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ecc680-5fb5-4e74-e70c-ac99d684c7d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram article generator loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trigram Generator with Backoff**"
      ],
      "metadata": {
        "id": "mHCaAQ5AfuIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_trigram_article(trigram_model, unigram_model, seed_tokens,\n",
        "                              min_words=200, max_words=300):\n",
        "    \"\"\"\n",
        "    Generate article using Trigram Language Model with Unigram Backoff\n",
        "\n",
        "    Args:\n",
        "        trigram_model: Trained TrigramModel\n",
        "        unigram_model: Trained UnigramModel (for backoff)\n",
        "        seed_tokens: List of seed tokens to start generation\n",
        "        min_words: Minimum number of words before allowing to stop\n",
        "        max_words: Maximum number of words to generate\n",
        "\n",
        "    Returns:\n",
        "        Generated article as string\n",
        "    \"\"\"\n",
        "    generated = seed_tokens.copy()\n",
        "\n",
        "    while len(generated) < max_words:\n",
        "        if len(generated) < 2:\n",
        "            next_word = random.choice(list(trigram_model.vocabulary))\n",
        "        else:\n",
        "            w1 = generated[-2]\n",
        "            w2 = generated[-1]\n",
        "\n",
        "            if w1 in trigram_model.vocabulary and w2 in trigram_model.vocabulary:\n",
        "                prob_dist = trigram_model.get_next_word_probabilities(w1, w2)\n",
        "                next_word = sample_next_word(prob_dist)\n",
        "            else:\n",
        "                next_word = random.choice(list(unigram_model.vocabulary))\n",
        "\n",
        "        generated.append(next_word)\n",
        "\n",
        "        if len(generated) >= min_words and next_word == \"€î\":\n",
        "            break\n",
        "\n",
        "    return \" \".join(generated)\n",
        "\n",
        "print(\"Trigram article generator with backoff loaded successfully!\")"
      ],
      "metadata": {
        "id": "rR5s9qwIa2qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4cdce99-1e96-4d7b-eae7-041a81104cd0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram article generator with backoff loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Console Interface**"
      ],
      "metadata": {
        "id": "dsSNON-Ef8zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def display_urdu_rtl(article_text, model_type):\n",
        "    \"\"\"\n",
        "    Display Urdu article with proper Right-to-Left (RTL) formatting\n",
        "\n",
        "    Args:\n",
        "        article_text: Generated article text\n",
        "        model_type: Name of the model used (Bigram/Trigram)\n",
        "    \"\"\"\n",
        "    html = f\"\"\"\n",
        "    <div style=\"border: 3px solid #667eea; border-radius: 12px; padding: 0;\n",
        "                margin: 25px 0; background: #fff; box-shadow: 0 8px 16px rgba(102,126,234,0.2);\">\n",
        "\n",
        "        <!-- Header -->\n",
        "        <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                    color: white; padding: 20px; border-radius: 10px 10px 0 0;\">\n",
        "            <h2 style=\"margin: 0; font-size: 26px;\">üì∞ Generated BBC Urdu Article</h2>\n",
        "            <div style=\"margin-top: 8px; font-size: 14px; opacity: 0.9;\">\n",
        "                Model: {model_type}\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <!-- Article Content with RTL -->\n",
        "        <div style=\"direction: rtl; text-align: right;\n",
        "                    font-family: 'Jameel Noori Nastaleeq', 'Nafees Web Naskh', 'Arial', sans-serif;\n",
        "                    font-size: 20px; line-height: 2.5; padding: 30px; background: #fafbfc;\">\n",
        "            {article_text}\n",
        "        </div>\n",
        "\n",
        "        <!-- Footer -->\n",
        "        <div style=\"padding: 15px 20px; background: #f0f0f0; border-top: 1px solid #e0e0e0;\n",
        "                    border-radius: 0 0 10px 10px; font-size: 13px; color: #666;\">\n",
        "            <span>Total Tokens: {len(article_text.split())}</span>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html))\n",
        "\n",
        "\n",
        "def article_generation_interface():\n",
        "    \"\"\"\n",
        "    Article Generation System and Interface\n",
        "\n",
        "    System supports:\n",
        "    ‚úì Language model selection (Bigram or Trigram)\n",
        "    ‚úì Seed prompt input\n",
        "    ‚úì Automatic article generation\n",
        "    ‚úì Proper RTL display of Urdu text\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"BBC Style Urdu News Article Generator\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nSelect Language Model:\")\n",
        "    print(\"1. Bigram Model\")\n",
        "    print(\"2. Trigram Model (with Backoff)\")\n",
        "\n",
        "    choice = input(\"\\nEnter choice (1 or 2): \").strip()\n",
        "\n",
        "    seed_prompt = input(\"Enter seed prompt (5‚Äì8 Urdu words): \").strip()\n",
        "    seed_tokens = seed_prompt.split()\n",
        "\n",
        "    if len(seed_tokens) < 5:\n",
        "        print(\"Invalid seed prompt. Must contain at least 5 words.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nGenerating article...\\n\")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        article = generate_bigram_article(bigram_model, seed_tokens)\n",
        "        model_name = \"Bigram Model\"\n",
        "    elif choice == \"2\":\n",
        "        article = generate_trigram_article(trigram_model, unigram_model, seed_tokens)\n",
        "        model_name = \"Trigram Model (with Backoff)\"\n",
        "    else:\n",
        "        print(\"Invalid model choice.\")\n",
        "        return\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Article Generated Successfully!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\n Article with RTL Display:\\n\")\n",
        "    display_urdu_rtl(article, model_name)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Plain Text Output:\")\n",
        "    print(\"=\"*60)\n",
        "    print(article)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"\\n Statistics:\")\n",
        "    print(f\"   - Model used: {model_name}\")\n",
        "    print(f\"   - Seed tokens: {len(seed_tokens)}\")\n",
        "    print(f\"   - Total tokens generated: {len(article.split())}\")\n",
        "    print(f\"   - Generated tokens: {len(article.split()) - len(seed_tokens)}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(\"\\nConsole interface ready!\")\n",
        "print(\"Run article_generation_interface() to start generating articles\\n\")\n",
        "\n",
        "article_generation_interface()"
      ],
      "metadata": {
        "id": "qDLJ5jHMa4G9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad40dc52-dc11-4b3f-bef6-3b3e148e6f1b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Console interface ready!\n",
            "Run article_generation_interface() to start generating articles\n",
            "\n",
            "============================================================\n",
            "BBC Style Urdu News Article Generator\n",
            "============================================================\n",
            "\n",
            "Select Language Model:\n",
            "1. Bigram Model\n",
            "2. Trigram Model (with Backoff)\n",
            "\n",
            "Enter choice (1 or 2): 1\n",
            "Enter seed prompt (5‚Äì8 Urdu words): Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫\n",
            "\n",
            "Generating article...\n",
            "\n",
            "============================================================\n",
            "Article Generated Successfully!\n",
            "============================================================\n",
            "\n",
            " Article with RTL Display:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"border: 3px solid #667eea; border-radius: 12px; padding: 0; \n",
              "                margin: 25px 0; background: #fff; box-shadow: 0 8px 16px rgba(102,126,234,0.2);\">\n",
              "        \n",
              "        <!-- Header -->\n",
              "        <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
              "                    color: white; padding: 20px; border-radius: 10px 10px 0 0;\">\n",
              "            <h2 style=\"margin: 0; font-size: 26px;\">üì∞ Generated BBC Urdu Article</h2>\n",
              "            <div style=\"margin-top: 8px; font-size: 14px; opacity: 0.9;\">\n",
              "                Model: Bigram Model\n",
              "            </div>\n",
              "        </div>\n",
              "        \n",
              "        <!-- Article Content with RTL -->\n",
              "        <div style=\"direction: rtl; text-align: right; \n",
              "                    font-family: 'Jameel Noori Nastaleeq', 'Nafees Web Naskh', 'Arial', sans-serif;\n",
              "                    font-size: 20px; line-height: 2.5; padding: 30px; background: #fafbfc;\">\n",
              "            Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫ ÿ≤€åÿ± Ÿπÿ¥Ÿàÿ≤ ÿ¨ŸàŸÜÿ≤ ÿßŸàŸÑÿßÿØ ⁄ØŸÑŸàÿ®ŸÑ ŸÖÿ∂ŸÖÿ± ÿ¨⁄æŸπ⁄© ŸÖŸàÿßÿ≤ ⁄àÿßŸÜŸàÿß ÿ≥ÿ±Ÿàÿ±ÿ≤ ŸÜÿßÿ®ÿßŸÑÿ∫ ÿßÿ≠ÿ≥ÿßŸÜ ŸÖ⁄©ÿßŸÜÿß ŸÜŸÜ⁄Øÿ±€Åÿßÿ± ŸÜÿ¥ÿßÿ∑ ⁄©ÿßŸÑÿ¨ÿ≤ ÿ®ÿßÿ±€å⁄©€å ÿ¥€åŸæÿ±⁄à ⁄à€åŸπŸàŸÜ€åŸπÿ±ÿ≤ ⁄ØŸàŸÜÿ¨ÿ™ÿß ŸÖÿ∞ÿß⁄©ÿ±ÿßÿ™ÿß ÿß€å⁄©ÿ≥Ÿπ€åŸÜÿ¥ŸÜ ⁄Ü⁄æŸà⁄ëŸà ŸÖ€å⁄ØÿßŸàÿßŸπ ŸÅÿ±ÿßÿ∫ÿ™ ÿ™ÿ®ÿØ€åŸÑÿß ÿ≥€å⁄©ŸàŸÜÿ≥ŸÜ⁄Ø ⁄©ÿßÿ±ÿ≥€åŸà⁄© ÿ¢ÿØ€åÿßŸÑ€Å ⁄ØÿßŸÑŸÅ Ÿæ€åÿ¶ ŸÖÿ¥ÿß€ÅÿØ€Å ÿ≥ÿßÿ≤Ÿà ⁄©ÿßŸÑ€åÿ≥Ÿπÿ±ŸàŸÑ ÿÆÿ±⁄Ü€í ⁄©€åŸÖÿ±ŸàŸÜ ŸÑÿß ⁄à€åŸæÿßÿ±ŸπŸÖŸÜŸπÿ≥ ÿπÿ®ÿØÿßŸÑÿ≥ÿ™ÿßÿ± ⁄©⁄æÿ®€Å Ÿæ€å⁄à ÿ≥Ÿà⁄Üÿ™€í ŸÖ⁄Øÿ± ÿ≥€å⁄©Ÿàÿ±Ÿπÿß ŸæŸàÿ≤€åÿ¥ŸÜ ⁄Øÿß€Å ÿÆÿ®ÿ±ÿØÿßÿ± ÿßÿ≥ŸÖÿ®ŸÑÿß ⁄ØŸàŸÑ⁄à ÿ™ŸÇÿ±€åÿ±ÿ≥€í ŸÖÿ≥ÿ™ŸÇŸÑ ⁄Øÿß⁄ë€í ÿ™ÿ±ÿßŸÜ⁄à€åÿß ⁄Ø€å€í ŸÖÿ´ÿßŸÑ ÿßÿπÿ™ÿßÿ® ⁄©⁄æŸÖÿ®€í ŸÖŸÜÿµŸàÿ± Ÿàÿßÿ¶ŸÑŸÜÿ≥ ÿ≥€åŸÜ⁄Üÿ±ÿß ÿ≥⁄©ÿß ÿßŸÜ⁄Ü ŸÇ€åŸàŸÖ ÿßÿπÿ∏ŸÖÿß ÿ±€åŸÑ€åŸÅ ÿßÿ≤ÿ®⁄©ÿ≥ÿ™ÿßŸÜ ÿ®ÿ∞ÿ±€åÿπ€Å ÿßŸπ⁄æÿßŸÜŸà€í ÿßŸàŸæÿ± ⁄àÿßÿ±Ÿπ ⁄ØŸàÿ±⁄©⁄æŸæŸàÿ± ÿ®⁄æ€åÿ¨€í ŸÖÿπÿßŸÅÿß ÿ®ÿßÿ®ŸÜÿØÿß ÿ±ŸàŸπ⁄æ ÿ¥ÿßÿØÿß ŸÖŸÜÿ¥€å ⁄©ŸàŸπ ÿ¥€åÿ±Ÿà Ÿà€åŸÜŸπÿß ÿπÿ®ÿØÿßŸÑÿ±ÿ¥€åÿØ ⁄àÿßŸÑÿ±<NUM> ÿ™ÿ¥Ÿà€åÿ¥ŸÜÿß⁄© ⁄Ü⁄æŸà⁄ë ÿ∫€åÿ±ŸÖŸÇÿßŸÖÿß Ÿæ€åÿ± ŸÖÿßÿ¨ÿØ ÿ¥€åŸàÿ±ŸÑ€åŸπ ŸÜ€å⁄Ø€åŸπŸà ⁄Øÿ±ÿØÿßŸÜÿß ÿß€åŸà€åÿ¥ŸÜ ŸÖÿ≠ÿ≥ŸÜ ŸÇŸàŸÖ€åÿß ÿ≠ÿßÿ∂ÿ± ŸÖŸÜÿ∑ŸÇÿß ⁄©ŸàÿßŸÑ€åŸÅÿßÿ¶ÿß Ÿæÿ±ÿØ⁄Øÿß ÿ™Ÿàÿ¥€Å ÿπŸÑÿßŸÖÿ™ ⁄©ÿ±ŸÑÿß €ÅŸàŸÑ⁄à ÿ±ÿ≥ÿßÿ§ ŸÑ⁄ØŸàÿßÿ¶ÿß ÿ®€ÅŸÜŸàÿ¶ÿß ⁄©ŸÜŸàÿßÿ±ÿß ÿ™€ÅŸàÿßÿ± ÿ¥ÿ±ŸÖŸÜÿØ€Å ÿ®ÿßÿ¨ŸàÿØ ⁄©⁄æŸÜ⁄Ü ÿ®ÿ≥€å ŸÖ⁄Ü⁄æ Ÿπÿßÿ§ŸÜÿ≤ ÿ≥ŸÖÿ¨⁄æÿßÿ™ÿß ŸæÿßŸæŸàŸÑÿ≥Ÿπ Ÿæÿ±ŸàŸÅ ÿ®ÿßÿ¢ÿ≥ÿßŸÜÿß ÿßŸÑŸàÿ∑ŸÜÿß ÿ≥ŸÑ€åŸæŸÜ⁄Ø ÿ∏€Å€åÿ± ÿßÿ±ÿ¥ÿßÿØ ÿ¥€åŸàÿß ⁄Ü€åŸÖŸæÿ¶ŸÜ ⁄Ü€åŸÑŸÜÿ¨ ÿßŸÜŸÅÿ±ÿßÿ≥Ÿπÿ±⁄©⁄Üÿ± ⁄Üÿ±⁄Üÿß €ÅŸàŸÜ€íŸàÿßŸÑÿß ⁄Üÿ± ÿßŸÖÿ±ÿß ÿ™⁄æ€åÿ™ÿß€ÅŸÖ ÿ™ŸÖÿßÿ¥ÿßÿ¶ÿß ÿ´ÿ®Ÿàÿ™ ÿ≤⁄Ü€Å ÿ®⁄ÜÿßŸÜÿß ÿ≤€å ÿ±€åŸÜÿ¨ ÿ®⁄ë⁄æÿßÿ¶€í ŸÖ€åÿ≥ÿ≤ ÿ®ÿßÿ±ÿ¥ ÿßÿ≥ÿπÿ™ŸÖÿßŸÑ ÿ¨⁄©⁄ë ÿ™ÿ± ÿ®Ÿàÿßÿ¶ŸÑŸÜ⁄Ø Ÿæÿ±ÿßÿ¶ÿ≥ŸÜ⁄Ø ÿ≥ŸÖ€åÿ™ <NUM> ÿ™⁄æŸÜ⁄àÿ± ⁄ÜÿßŸπ ÿ™ÿ±⁄©ÿß ÿØŸÑ€ÅŸÜ ÿß€åÿ¶ÿ±⁄©ŸÖŸà⁄àŸàÿ± ÿ≥ŸÜ⁄à€å⁄©€åŸπ ⁄àÿßÿ¶ÿ±€å⁄©ŸπŸàÿ±€åŸπÿ≥ ÿ≥ÿßÿ¶ŸÜÿ≥ÿØÿßŸÜ ŸÜŸàÿ¥ ÿ¢ŸÖÿØŸàÿ±ŸÅÿ™ ÿ™ÿ¥Ÿà€åÿ¥ ÿØŸàŸÑ€ÅŸÜ ⁄Üÿßÿ¶ŸÑ⁄àÿ≥ ÿ±€åŸÅÿßÿ¶ŸÜÿ±ÿß €ÅŸàÿ¶€íÿ™⁄æ€í ÿ®€åÿØÿßÿ± ŸÖŸÜÿßÿ¶ÿß ⁄Øÿ¶€å ÿ≤ŸÖÿßŸÜ€Å ÿ®€åÿ¥ŸÖÿßÿ± ÿ¢ÿ≥ ÿÆŸÑ€å ⁄©ÿßÿ±€å⁄Øÿ± ŸÖÿ≤ÿßÿ≠ŸÖÿ™ ÿßŸàÿ®ÿßŸÖÿß ÿØÿßÿ§ÿØ ÿ∫€åÿ±ÿ≥ÿ±⁄©ÿßÿ±ÿß ⁄©Ÿàÿ±€åÿ¨ ÿ®ÿ±€í ÿ®ÿßÿ≤€åÿßÿ®ÿß Ÿà€å⁄∫ ⁄©⁄æÿßŸÜ ÿ¢ÿ≥ÿ™ÿßŸÜ€Å ŸπÿßŸÑ€åÿ±ŸÜÿ≥ ÿ®ÿ±ŸàŸÜÿßÿ¶ÿß ÿÆŸàÿ¥ÿ≠ÿßŸÑÿß ÿ±Ÿàÿß⁄∫ ÿ™⁄æÿ±ÿ≥Ÿπ ÿµÿßÿ®ŸÜ ÿßŸÜŸπ€åÿ±€åÿ¶ÿ± Ÿæ€å⁄Ü€åÿØ⁄Ø€å ŸÅÿßÿ±ŸàŸÇ ⁄à€åŸæ ÿµÿØ€Å ÿØÿ≥ÿ™ ŸÖŸÜÿµÿ® ÿß⁄Ü⁄æÿßŸÑ ⁄©€åŸÅ€åÿ™ ÿ∑Ÿà€åŸÑ ÿ™ŸÑÿßÿ¥ ⁄©ÿ±ÿ≥⁄© ÿ¥ÿÆÿµ€å ÿ®ŸàŸÑÿ™€í Ÿæ€ÅŸÑ⁄ØÿßŸÖ ⁄Ü€åÿ¶ÿ± ÿßŸÑÿßÿ¶ŸÜÿ≥ ÿ±Ÿà€å€Å ⁄ØŸÜÿ™ÿß ÿßŸÇÿ™ÿØÿßÿ±ÿ≥€í Ÿæÿ¥ÿ™ ÿ±€åŸÑ€å ŸÖŸÑ⁄©€Å ÿ¨ŸÖÿ¥€åÿØ ÿØŸÖÿßÿ∫ÿß Ÿæÿ±ÿß ÿ≠ŸÖŸÑ€Å ÿß€åŸÜÿ¥ŸÜŸπ ŸÖŸÜÿµŸàÿ±⁄©ÿß ÿ®⁄æŸÑÿßŸÜÿß ÿ¨ŸÖÿßÿπÿ™ ÿ≥€åŸÜŸπ ŸÖÿ∂ÿßŸÖ€åŸÜ ÿµŸàŸÅ€Å ÿ≥ÿßŸÖÿπ€åŸÜ ÿßŸπ€å⁄ÜŸÖ€åŸÜŸπ ÿ≥€Å€å ŸÖÿ≥ÿßŸÅÿ™ ÿπŸÑÿßŸÇ€í ÿ™ÿ≠ŸÇŸÇ€å Ÿàÿ±ŸÅÿ™ Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ€åÿµÿ≠ÿßŸÅÿß ŸÅŸÑ ⁄©€åŸÖŸæ ÿ±ÿßŸàÿß ÿπÿ®Ÿàÿ±ÿß ŸÜŸÇÿµÿß⁄∫ ŸÖ€åÿ±ÿß⁄∫ ⁄©ÿ±ÿ™ ⁄©ŸÖÿ®ŸÑ ÿ®€åŸπÿ±ÿß ŸÖÿ¨€åÿ® ÿßŸÖŸÑÿß⁄© ŸæÿßŸÑÿ™€í ÿ¢Ÿæ€åÿ±€åŸπÿ±ÿ≤ ÿØ⁄æÿßÿ±ÿß ÿ∫€åÿ±ÿ∂ÿ±Ÿàÿ±ÿß ŸàŸÑ€åŸÖ€í ÿßÿπÿ≤ÿßÿ≤ ŸÖŸÖ⁄©ŸÜ ŸæŸàÿ±⁄©ÿß ÿØ€åŸà ÿßŸÜŸπÿ±ŸÖ€å⁄à€åŸπ ⁄©€åŸÖ€å⁄©ŸÑ Ÿπÿ±⁄© ÿ≥⁄©€åŸÜÿ± ÿ®Ÿπÿß ŸÑ⁄Øÿ™ÿß ⁄Øÿ¥ÿ™ ⁄Øÿ±€åŸÜ€å⁄àÿß ÿ¥€åŸÑŸπŸÜ ÿ®⁄æ€åÿ¨ÿ™€í ÿßŸÜÿØÿßÿ±ÿßÿ®ÿß ÿßŸà€åÿ≥ ŸÜŸÖÿßÿ¶ŸÜÿØ€í ŸÖÿ∫ÿ±ÿ® Ÿπ€å⁄©ÿ≥ÿ≤ €ÅŸÖÿ≥ÿß€å€Å Ÿà€Å ⁄©€Åÿ™€í ŸÖ€å⁄©ÿ≥€åŸÑŸà ÿ¢ÿ± ÿ¨ÿØÿ™ ⁄©ÿ±ÿ™€í ÿ™ŸÇÿßÿ∂ÿß ÿπŸÑÿßŸÖÿ™ ÿßŸÜ⁄àÿ≥Ÿπÿ±ÿß ÿßÿÆŸàŸÜÿØÿ≤ÿßÿØ€Å €åÿßÿØÿØÿßÿ¥ÿ™ ÿ¨ÿßŸÜÿ®ÿ± ŸÖÿπŸÜÿß ÿ¥ÿ±ÿß⁄©ÿ™ Ÿàÿ±⁄© €ÅŸÜÿ≥ÿß Ÿπ€å⁄©ÿ≥ÿ≤ ÿßÿ±ÿ¨ŸÜŸπ€åŸÜÿß ŸÖÿ¨ÿ±€í ŸÖÿπÿ±⁄©€Å ÿÆÿØÿßŸÜÿÆŸàÿßÿ≥ÿ™€Å ⁄©⁄æÿß ⁄Ø⁄æÿ±€åŸÑŸà ÿ≥ÿ®⁄©ÿØŸàÿ¥ ÿ±€åŸπ€åŸÜŸÑ ÿ≤ÿ®ÿ±ÿ≥ÿ™ÿß ŸÖ⁄©⁄æ€å ÿ¨ÿßŸÜ⁄Ü ŸÇÿ±ÿßÿ±ÿØÿßÿ± €ÅŸàÿ¥ÿßŸæ ⁄ØÿßŸÖÿ≤ŸÜ ⁄©ÿßŸÖÿ±€å⁄à\n",
              "        </div>\n",
              "        \n",
              "        <!-- Footer -->\n",
              "        <div style=\"padding: 15px 20px; background: #f0f0f0; border-top: 1px solid #e0e0e0; \n",
              "                    border-radius: 0 0 10px 10px; font-size: 13px; color: #666;\">\n",
              "            <span>Total Tokens: 300</span>\n",
              "        </div>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Plain Text Output:\n",
            "============================================================\n",
            "Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫ ÿ≤€åÿ± Ÿπÿ¥Ÿàÿ≤ ÿ¨ŸàŸÜÿ≤ ÿßŸàŸÑÿßÿØ ⁄ØŸÑŸàÿ®ŸÑ ŸÖÿ∂ŸÖÿ± ÿ¨⁄æŸπ⁄© ŸÖŸàÿßÿ≤ ⁄àÿßŸÜŸàÿß ÿ≥ÿ±Ÿàÿ±ÿ≤ ŸÜÿßÿ®ÿßŸÑÿ∫ ÿßÿ≠ÿ≥ÿßŸÜ ŸÖ⁄©ÿßŸÜÿß ŸÜŸÜ⁄Øÿ±€Åÿßÿ± ŸÜÿ¥ÿßÿ∑ ⁄©ÿßŸÑÿ¨ÿ≤ ÿ®ÿßÿ±€å⁄©€å ÿ¥€åŸæÿ±⁄à ⁄à€åŸπŸàŸÜ€åŸπÿ±ÿ≤ ⁄ØŸàŸÜÿ¨ÿ™ÿß ŸÖÿ∞ÿß⁄©ÿ±ÿßÿ™ÿß ÿß€å⁄©ÿ≥Ÿπ€åŸÜÿ¥ŸÜ ⁄Ü⁄æŸà⁄ëŸà ŸÖ€å⁄ØÿßŸàÿßŸπ ŸÅÿ±ÿßÿ∫ÿ™ ÿ™ÿ®ÿØ€åŸÑÿß ÿ≥€å⁄©ŸàŸÜÿ≥ŸÜ⁄Ø ⁄©ÿßÿ±ÿ≥€åŸà⁄© ÿ¢ÿØ€åÿßŸÑ€Å ⁄ØÿßŸÑŸÅ Ÿæ€åÿ¶ ŸÖÿ¥ÿß€ÅÿØ€Å ÿ≥ÿßÿ≤Ÿà ⁄©ÿßŸÑ€åÿ≥Ÿπÿ±ŸàŸÑ ÿÆÿ±⁄Ü€í ⁄©€åŸÖÿ±ŸàŸÜ ŸÑÿß ⁄à€åŸæÿßÿ±ŸπŸÖŸÜŸπÿ≥ ÿπÿ®ÿØÿßŸÑÿ≥ÿ™ÿßÿ± ⁄©⁄æÿ®€Å Ÿæ€å⁄à ÿ≥Ÿà⁄Üÿ™€í ŸÖ⁄Øÿ± ÿ≥€å⁄©Ÿàÿ±Ÿπÿß ŸæŸàÿ≤€åÿ¥ŸÜ ⁄Øÿß€Å ÿÆÿ®ÿ±ÿØÿßÿ± ÿßÿ≥ŸÖÿ®ŸÑÿß ⁄ØŸàŸÑ⁄à ÿ™ŸÇÿ±€åÿ±ÿ≥€í ŸÖÿ≥ÿ™ŸÇŸÑ ⁄Øÿß⁄ë€í ÿ™ÿ±ÿßŸÜ⁄à€åÿß ⁄Ø€å€í ŸÖÿ´ÿßŸÑ ÿßÿπÿ™ÿßÿ® ⁄©⁄æŸÖÿ®€í ŸÖŸÜÿµŸàÿ± Ÿàÿßÿ¶ŸÑŸÜÿ≥ ÿ≥€åŸÜ⁄Üÿ±ÿß ÿ≥⁄©ÿß ÿßŸÜ⁄Ü ŸÇ€åŸàŸÖ ÿßÿπÿ∏ŸÖÿß ÿ±€åŸÑ€åŸÅ ÿßÿ≤ÿ®⁄©ÿ≥ÿ™ÿßŸÜ ÿ®ÿ∞ÿ±€åÿπ€Å ÿßŸπ⁄æÿßŸÜŸà€í ÿßŸàŸæÿ± ⁄àÿßÿ±Ÿπ ⁄ØŸàÿ±⁄©⁄æŸæŸàÿ± ÿ®⁄æ€åÿ¨€í ŸÖÿπÿßŸÅÿß ÿ®ÿßÿ®ŸÜÿØÿß ÿ±ŸàŸπ⁄æ ÿ¥ÿßÿØÿß ŸÖŸÜÿ¥€å ⁄©ŸàŸπ ÿ¥€åÿ±Ÿà Ÿà€åŸÜŸπÿß ÿπÿ®ÿØÿßŸÑÿ±ÿ¥€åÿØ ⁄àÿßŸÑÿ±<NUM> ÿ™ÿ¥Ÿà€åÿ¥ŸÜÿß⁄© ⁄Ü⁄æŸà⁄ë ÿ∫€åÿ±ŸÖŸÇÿßŸÖÿß Ÿæ€åÿ± ŸÖÿßÿ¨ÿØ ÿ¥€åŸàÿ±ŸÑ€åŸπ ŸÜ€å⁄Ø€åŸπŸà ⁄Øÿ±ÿØÿßŸÜÿß ÿß€åŸà€åÿ¥ŸÜ ŸÖÿ≠ÿ≥ŸÜ ŸÇŸàŸÖ€åÿß ÿ≠ÿßÿ∂ÿ± ŸÖŸÜÿ∑ŸÇÿß ⁄©ŸàÿßŸÑ€åŸÅÿßÿ¶ÿß Ÿæÿ±ÿØ⁄Øÿß ÿ™Ÿàÿ¥€Å ÿπŸÑÿßŸÖÿ™ ⁄©ÿ±ŸÑÿß €ÅŸàŸÑ⁄à ÿ±ÿ≥ÿßÿ§ ŸÑ⁄ØŸàÿßÿ¶ÿß ÿ®€ÅŸÜŸàÿ¶ÿß ⁄©ŸÜŸàÿßÿ±ÿß ÿ™€ÅŸàÿßÿ± ÿ¥ÿ±ŸÖŸÜÿØ€Å ÿ®ÿßÿ¨ŸàÿØ ⁄©⁄æŸÜ⁄Ü ÿ®ÿ≥€å ŸÖ⁄Ü⁄æ Ÿπÿßÿ§ŸÜÿ≤ ÿ≥ŸÖÿ¨⁄æÿßÿ™ÿß ŸæÿßŸæŸàŸÑÿ≥Ÿπ Ÿæÿ±ŸàŸÅ ÿ®ÿßÿ¢ÿ≥ÿßŸÜÿß ÿßŸÑŸàÿ∑ŸÜÿß ÿ≥ŸÑ€åŸæŸÜ⁄Ø ÿ∏€Å€åÿ± ÿßÿ±ÿ¥ÿßÿØ ÿ¥€åŸàÿß ⁄Ü€åŸÖŸæÿ¶ŸÜ ⁄Ü€åŸÑŸÜÿ¨ ÿßŸÜŸÅÿ±ÿßÿ≥Ÿπÿ±⁄©⁄Üÿ± ⁄Üÿ±⁄Üÿß €ÅŸàŸÜ€íŸàÿßŸÑÿß ⁄Üÿ± ÿßŸÖÿ±ÿß ÿ™⁄æ€åÿ™ÿß€ÅŸÖ ÿ™ŸÖÿßÿ¥ÿßÿ¶ÿß ÿ´ÿ®Ÿàÿ™ ÿ≤⁄Ü€Å ÿ®⁄ÜÿßŸÜÿß ÿ≤€å ÿ±€åŸÜÿ¨ ÿ®⁄ë⁄æÿßÿ¶€í ŸÖ€åÿ≥ÿ≤ ÿ®ÿßÿ±ÿ¥ ÿßÿ≥ÿπÿ™ŸÖÿßŸÑ ÿ¨⁄©⁄ë ÿ™ÿ± ÿ®Ÿàÿßÿ¶ŸÑŸÜ⁄Ø Ÿæÿ±ÿßÿ¶ÿ≥ŸÜ⁄Ø ÿ≥ŸÖ€åÿ™ <NUM> ÿ™⁄æŸÜ⁄àÿ± ⁄ÜÿßŸπ ÿ™ÿ±⁄©ÿß ÿØŸÑ€ÅŸÜ ÿß€åÿ¶ÿ±⁄©ŸÖŸà⁄àŸàÿ± ÿ≥ŸÜ⁄à€å⁄©€åŸπ ⁄àÿßÿ¶ÿ±€å⁄©ŸπŸàÿ±€åŸπÿ≥ ÿ≥ÿßÿ¶ŸÜÿ≥ÿØÿßŸÜ ŸÜŸàÿ¥ ÿ¢ŸÖÿØŸàÿ±ŸÅÿ™ ÿ™ÿ¥Ÿà€åÿ¥ ÿØŸàŸÑ€ÅŸÜ ⁄Üÿßÿ¶ŸÑ⁄àÿ≥ ÿ±€åŸÅÿßÿ¶ŸÜÿ±ÿß €ÅŸàÿ¶€íÿ™⁄æ€í ÿ®€åÿØÿßÿ± ŸÖŸÜÿßÿ¶ÿß ⁄Øÿ¶€å ÿ≤ŸÖÿßŸÜ€Å ÿ®€åÿ¥ŸÖÿßÿ± ÿ¢ÿ≥ ÿÆŸÑ€å ⁄©ÿßÿ±€å⁄Øÿ± ŸÖÿ≤ÿßÿ≠ŸÖÿ™ ÿßŸàÿ®ÿßŸÖÿß ÿØÿßÿ§ÿØ ÿ∫€åÿ±ÿ≥ÿ±⁄©ÿßÿ±ÿß ⁄©Ÿàÿ±€åÿ¨ ÿ®ÿ±€í ÿ®ÿßÿ≤€åÿßÿ®ÿß Ÿà€å⁄∫ ⁄©⁄æÿßŸÜ ÿ¢ÿ≥ÿ™ÿßŸÜ€Å ŸπÿßŸÑ€åÿ±ŸÜÿ≥ ÿ®ÿ±ŸàŸÜÿßÿ¶ÿß ÿÆŸàÿ¥ÿ≠ÿßŸÑÿß ÿ±Ÿàÿß⁄∫ ÿ™⁄æÿ±ÿ≥Ÿπ ÿµÿßÿ®ŸÜ ÿßŸÜŸπ€åÿ±€åÿ¶ÿ± Ÿæ€å⁄Ü€åÿØ⁄Ø€å ŸÅÿßÿ±ŸàŸÇ ⁄à€åŸæ ÿµÿØ€Å ÿØÿ≥ÿ™ ŸÖŸÜÿµÿ® ÿß⁄Ü⁄æÿßŸÑ ⁄©€åŸÅ€åÿ™ ÿ∑Ÿà€åŸÑ ÿ™ŸÑÿßÿ¥ ⁄©ÿ±ÿ≥⁄© ÿ¥ÿÆÿµ€å ÿ®ŸàŸÑÿ™€í Ÿæ€ÅŸÑ⁄ØÿßŸÖ ⁄Ü€åÿ¶ÿ± ÿßŸÑÿßÿ¶ŸÜÿ≥ ÿ±Ÿà€å€Å ⁄ØŸÜÿ™ÿß ÿßŸÇÿ™ÿØÿßÿ±ÿ≥€í Ÿæÿ¥ÿ™ ÿ±€åŸÑ€å ŸÖŸÑ⁄©€Å ÿ¨ŸÖÿ¥€åÿØ ÿØŸÖÿßÿ∫ÿß Ÿæÿ±ÿß ÿ≠ŸÖŸÑ€Å ÿß€åŸÜÿ¥ŸÜŸπ ŸÖŸÜÿµŸàÿ±⁄©ÿß ÿ®⁄æŸÑÿßŸÜÿß ÿ¨ŸÖÿßÿπÿ™ ÿ≥€åŸÜŸπ ŸÖÿ∂ÿßŸÖ€åŸÜ ÿµŸàŸÅ€Å ÿ≥ÿßŸÖÿπ€åŸÜ ÿßŸπ€å⁄ÜŸÖ€åŸÜŸπ ÿ≥€Å€å ŸÖÿ≥ÿßŸÅÿ™ ÿπŸÑÿßŸÇ€í ÿ™ÿ≠ŸÇŸÇ€å Ÿàÿ±ŸÅÿ™ Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ€åÿµÿ≠ÿßŸÅÿß ŸÅŸÑ ⁄©€åŸÖŸæ ÿ±ÿßŸàÿß ÿπÿ®Ÿàÿ±ÿß ŸÜŸÇÿµÿß⁄∫ ŸÖ€åÿ±ÿß⁄∫ ⁄©ÿ±ÿ™ ⁄©ŸÖÿ®ŸÑ ÿ®€åŸπÿ±ÿß ŸÖÿ¨€åÿ® ÿßŸÖŸÑÿß⁄© ŸæÿßŸÑÿ™€í ÿ¢Ÿæ€åÿ±€åŸπÿ±ÿ≤ ÿØ⁄æÿßÿ±ÿß ÿ∫€åÿ±ÿ∂ÿ±Ÿàÿ±ÿß ŸàŸÑ€åŸÖ€í ÿßÿπÿ≤ÿßÿ≤ ŸÖŸÖ⁄©ŸÜ ŸæŸàÿ±⁄©ÿß ÿØ€åŸà ÿßŸÜŸπÿ±ŸÖ€å⁄à€åŸπ ⁄©€åŸÖ€å⁄©ŸÑ Ÿπÿ±⁄© ÿ≥⁄©€åŸÜÿ± ÿ®Ÿπÿß ŸÑ⁄Øÿ™ÿß ⁄Øÿ¥ÿ™ ⁄Øÿ±€åŸÜ€å⁄àÿß ÿ¥€åŸÑŸπŸÜ ÿ®⁄æ€åÿ¨ÿ™€í ÿßŸÜÿØÿßÿ±ÿßÿ®ÿß ÿßŸà€åÿ≥ ŸÜŸÖÿßÿ¶ŸÜÿØ€í ŸÖÿ∫ÿ±ÿ® Ÿπ€å⁄©ÿ≥ÿ≤ €ÅŸÖÿ≥ÿß€å€Å Ÿà€Å ⁄©€Åÿ™€í ŸÖ€å⁄©ÿ≥€åŸÑŸà ÿ¢ÿ± ÿ¨ÿØÿ™ ⁄©ÿ±ÿ™€í ÿ™ŸÇÿßÿ∂ÿß ÿπŸÑÿßŸÖÿ™ ÿßŸÜ⁄àÿ≥Ÿπÿ±ÿß ÿßÿÆŸàŸÜÿØÿ≤ÿßÿØ€Å €åÿßÿØÿØÿßÿ¥ÿ™ ÿ¨ÿßŸÜÿ®ÿ± ŸÖÿπŸÜÿß ÿ¥ÿ±ÿß⁄©ÿ™ Ÿàÿ±⁄© €ÅŸÜÿ≥ÿß Ÿπ€å⁄©ÿ≥ÿ≤ ÿßÿ±ÿ¨ŸÜŸπ€åŸÜÿß ŸÖÿ¨ÿ±€í ŸÖÿπÿ±⁄©€Å ÿÆÿØÿßŸÜÿÆŸàÿßÿ≥ÿ™€Å ⁄©⁄æÿß ⁄Ø⁄æÿ±€åŸÑŸà ÿ≥ÿ®⁄©ÿØŸàÿ¥ ÿ±€åŸπ€åŸÜŸÑ ÿ≤ÿ®ÿ±ÿ≥ÿ™ÿß ŸÖ⁄©⁄æ€å ÿ¨ÿßŸÜ⁄Ü ŸÇÿ±ÿßÿ±ÿØÿßÿ± €ÅŸàÿ¥ÿßŸæ ⁄ØÿßŸÖÿ≤ŸÜ ⁄©ÿßŸÖÿ±€å⁄à\n",
            "============================================================\n",
            "\n",
            " Statistics:\n",
            "   - Model used: Bigram Model\n",
            "   - Seed tokens: 6\n",
            "   - Total tokens generated: 300\n",
            "   - Generated tokens: 294\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_generation_interface()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AuHDd7Jra6cu",
        "outputId": "15bf98c3-41b4-4b71-ffdc-8acaffd0c797"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BBC Style Urdu News Article Generator\n",
            "============================================================\n",
            "\n",
            "Select Language Model:\n",
            "1. Bigram Model\n",
            "2. Trigram Model (with Backoff)\n",
            "\n",
            "Enter choice (1 or 2): 1\n",
            "Enter seed prompt (5‚Äì8 Urdu words): Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫\n",
            "\n",
            "Generating article...\n",
            "\n",
            "============================================================\n",
            "Article Generated Successfully!\n",
            "============================================================\n",
            "\n",
            " Article with RTL Display:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"border: 3px solid #667eea; border-radius: 12px; padding: 0; \n",
              "                margin: 25px 0; background: #fff; box-shadow: 0 8px 16px rgba(102,126,234,0.2);\">\n",
              "        \n",
              "        <!-- Header -->\n",
              "        <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
              "                    color: white; padding: 20px; border-radius: 10px 10px 0 0;\">\n",
              "            <h2 style=\"margin: 0; font-size: 26px;\">üì∞ Generated BBC Urdu Article</h2>\n",
              "            <div style=\"margin-top: 8px; font-size: 14px; opacity: 0.9;\">\n",
              "                Model: Bigram Model\n",
              "            </div>\n",
              "        </div>\n",
              "        \n",
              "        <!-- Article Content with RTL -->\n",
              "        <div style=\"direction: rtl; text-align: right; \n",
              "                    font-family: 'Jameel Noori Nastaleeq', 'Nafees Web Naskh', 'Arial', sans-serif;\n",
              "                    font-size: 20px; line-height: 2.5; padding: 30px; background: #fafbfc;\">\n",
              "            Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫ €å€Å ÿßŸà⁄©€í ÿ≥Ÿπ€å⁄à€åŸÖÿ≤ Ÿàÿ≠€åÿØ ⁄©ÿ±⁄©€í €Å⁄ëÿ™ÿßŸÑ ÿ™ŸÜÿØ ÿ¢ÿ¶ÿ≤ŸÜ ⁄Üÿßÿ±Ÿæÿßÿ¶ ÿ™ÿ®ÿß€Åÿß ÿØŸÅŸÜ ÿ™ÿßÿÆ€åÿ± Ÿæÿ±ÿ™ÿ¥ÿØÿØ ŸàÿßŸÇÿπ ÿ®⁄æ€å Ÿæÿ±⁄©ÿ™ŸÜÿß ÿ≥ÿ™⁄æÿ±ÿß ÿ®ŸÑÿßŸàÿßÿ≥ÿ∑€Å ⁄ØŸàÿ¥Ÿàÿßÿ± ÿÆŸÑÿßŸÅ ŸÅÿßÿ¶ÿ≤ ÿπÿßŸÑÿ¥€åÿßŸÜ ⁄©€ÅŸÜ ⁄©€åŸÅ€å ÿßŸÜ⁄æŸàŸÜ ÿ™ÿ±ÿßŸÜÿß⁄Ø ŸÖÿ¥ŸÜÿ≤ ÿ≠ÿßŸàÿß ŸÅÿßÿ±ÿ∫ ÿßÿ≤⁄©ŸÖ ⁄©ÿ±ÿ™€í€ÅŸàÿ¶€í ŸÑ€åÿßÿß Ÿàÿ≤ÿ±ÿß ÿßÿπŸÑÿßŸÜ ŸÅÿ±ÿ≠ÿ™ ÿ≥⁄©ÿßŸπ Ÿàÿ≠€åÿØ ⁄Ü€åŸÜŸÑ ÿßÿ¨ÿß⁄ë Ÿæÿ±ŸÖŸπ ŸÇ€åŸÖÿ™ÿß ÿ¨ŸÖ€å⁄©ÿß ÿµŸàÿ±ÿ™ÿ≠ÿßŸÑ ⁄Ü⁄æŸàŸπ €åŸàŸπ€åŸàÿ® ⁄Üÿ±⁄Üÿß ŸÖÿ≠ÿßÿµŸÑ Ÿæ€ÅŸÑ ÿ¢ÿ≤ŸÖŸàÿØ€Å ⁄©ÿ±ÿ≥⁄ÜŸÜ ÿ® Ÿæÿ™Ÿà ŸÖŸÇÿßÿ®ŸÑ ŸæÿßŸÑ€åÿ≥ÿß ÿ™ÿπŸÑ€åŸÖ ÿØ€Åÿßÿ¶ÿß ÿ¨ÿ®ÿ±ÿß €åÿÆ ÿ®ÿ±ÿ≥ÿ±ÿßŸÇÿ™ÿØÿßÿ± ÿ¢ÿ±⁄àÿ± ÿß€åÿ¨Ÿà⁄©€åÿ¥ŸÜŸÑ ÿ≠€åÿØÿ±ÿ¢ÿ®ÿßÿØ Ÿà€åÿ±ŸÜ Ÿæ€åÿ¥ÿ±ŸÅÿ™ ⁄©Ÿàÿßÿ¶ŸÅ ÿ∞ÿßÿ™ÿß ŸÖŸÜÿ∏ÿ±ŸÜÿßŸÖ€í ÿ™ÿ¨Ÿà€åÿ≤ ÿ±ŸÖ€åÿ¥ ŸÖÿπÿ∑ŸÑ <NUM>ÿ¨ŸÜÿ±ŸÑ ÿ®⁄ØŸÑ ÿ≠ÿ±ŸÖ ⁄Øÿ±ŸÖÿ¨Ÿàÿ¥ÿß ÿ¨€å⁄©Ÿπÿ≥ ÿ¢ŸÖÿØŸÜÿß ÿ≠ÿ±ÿßÿ≥ÿ™ÿß ŸæÿßŸÖ ÿ®ÿ±ÿ¢ŸÖÿØ ÿ®⁄©ŸÅ ÿßŸÜÿ≥Ÿπÿ±⁄©Ÿπÿ± ÿ≥⁄©Ÿàÿß⁄àÿ±ŸÜ ⁄©⁄æŸÑŸÜÿß ŸÜÿ∞€åÿ± ÿ≤ÿ±€åŸÜ€Å ÿß€å⁄©ÿ≥ŸÑÿ±€åŸπÿ± ÿ±Ÿà⁄àÿ≤ ⁄Ø€åŸÜ⁄ØŸÑ€åÿß Ÿæÿ±ÿßŸæÿ±Ÿπÿß ŸàŸÑ€åŸÖ€í ÿßŸÅÿ™ÿÆÿßÿ± ÿ¨⁄æÿßŸÜÿ≥€í ŸÖ€åÿ≥€åÿ¨ ŸÖÿ±ŸÖÿ™ ŸÑŸÜ⁄©ÿß ÿßÿ™ÿßÿ¥ÿß ÿπÿØÿØÿß ÿ≥ŸÖÿ¨⁄æÿ™€í ÿ¥ŸÅ€åÿπ ⁄©⁄æŸàŸÑÿß Ÿàÿ±ŸÖÿß ÿ¨€åÿ≥€íŸÖÿ±ÿ™ÿ∂ÿß ÿß€åŸπŸÖÿ≤ ÿ≠⁄©ÿßŸÖ ÿßÿπŸÑÿßŸÖ€å€Å ÿ±ÿßÿ≥ÿ™€í⁄©ÿß ŸÅÿÆÿ± ÿ≥⁄ë⁄© ⁄ØŸÖ ŸÜÿß⁄Ø ŸÜ€åÿ¥ŸÜŸÑ Ÿæÿ±ÿ≤Ÿàÿ± ŸÖÿÆŸÑŸàÿ∑ ÿØ⁄©⁄æÿßÿ™ÿß ŸÑŸÖÿ≤ ÿßŸÅÿ±ÿßÿØ ⁄©€í ŸÖÿÆÿµŸàÿµ ÿ≥ÿß⁄©⁄æ ŸàÿßŸÑŸπÿ≤ ŸÖÿßÿ±ÿ¨ŸÜ ÿ¥ÿßÿ¶ÿØ ÿ≥€åŸÜ€åŸπ ÿ≥ŸàÿØ€í ⁄ÜŸÑÿßŸÜÿß ŸÖÿπÿ∑ŸÑ ÿ≥€åŸÑŸÅ ŸÅÿ±€åŸÜ⁄àÿ¥Ÿæ ⁄Ü⁄ë⁄æ Ÿæÿ±ÿß€å⁄© ŸÜŸà⁄©ÿ±€å ÿ±€åŸÅÿßÿ¶ŸÜ ÿßŸÑÿØ€åŸÜ ŸÑÿßÿ¶ÿß ÿπÿßÿ®ÿØ ÿ¢ÿ¶€å⁄àŸÑ ⁄©ÿ±ÿ≥ŸÖÿ≥ ÿØÿ±ŸÖ€åÿßŸÜ ŸÇÿßÿ™ŸÑ€Å ÿ®ŸÜÿØ⁄æÿß ŸÇÿØÿ±€í Ÿæ€å⁄à€åÿßŸπÿ±⁄© ŸÅÿ™ŸàŸÖ ÿØ€åÿ¶€í ŸÇÿßŸÅŸÑ€Å ⁄àÿ≥⁄©ÿßÿ§ŸÜÿ≥ ⁄©ŸÖÿ±€Å ÿß€å⁄©ÿ≥Ÿæÿ±€åÿ≥ ⁄à€å⁄©ŸÑÿ¶€åÿ± ÿ®ÿ±ÿßÿ¨ŸÖÿßŸÜ ÿÆŸàÿØÿßÿ±ÿß ŸÅ€åÿ∂ ÿ™Ÿà⁄ë ÿπÿ®ÿßŸÑÿ≠ŸÜÿßŸÜ Ÿæ⁄ë⁄æ€í ŸÑÿßÿ¨Ÿàÿßÿ® ÿßŸÜŸàÿßŸÑŸàŸÖŸÜŸπ Ÿπÿ±€åÿ≥ÿ®ŸÑŸπÿß ŸÖÿ∞ÿßŸÇ ÿ≥⁄©ÿ™ €Å€åŸàÿß ŸÑÿßŸπ⁄æÿß ÿ¨ŸàŸÅÿ±ŸÜŸπ€åÿ¶ÿ± ⁄©ÿ±ÿ™€íÿßŸàÿ± ⁄©⁄æŸÑŸÜÿß ÿ≥ŸÑŸÅ ÿßŸà⁄ë⁄æÿ™€í ÿ¥ÿßŸæŸÜ⁄Ø ÿÆÿµŸàÿµÿß ŸÖŸÜÿ¨ŸÖÿØ ÿ¥⁄©ÿ≥ÿ™ Ÿæÿ≥ŸÜÿß ŸÖ€å⁄Ü ⁄à€åŸÜŸπÿ≥Ÿπ ÿ™ÿ±ÿ¨ŸÖÿßŸÜÿß ÿ®ÿ±€å⁄Ø€å⁄à Ÿæÿ±€åŸπ ÿßÿ≤ÿÆŸàÿØ ÿßŸÜÿØÿ±ÿßÿ¨ Ÿæÿ±Ÿàÿ±ÿ¥ ŸÑÿßÿ±€Åÿß ÿß€ÅŸÑ⁄©ÿßÿ± ÿ®ŸÜÿØ €ÅŸà ÿ¨ÿßÿ™ÿß €Å€í ⁄©€Å €å€Å ŸÅŸà⁄à ÿ™ŸÖ€Åÿßÿ±€í ÿßÿØÿ±⁄Øÿ±ÿØ ÿ®ÿ¨ÿßÿ™€í ŸÖÿß€Åÿß ÿßŸàÿ±⁄©ÿ≤ÿ¶ÿß ŸÖÿßŸÜÿ≥ÿ±ŸàŸàÿ± ÿ∫ÿµ€í ÿ¨⁄æŸàŸπ€í Ÿàÿ±⁄©ŸÜ⁄Ø ŸÑ⁄ëÿßÿ¶ ÿßÿØŸà€å ÿ±€Åÿßÿ¶ÿß ÿπÿßÿ¶ÿ¥€Å ÿπ€åÿ≥Ÿàÿß ÿßŸÜ⁄à€í ÿ™ŸÖŸÑ ŸÖÿ≥ÿ™ŸÅ€åÿØ ÿßŸÇÿ™ÿØÿßÿ± Ÿæÿ≥ÿ™ ÿ™ŸÖŸÜÿß ÿ≥ŸÜÿß €Åÿßÿ¶€å⁄©Ÿàÿ±Ÿπ Ÿà€åŸÑÿß⁄Ø ÿ±ÿßŸàÿß ŸÖÿ¨ÿ™ÿ®ÿß ÿ®⁄ë⁄æÿß€åÿß €Åÿ¨ÿ±ÿ™ ŸÜÿ¨⁄©ÿßÿ±ÿß ÿß⁄à€åÿ®ŸÑ ÿßŸÇÿ®ÿßŸÑ €åŸÇ€åŸÜÿß ⁄©ÿßŸÖÿ≥€åŸπÿ≥ ÿßÿ´ŸÜÿß ÿßÿ≥ÿß⁄∫ ÿ±€åŸÑ€å ŸÖŸÅÿ±Ÿàÿ± ÿ¥ÿπÿ®€í ÿßŸÇÿ™ÿØÿßÿ±ÿ≥ŸÜÿ®⁄æÿßŸÑ ⁄ÜŸÜÿßŸÜ⁄Ü€Å ÿ™ÿ±ÿßÿ¥ÿß ⁄Øÿ±ÿßŸÜÿß ÿßÿ¨ÿ™ŸÜÿßÿ® ÿßŸà⁄ë⁄æ ŸÇŸÑÿ® ÿØŸàÿ≥ÿ™ÿß ŸπŸàŸÖÿß ŸÅ€å⁄©Ÿπ €ÅŸÖÿØÿ±ÿØÿß Ÿπ⁄æÿß⁄∫ ŸÅŸÑÿßÿ¶ŸÜ⁄Ø Ÿæ⁄æÿ≥ŸÑ <NUM>ŸÖ Ÿπ€å⁄©ÿ≥ÿßÿ≥ ⁄àÿ±ÿßŸÖ€Å ÿÆÿ∑ÿ±€Å ŸàŸÅÿßÿØÿßÿ±ÿß ŸÜŸÇŸàŸÑ ÿ™ÿ≥ŸÜ€åŸÖ ŸÖ⁄©ÿß Ÿπ⁄©⁄ë€í ŸÑÿßÿ¶ÿ≥ŸÜÿ≥ŸÜ⁄Ø ÿ®ŸÜ⁄Ü ÿ≥ÿ™ŸÑÿ¨ ⁄©ŸÖŸæŸÑÿßÿ¶ŸÜŸπ ÿ™ÿπŸÖ€åÿ±ÿß ÿ≤ÿßŸà€å€Å ŸÜ€åŸà€åÿßÿ±⁄© ÿßÿ±ÿßÿØ ÿ≥Ÿπÿ±€åŸπ€åÿ¨⁄© ⁄àŸæŸÑŸàŸÖ€åŸπ ŸÑÿßÿ™ ÿßÿÆÿ∞ ÿ®€åÿ±⁄© ÿ≥ÿ®ÿ≤ ŸÖ€åÿßŸÖÿß ÿ≥ÿß€Å€åŸàÿßŸÑ ÿ¨€åÿ¥ ŸÅ€åÿ≥ Ÿæÿ≥€å ÿ≥Ÿàÿ™€í ÿßÿ¥ÿ±ÿßŸÅ€å€Å ŸÜÿ¶€å ⁄©ÿ±ÿß€Å €å€Åÿß⁄∫ ŸÇÿßÿ≤ŸÇÿ≥ÿ™ÿßŸÜ ÿ®ÿßÿ±ÿ®€åŸà⁄àÿß ŸÑ€åÿßÿß ŸÖŸÜÿπŸÇÿØ€Å ⁄©ÿß⁄©ÿ≥ ŸÖ€å⁄ØŸàÿ¶€å ÿØ⁄æŸÜÿØŸÑÿß ÿß€åÿ¨ŸÜÿ≥€å ⁄©ÿ±ÿ® ÿ±€å⁄©ÿßÿ±⁄à⁄à ⁄©ÿ¥€å ⁄àÿ±€åŸÜ ÿ¢⁄ØŸÖŸÜŸπ ŸÜŸÇÿ∑€í ÿßŸàÿ±€å€Å ⁄©ŸÖÿ±€Å Ÿæ⁄æ€åŸÑÿßÿ™ÿß ÿ¥⁄©ÿß€å ŸæŸàÿ±Ÿπ ⁄©ÿ±ÿ®⁄æÿß⁄Ø ÿ±Ÿæÿ±\n",
              "        </div>\n",
              "        \n",
              "        <!-- Footer -->\n",
              "        <div style=\"padding: 15px 20px; background: #f0f0f0; border-top: 1px solid #e0e0e0; \n",
              "                    border-radius: 0 0 10px 10px; font-size: 13px; color: #666;\">\n",
              "            <span>Total Tokens: 300</span>\n",
              "        </div>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Plain Text Output:\n",
            "============================================================\n",
            "Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫ €å€Å ÿßŸà⁄©€í ÿ≥Ÿπ€å⁄à€åŸÖÿ≤ Ÿàÿ≠€åÿØ ⁄©ÿ±⁄©€í €Å⁄ëÿ™ÿßŸÑ ÿ™ŸÜÿØ ÿ¢ÿ¶ÿ≤ŸÜ ⁄Üÿßÿ±Ÿæÿßÿ¶ ÿ™ÿ®ÿß€Åÿß ÿØŸÅŸÜ ÿ™ÿßÿÆ€åÿ± Ÿæÿ±ÿ™ÿ¥ÿØÿØ ŸàÿßŸÇÿπ ÿ®⁄æ€å Ÿæÿ±⁄©ÿ™ŸÜÿß ÿ≥ÿ™⁄æÿ±ÿß ÿ®ŸÑÿßŸàÿßÿ≥ÿ∑€Å ⁄ØŸàÿ¥Ÿàÿßÿ± ÿÆŸÑÿßŸÅ ŸÅÿßÿ¶ÿ≤ ÿπÿßŸÑÿ¥€åÿßŸÜ ⁄©€ÅŸÜ ⁄©€åŸÅ€å ÿßŸÜ⁄æŸàŸÜ ÿ™ÿ±ÿßŸÜÿß⁄Ø ŸÖÿ¥ŸÜÿ≤ ÿ≠ÿßŸàÿß ŸÅÿßÿ±ÿ∫ ÿßÿ≤⁄©ŸÖ ⁄©ÿ±ÿ™€í€ÅŸàÿ¶€í ŸÑ€åÿßÿß Ÿàÿ≤ÿ±ÿß ÿßÿπŸÑÿßŸÜ ŸÅÿ±ÿ≠ÿ™ ÿ≥⁄©ÿßŸπ Ÿàÿ≠€åÿØ ⁄Ü€åŸÜŸÑ ÿßÿ¨ÿß⁄ë Ÿæÿ±ŸÖŸπ ŸÇ€åŸÖÿ™ÿß ÿ¨ŸÖ€å⁄©ÿß ÿµŸàÿ±ÿ™ÿ≠ÿßŸÑ ⁄Ü⁄æŸàŸπ €åŸàŸπ€åŸàÿ® ⁄Üÿ±⁄Üÿß ŸÖÿ≠ÿßÿµŸÑ Ÿæ€ÅŸÑ ÿ¢ÿ≤ŸÖŸàÿØ€Å ⁄©ÿ±ÿ≥⁄ÜŸÜ ÿ® Ÿæÿ™Ÿà ŸÖŸÇÿßÿ®ŸÑ ŸæÿßŸÑ€åÿ≥ÿß ÿ™ÿπŸÑ€åŸÖ ÿØ€Åÿßÿ¶ÿß ÿ¨ÿ®ÿ±ÿß €åÿÆ ÿ®ÿ±ÿ≥ÿ±ÿßŸÇÿ™ÿØÿßÿ± ÿ¢ÿ±⁄àÿ± ÿß€åÿ¨Ÿà⁄©€åÿ¥ŸÜŸÑ ÿ≠€åÿØÿ±ÿ¢ÿ®ÿßÿØ Ÿà€åÿ±ŸÜ Ÿæ€åÿ¥ÿ±ŸÅÿ™ ⁄©Ÿàÿßÿ¶ŸÅ ÿ∞ÿßÿ™ÿß ŸÖŸÜÿ∏ÿ±ŸÜÿßŸÖ€í ÿ™ÿ¨Ÿà€åÿ≤ ÿ±ŸÖ€åÿ¥ ŸÖÿπÿ∑ŸÑ <NUM>ÿ¨ŸÜÿ±ŸÑ ÿ®⁄ØŸÑ ÿ≠ÿ±ŸÖ ⁄Øÿ±ŸÖÿ¨Ÿàÿ¥ÿß ÿ¨€å⁄©Ÿπÿ≥ ÿ¢ŸÖÿØŸÜÿß ÿ≠ÿ±ÿßÿ≥ÿ™ÿß ŸæÿßŸÖ ÿ®ÿ±ÿ¢ŸÖÿØ ÿ®⁄©ŸÅ ÿßŸÜÿ≥Ÿπÿ±⁄©Ÿπÿ± ÿ≥⁄©Ÿàÿß⁄àÿ±ŸÜ ⁄©⁄æŸÑŸÜÿß ŸÜÿ∞€åÿ± ÿ≤ÿ±€åŸÜ€Å ÿß€å⁄©ÿ≥ŸÑÿ±€åŸπÿ± ÿ±Ÿà⁄àÿ≤ ⁄Ø€åŸÜ⁄ØŸÑ€åÿß Ÿæÿ±ÿßŸæÿ±Ÿπÿß ŸàŸÑ€åŸÖ€í ÿßŸÅÿ™ÿÆÿßÿ± ÿ¨⁄æÿßŸÜÿ≥€í ŸÖ€åÿ≥€åÿ¨ ŸÖÿ±ŸÖÿ™ ŸÑŸÜ⁄©ÿß ÿßÿ™ÿßÿ¥ÿß ÿπÿØÿØÿß ÿ≥ŸÖÿ¨⁄æÿ™€í ÿ¥ŸÅ€åÿπ ⁄©⁄æŸàŸÑÿß Ÿàÿ±ŸÖÿß ÿ¨€åÿ≥€íŸÖÿ±ÿ™ÿ∂ÿß ÿß€åŸπŸÖÿ≤ ÿ≠⁄©ÿßŸÖ ÿßÿπŸÑÿßŸÖ€å€Å ÿ±ÿßÿ≥ÿ™€í⁄©ÿß ŸÅÿÆÿ± ÿ≥⁄ë⁄© ⁄ØŸÖ ŸÜÿß⁄Ø ŸÜ€åÿ¥ŸÜŸÑ Ÿæÿ±ÿ≤Ÿàÿ± ŸÖÿÆŸÑŸàÿ∑ ÿØ⁄©⁄æÿßÿ™ÿß ŸÑŸÖÿ≤ ÿßŸÅÿ±ÿßÿØ ⁄©€í ŸÖÿÆÿµŸàÿµ ÿ≥ÿß⁄©⁄æ ŸàÿßŸÑŸπÿ≤ ŸÖÿßÿ±ÿ¨ŸÜ ÿ¥ÿßÿ¶ÿØ ÿ≥€åŸÜ€åŸπ ÿ≥ŸàÿØ€í ⁄ÜŸÑÿßŸÜÿß ŸÖÿπÿ∑ŸÑ ÿ≥€åŸÑŸÅ ŸÅÿ±€åŸÜ⁄àÿ¥Ÿæ ⁄Ü⁄ë⁄æ Ÿæÿ±ÿß€å⁄© ŸÜŸà⁄©ÿ±€å ÿ±€åŸÅÿßÿ¶ŸÜ ÿßŸÑÿØ€åŸÜ ŸÑÿßÿ¶ÿß ÿπÿßÿ®ÿØ ÿ¢ÿ¶€å⁄àŸÑ ⁄©ÿ±ÿ≥ŸÖÿ≥ ÿØÿ±ŸÖ€åÿßŸÜ ŸÇÿßÿ™ŸÑ€Å ÿ®ŸÜÿØ⁄æÿß ŸÇÿØÿ±€í Ÿæ€å⁄à€åÿßŸπÿ±⁄© ŸÅÿ™ŸàŸÖ ÿØ€åÿ¶€í ŸÇÿßŸÅŸÑ€Å ⁄àÿ≥⁄©ÿßÿ§ŸÜÿ≥ ⁄©ŸÖÿ±€Å ÿß€å⁄©ÿ≥Ÿæÿ±€åÿ≥ ⁄à€å⁄©ŸÑÿ¶€åÿ± ÿ®ÿ±ÿßÿ¨ŸÖÿßŸÜ ÿÆŸàÿØÿßÿ±ÿß ŸÅ€åÿ∂ ÿ™Ÿà⁄ë ÿπÿ®ÿßŸÑÿ≠ŸÜÿßŸÜ Ÿæ⁄ë⁄æ€í ŸÑÿßÿ¨Ÿàÿßÿ® ÿßŸÜŸàÿßŸÑŸàŸÖŸÜŸπ Ÿπÿ±€åÿ≥ÿ®ŸÑŸπÿß ŸÖÿ∞ÿßŸÇ ÿ≥⁄©ÿ™ €Å€åŸàÿß ŸÑÿßŸπ⁄æÿß ÿ¨ŸàŸÅÿ±ŸÜŸπ€åÿ¶ÿ± ⁄©ÿ±ÿ™€íÿßŸàÿ± ⁄©⁄æŸÑŸÜÿß ÿ≥ŸÑŸÅ ÿßŸà⁄ë⁄æÿ™€í ÿ¥ÿßŸæŸÜ⁄Ø ÿÆÿµŸàÿµÿß ŸÖŸÜÿ¨ŸÖÿØ ÿ¥⁄©ÿ≥ÿ™ Ÿæÿ≥ŸÜÿß ŸÖ€å⁄Ü ⁄à€åŸÜŸπÿ≥Ÿπ ÿ™ÿ±ÿ¨ŸÖÿßŸÜÿß ÿ®ÿ±€å⁄Ø€å⁄à Ÿæÿ±€åŸπ ÿßÿ≤ÿÆŸàÿØ ÿßŸÜÿØÿ±ÿßÿ¨ Ÿæÿ±Ÿàÿ±ÿ¥ ŸÑÿßÿ±€Åÿß ÿß€ÅŸÑ⁄©ÿßÿ± ÿ®ŸÜÿØ €ÅŸà ÿ¨ÿßÿ™ÿß €Å€í ⁄©€Å €å€Å ŸÅŸà⁄à ÿ™ŸÖ€Åÿßÿ±€í ÿßÿØÿ±⁄Øÿ±ÿØ ÿ®ÿ¨ÿßÿ™€í ŸÖÿß€Åÿß ÿßŸàÿ±⁄©ÿ≤ÿ¶ÿß ŸÖÿßŸÜÿ≥ÿ±ŸàŸàÿ± ÿ∫ÿµ€í ÿ¨⁄æŸàŸπ€í Ÿàÿ±⁄©ŸÜ⁄Ø ŸÑ⁄ëÿßÿ¶ ÿßÿØŸà€å ÿ±€Åÿßÿ¶ÿß ÿπÿßÿ¶ÿ¥€Å ÿπ€åÿ≥Ÿàÿß ÿßŸÜ⁄à€í ÿ™ŸÖŸÑ ŸÖÿ≥ÿ™ŸÅ€åÿØ ÿßŸÇÿ™ÿØÿßÿ± Ÿæÿ≥ÿ™ ÿ™ŸÖŸÜÿß ÿ≥ŸÜÿß €Åÿßÿ¶€å⁄©Ÿàÿ±Ÿπ Ÿà€åŸÑÿß⁄Ø ÿ±ÿßŸàÿß ŸÖÿ¨ÿ™ÿ®ÿß ÿ®⁄ë⁄æÿß€åÿß €Åÿ¨ÿ±ÿ™ ŸÜÿ¨⁄©ÿßÿ±ÿß ÿß⁄à€åÿ®ŸÑ ÿßŸÇÿ®ÿßŸÑ €åŸÇ€åŸÜÿß ⁄©ÿßŸÖÿ≥€åŸπÿ≥ ÿßÿ´ŸÜÿß ÿßÿ≥ÿß⁄∫ ÿ±€åŸÑ€å ŸÖŸÅÿ±Ÿàÿ± ÿ¥ÿπÿ®€í ÿßŸÇÿ™ÿØÿßÿ±ÿ≥ŸÜÿ®⁄æÿßŸÑ ⁄ÜŸÜÿßŸÜ⁄Ü€Å ÿ™ÿ±ÿßÿ¥ÿß ⁄Øÿ±ÿßŸÜÿß ÿßÿ¨ÿ™ŸÜÿßÿ® ÿßŸà⁄ë⁄æ ŸÇŸÑÿ® ÿØŸàÿ≥ÿ™ÿß ŸπŸàŸÖÿß ŸÅ€å⁄©Ÿπ €ÅŸÖÿØÿ±ÿØÿß Ÿπ⁄æÿß⁄∫ ŸÅŸÑÿßÿ¶ŸÜ⁄Ø Ÿæ⁄æÿ≥ŸÑ <NUM>ŸÖ Ÿπ€å⁄©ÿ≥ÿßÿ≥ ⁄àÿ±ÿßŸÖ€Å ÿÆÿ∑ÿ±€Å ŸàŸÅÿßÿØÿßÿ±ÿß ŸÜŸÇŸàŸÑ ÿ™ÿ≥ŸÜ€åŸÖ ŸÖ⁄©ÿß Ÿπ⁄©⁄ë€í ŸÑÿßÿ¶ÿ≥ŸÜÿ≥ŸÜ⁄Ø ÿ®ŸÜ⁄Ü ÿ≥ÿ™ŸÑÿ¨ ⁄©ŸÖŸæŸÑÿßÿ¶ŸÜŸπ ÿ™ÿπŸÖ€åÿ±ÿß ÿ≤ÿßŸà€å€Å ŸÜ€åŸà€åÿßÿ±⁄© ÿßÿ±ÿßÿØ ÿ≥Ÿπÿ±€åŸπ€åÿ¨⁄© ⁄àŸæŸÑŸàŸÖ€åŸπ ŸÑÿßÿ™ ÿßÿÆÿ∞ ÿ®€åÿ±⁄© ÿ≥ÿ®ÿ≤ ŸÖ€åÿßŸÖÿß ÿ≥ÿß€Å€åŸàÿßŸÑ ÿ¨€åÿ¥ ŸÅ€åÿ≥ Ÿæÿ≥€å ÿ≥Ÿàÿ™€í ÿßÿ¥ÿ±ÿßŸÅ€å€Å ŸÜÿ¶€å ⁄©ÿ±ÿß€Å €å€Åÿß⁄∫ ŸÇÿßÿ≤ŸÇÿ≥ÿ™ÿßŸÜ ÿ®ÿßÿ±ÿ®€åŸà⁄àÿß ŸÑ€åÿßÿß ŸÖŸÜÿπŸÇÿØ€Å ⁄©ÿß⁄©ÿ≥ ŸÖ€å⁄ØŸàÿ¶€å ÿØ⁄æŸÜÿØŸÑÿß ÿß€åÿ¨ŸÜÿ≥€å ⁄©ÿ±ÿ® ÿ±€å⁄©ÿßÿ±⁄à⁄à ⁄©ÿ¥€å ⁄àÿ±€åŸÜ ÿ¢⁄ØŸÖŸÜŸπ ŸÜŸÇÿ∑€í ÿßŸàÿ±€å€Å ⁄©ŸÖÿ±€Å Ÿæ⁄æ€åŸÑÿßÿ™ÿß ÿ¥⁄©ÿß€å ŸæŸàÿ±Ÿπ ⁄©ÿ±ÿ®⁄æÿß⁄Ø ÿ±Ÿæÿ±\n",
            "============================================================\n",
            "\n",
            " Statistics:\n",
            "   - Model used: Bigram Model\n",
            "   - Seed tokens: 6\n",
            "   - Total tokens generated: 300\n",
            "   - Generated tokens: 294\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-T2rEcixeoXu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIGRAM MODEL ‚Äì ARTICLE GENERATION**"
      ],
      "metadata": {
        "id": "_FcvmFpUyBcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_bigram_article(seed_prompt,\n",
        "                            min_words=200,\n",
        "                            max_words=300,\n",
        "                            min_sentences=5):\n",
        "\n",
        "    seed_tokens = seed_prompt.split()\n",
        "\n",
        "    if len(seed_tokens) < 5 or len(seed_tokens) > 8:\n",
        "        print(\"Seed must contain 5‚Äì8 Urdu words.\")\n",
        "        return None\n",
        "\n",
        "    generated = seed_tokens.copy()\n",
        "    sentence_count = 0\n",
        "\n",
        "    while len(generated) < max_words:\n",
        "\n",
        "        last_word = generated[-1]\n",
        "\n",
        "        if last_word in bigram_model.bigram_counts:\n",
        "            dist = bigram_model.get_next_word_probabilities(last_word)\n",
        "            words = list(dist.keys())\n",
        "            probs = list(dist.values())\n",
        "\n",
        "            total = sum(probs)\n",
        "            if total > 0:\n",
        "                probs = [p/total for p in probs]\n",
        "                next_word = random.choices(words, weights=probs, k=1)[0]\n",
        "            else:\n",
        "                next_word = random.choice(list(unigram_model.vocabulary))\n",
        "        else:\n",
        "            next_word = random.choice(list(unigram_model.vocabulary))\n",
        "\n",
        "        generated.append(next_word)\n",
        "\n",
        "        if next_word == \"€î\":\n",
        "            sentence_count += 1\n",
        "\n",
        "        if len(generated) >= min_words and sentence_count >= min_sentences:\n",
        "            break\n",
        "\n",
        "    if generated[-1] != \"€î\":\n",
        "        generated.append(\"€î\")\n",
        "\n",
        "    return \" \".join(generated[:max_words])\n",
        "\n",
        "\n",
        "\n",
        "seed = \"Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫\"\n",
        "\n",
        "for i in range(1, 4):\n",
        "    article = generate_bigram_article(seed)\n",
        "    print(f\"\\nBigram Article {i}\")\n",
        "    print(\"-\"*60)\n",
        "    print(article)\n",
        "    print(f\"\\nWords: {len(article.split())} | Sentences: {article.count('€î')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eqqbT2Ix8xP",
        "outputId": "3933c531-f402-4f95-e164-4fb5ef304030"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigram Article 1\n",
            "------------------------------------------------------------\n",
            "Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫ ŸÖ⁄©ŸÖŸÑ ÿØŸà⁄Üÿßÿ± ⁄à€åŸÖŸà ÿßŸæÿ±€åÿ¥ŸÜŸÑ ⁄ØŸàÿ±Ÿæÿ≥ ŸàÿßŸÑÿ≥ ŸÜ€åŸà€å⁄Ø€åÿ¥ŸÜŸÑ ŸÇÿ≥ŸÖÿ™ÿß ŸÖÿ¨ÿ±ŸÖÿßŸÜ ÿ™ÿ≠⁄©ŸÖÿßŸÜ€Å ÿ≥ŸàŸÜŸæ€í ŸÑ€åÿßÿ±Ÿπÿ±ÿß ÿß⁄©ÿ≥ÿß€åÿß ŸÜ€å⁄Üÿ± ÿ∫€åÿ±ÿ±Ÿàÿß€åÿ™ÿß ⁄©⁄æ€åŸÜ⁄Üÿß Ÿæ⁄©ŸÜ⁄Ø ŸÜÿ∏€åÿ± Ÿæ€åÿØÿßŸàÿ±ÿß Ÿπ€åÿ±€åÿßŸÜ ŸàÿßŸÑ€Å ÿßÿ≤ŸÖ ÿ´ÿßÿ®ÿ™ ÿ¥ŸÖÿßÿ¶ŸÑ€Å Ÿπÿ±€åŸÜ⁄©€åŸàŸÑÿßÿ¶ÿ≤ÿ± €å⁄©ÿ≥ÿßŸÜ€åÿ™ ÿ¨€åŸàŸÑÿ±ÿß ÿ≥€åŸπ ÿß€ÅÿØÿßŸÅ ŸàŸπ⁄©ŸàŸÅ ⁄©⁄æÿßÿ™€í ÿ±⁄©ŸÜ€åÿ™ ŸÅÿßÿ±ÿ≥ ÿ¢ÿØ⁄æ€í ÿ≥ÿßÿ≤Ÿàÿ≥ÿßŸÖÿßŸÜ ÿ≤ÿ±ŸÇ ÿßŸÜŸπÿ±ŸÜ€åÿ¥ŸÜŸÑ ÿ≥€å⁄©€åÿ±Ÿπÿ±ÿß ⁄©€åŸÖÿßÿ¶ÿß ÿ±ŸÅÿ™ÿßÿ± ÿ≥Ÿà€Å ÿ®€åŸπ€å €ÅŸà ⁄à€åŸÑÿ±ÿ≤ ÿßŸà⁄Ü ŸÜŸàÿπŸÖÿ± ⁄©ÿßŸÜÿ≥Ÿπ€åÿ®ŸÑ ÿ¨⁄æÿßÿ§ ÿ¨⁄æŸàŸπ ⁄©Ÿàÿßÿ±Ÿπÿ± Ÿæÿ±ÿß ŸÅŸÑŸÖ ÿßÿ¨ÿ™ŸÖÿßÿπ Ÿæ⁄Ü⁄æŸÑÿß ÿØÿßŸÜÿ¥Ÿàÿ± ⁄©ÿßÿ±⁄à€åŸÜ ÿ®⁄©⁄æÿ± ŸÖÿ±⁄©ŸÜŸπÿßÿ¶ŸÑ ÿßŸÜÿ¥Ÿàÿ±⁄à €Å€åŸÑŸÖŸπ ⁄©Ÿàÿ±⁄© ÿ¥ÿπÿ® ⁄©⁄æÿßŸÜÿß Ÿæÿ¥ÿ™ŸàŸÜÿÆŸàÿß ÿ¢ÿ¥ŸÜÿßÿ¶ÿß ÿ∂ŸÑÿπ€í Ÿπÿ±ÿßŸÜÿ≥ŸæŸàÿ±Ÿπ ÿ®ÿ±€åÿ≥ÿ≤ ŸÜŸÇÿ∑€Å ÿ®ŸÑŸàÿßÿ≥ÿ∑€Å ŸÖÿ±€åÿßŸÜÿß ÿ®⁄æÿß⁄Øÿß ŸÖÿßÿ±ÿ¥ŸÑ ⁄òÿßŸÑ€Å ÿßÿπÿßŸÜÿ™ ŸÖÿßÿÆŸàÿ∞ ÿ®ŸÜÿ¨ÿ± ŸÅÿ±€å⁄Øÿ±€åŸÜÿ≥ ÿßŸàÿ± ÿ™ÿπŸÖ€åÿ±ÿßÿ™ÿß Ÿπ€å⁄©ÿ≥ ŸÅÿ∂ÿßÿ§⁄∫ ÿØ⁄©⁄æÿßÿ™ÿß ⁄©ÿßÿ®ŸÑ ⁄àÿ¨€å ŸÖÿß€Åÿ±ŸÖÿ≠ŸÖÿØ ÿ™ÿ±ÿ∫€åÿ® ÿØŸàÿ≥ÿ™ŸÖ ÿÆÿßŸÜ€Å ÿ±⁄©⁄æÿ¥ÿß €åŸàÿ±€åŸÜ€åŸÖ Ÿæÿ±ÿØ⁄Øÿß ÿ±€åÿ≥€å⁄©Ÿà ÿ®⁄æŸàÿ¥ŸÜ ÿ∏ŸÅÿ± €Å€åŸÜ⁄Øÿ± ŸÜÿß€Å ⁄òÿ±ŸÅ ÿßŸÜÿ≥Ÿπÿß ÿ≤ŸÜ⁄Ø ÿ≥€å<NUM> ÿ±€ÅŸÜŸÖÿßÿ§⁄∫ ŸÅ€åÿ±Ÿàÿ≤ŸàÿßŸÑÿß ÿ™ŸÖÿØŸÜ ÿ¢ÿ¨ÿßÿ™ÿß ÿØŸÑ€å€Å ÿ¨⁄æÿß⁄ë€å ÿµŸÜÿπÿ™ÿß ŸÖÿ≠ÿ™ÿßÿ¨ ÿ®ÿßÿ±ÿ®€åŸà⁄àÿß ÿ≠ÿßÿ∂ÿ±€Å ÿßŸàÿ±⁄©ÿßŸÜ⁄Øÿ±€åÿ≥ ÿÆŸàÿßÿ®€åÿØ€Å ÿ™⁄æ€åŸÑ ŸÑÿ®ÿ±€åÿ¥ŸÜ ŸÑ€å⁄Øÿß ÿ¢ŸÜ€å ŸÖÿπÿß€ÅÿØ€í ÿ≥ŸÜÿØ⁄æÿß ÿ®ÿßÿ≤ ŸÜ€åŸπ €ÅŸàÿ™ÿß ŸæŸÑ⁄Ø ⁄©€åŸÖ€å⁄©ŸÑ ⁄©€åÿ¥€åÿ¶ÿ± ÿß⁄©ÿ´ÿ±€åÿ™ ŸÑ€åÿßÿ±ÿß ŸÅÿ™Ÿàÿß ÿ®ÿ∑ÿß€Åÿ± ÿ™⁄æÿ±ÿß Ÿπÿ±ÿßŸÜÿ≤€å⁄©ÿ¥ŸÜ ÿßŸÜŸÜ⁄Ø ÿ®⁄æÿßÿ¶€åŸà ŸÑ⁄ë⁄© ÿ∫€åÿ±ÿ≥€åÿßÿ≥ÿß ⁄Ü⁄æŸÑŸÜ€å ÿßŸÜ⁄ØŸÑÿ¥⁄∫ ŸÅŸàÿ¨ÿØÿßÿ±ÿß ⁄©ÿß⁄©ÿßÿÆ€åŸÑ ÿ≥ÿ±ÿ∑ÿßŸÜ ÿßŸÜÿØÿ±ÿßŸÜÿØÿ± ŸÑÿßÿπŸÑŸÖ ŸÖ⁄©ÿßŸÑŸÖ€Å ÿ±ÿ≥ÿØ ÿß⁄©ÿßÿ¶ÿß ÿ¢ÿ±Ÿπ ŸÖÿ≥⁄©ÿ±ÿßÿ™ÿß ⁄Øÿßÿ±⁄à€åÿ¶ŸÜ ÿ™ŸÇÿ±€åÿ®ÿßÿ™ÿß ÿßŸÜ⁄à€åŸæ€åŸÜ⁄àŸÜÿ≥ ⁄ÜŸπÿßŸÜ ÿ≥⁄©€åŸÜ ÿ¨€åŸÖÿ±ÿ≤ ÿßÿÆÿ™ÿ™ÿßŸÖ ŸÖ⁄Üÿß€åÿß ŸÖ⁄©⁄ëÿß ÿ±⁄©⁄æÿ™€í ÿßÿ´ÿßÿ´€Å ÿ¶Ÿπ€Å ⁄©ŸàŸπ€åŸàÿ¶ÿ± ÿØÿßŸÜÿ¥ Ÿà€å⁄òŸÜ ÿØ€åÿ™Ÿàÿß ÿ¢Ÿæÿ±€åÿ¥ŸÜŸÑ ⁄Ü⁄©€å ⁄©ÿ®€åÿ± Ÿæÿ±Ÿàÿ±ÿ¥ Ÿæÿßÿ™ÿß ÿ®ŸπŸàÿ± ÿ≤€å⁄à ÿßŸÜÿ¥ÿßŸÑ ÿ±ÿßÿ¶Ÿπ ⁄Ü⁄æŸàŸπ ÿ®ÿØŸÑ€í ÿ®€å⁄Ø ÿ¢ŸÑŸàÿØ⁄Øÿß ÿ≥ÿßÿ¶Ÿπÿß€å⁄©ÿ≥Ÿæÿ± Ÿπÿ±€åÿ¨⁄àÿß ŸÖŸÅÿßÿØ ŸÖ€å⁄∫ ÿßŸÜ ⁄©€í ÿßÿØÿ±⁄Øÿ±ÿØ ⁄©⁄æŸÑÿß⁄ë€å ÿ®ÿØÿ± ŸÖÿπ€åÿßÿ± ÿ≥ÿ±ÿØÿß ÿ∂€åÿßÿßŸÑÿ≠ÿ≥ŸÜ ÿ≥€åÿ≥ŸÜÿß ÿØŸà€Åÿ±ÿß ÿ®€ÅÿßŸÖÿßÿ≥ ⁄©ŸÜÿ≥ŸÑŸπŸÜŸπ ÿ®ÿ¨ ŸÖÿπÿßÿ¥ ÿ∞ÿ±ÿßÿ¶ÿπ ÿ∑ŸàÿßŸÑÿ™ ÿ®ŸÑŸÜÿØ ÿ™⁄æ€åÿ¨ŸÜ ⁄©ŸÖÿßÿ§⁄∫ ÿ®€åŸÅ ÿØÿ®ÿØÿ®€Å ÿ≠ÿ±€åÿ™ ÿßÿ¥ÿ™€åÿßŸÇ ÿÆŸÑÿßÿµÿß ŸÜÿ®⁄æÿß ⁄ÜŸπ⁄æ€Å ÿßŸÜŸπÿ±€åŸà ÿßŸÜ€ÅÿØÿßŸÖ ⁄©€íŸÑ€å€í ŸÜÿ≤ÿØ ŸÑŸæ€åŸπ ŸÅÿ¥ ŸÖÿ´ÿßŸÑ ÿ®€Åÿ±ÿ≠ÿßŸÑ ÿØŸàŸÑ€ÅŸÜ ÿØÿßŸÜÿß ÿ®ÿØÿÆÿ¥ÿßŸÜÿß ÿ±⁄©⁄æ€í ŸÖ€åÿ≥ÿ¨ÿ≤ ÿ≥ŸÜÿ™€í €ÅŸàŸÑ ÿ®ÿ±ÿ∑ÿ±ŸÅ ŸπŸàÿ±ŸÜÿßŸÖŸÜŸπ ÿ∫€åÿ±ŸÇÿßŸÜŸàŸÜÿß ŸÑ⁄ëÿßÿ¶ÿß ÿ≠Ÿàÿßÿ≥ ÿ™ÿπŸÑ€åŸÖÿß ÿ¢ÿ¶ÿ≥⁄©ÿ±€åŸÖ ÿ±ŸÅÿßŸÇÿ™ ÿ±ŸÅ€åŸÇ ÿØ⁄ØŸÜÿß ŸÜŸÅŸàÿ∞ ŸÜÿ¥€åŸÜ ŸæŸπ⁄æÿßŸÜ €ÅŸàŸÑÿ≤ ÿßŸÜÿßÿ∑ŸàŸÑŸà ŸÇŸàÿßŸÜ€åŸÜ Ÿæ€åŸπÿ±ŸÜ ŸÅ€åŸàÿ≤ ⁄ØÿØ⁄æ ŸÇÿßÿ¶ÿØ ŸàŸàŸÖŸÜ €ÅŸàÿßÿ¶ÿß ÿ≥Ÿàÿ¨ŸÜ ÿ±€å⁄©ÿßÿ±⁄àÿ≤ ÿ®ÿ≠ÿßŸÑÿß €ÅŸàŸÑ⁄àÿ± ⁄ÜŸàÿßÿ¶ÿ≥ ÿ™ÿßŸà€í ⁄Øÿ±ÿßŸÅ⁄©ÿ≥ ÿ±ÿ≥ŸàŸÑÿß ÿ´ÿ®Ÿàÿ™ ⁄©ÿßŸÜÿß ŸàÿßŸÑ€ÅÿßŸÜ€Å ÿ¨ŸÑÿ™ÿß ⁄©ÿ±€å⁄Ø ⁄©ÿßŸπ€í ÿ±€å⁄©ÿßÿ±⁄à⁄à ÿ¥ÿß€åÿØ ÿ¢ÿ™ÿ¥ ⁄©ŸÜÿ≥Ÿàÿ±ÿ¥€åÿ¶ŸÖÿ≤ ⁄©€åÿ≥ÿß ÿßŸÖÿØÿßÿØÿß ÿ®ŸàŸÑÿß ÿ®⁄æÿ¨Ÿàÿß€åÿß ŸÑÿ¥⁄©ÿ± Ÿæÿ±ÿßÿ≥€å⁄©ŸàŸπÿ± ŸÖÿ≠ÿ™ÿßÿ∑ ÿ≥ŸÖÿ¨⁄æ€í ŸÅÿ±€å⁄Ø€åŸπ ⁄©€åÿßÿ¨ÿ≥ ÿ∞ÿßÿ™ ÿßŸàŸÑÿßÿØ ⁄©ÿ±Ÿàÿßÿ¶€í Ÿæ€åÿßÿ≥€í ÿ∂ÿ±Ÿàÿ± ⁄©€åÿ±ÿß Ÿæÿ±⁄©ÿ™ŸÜÿß ŸÖŸÜÿ≥ŸÑ⁄© ŸÖ€åŸæÿ≥ ÿ¥ÿπŸàÿ± ŸÜ⁄©⁄æÿßÿ±ŸÜÿß ÿ¢ÿ±⁄à€åŸÜŸÜÿ≥ ÿ≠€åÿØÿ± ÿßÿ≤ ⁄Øÿ∞ÿ≥ÿ™€Å ŸÖÿπÿß€ÅÿØ€í ÿßÿ≥ÿ≥⁄Øÿßÿ±⁄à ŸàŸÑÿßÿØÿ™ ŸÖÿ±⁄ÜŸÜŸπÿ≥ ⁄©ÿ±ÿ∫ÿ≥ÿ™ÿßŸÜ ⁄©€åŸÜ€å⁄àÿß ŸÖŸÑÿßŸπ⁄æ€í ÿ≥€å⁄à\n",
            "\n",
            "Words: 300 | Sentences: 0\n",
            "\n",
            "Bigram Article 2\n",
            "------------------------------------------------------------\n",
            "Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫ ÿπŸÑÿßŸÇ€Å ŸÜÿ∏ÿ± Ÿπÿ±ÿßŸÜÿ≥Ÿæÿ±ŸÜÿ≥ÿß ⁄©ÿ¥ ÿπÿ≥⁄©ÿ±ÿß ÿ≥ŸàÿØŸÖŸÜÿØ ÿ¥ÿ±ÿßÿ®Ÿàÿ± ÿ≥€åÿØ⁄æÿß ŸÖÿ±€Å ÿ®ŸÜ⁄ØÿßŸÑÿß ⁄Øÿ±€åÿ¨Ÿàÿ¶€åÿ¥ŸÜ ÿ≥ÿ≤ÿßÿ¶€í Ÿàÿßÿ®ÿ≥ÿ™⁄Øÿß ÿ≥Ÿàÿ¥ŸÑ ŸÖ€å⁄à€åÿß Ÿàÿ≤ŸÜ ÿ¨ŸÑÿßŸàÿ∑ŸÜÿß ÿ™ÿ≠ÿ≥€åŸÜ Ÿàÿ±ÿØ€Å ÿ≥ÿ±⁄©ÿßÿ±ÿß ŸÜ€åŸæÿ±ÿß ÿßŸÑŸÖÿ≥ŸÑŸÖ€åŸÜ ŸÖ€åŸπÿ±Ÿà ÿ¨Ÿàÿß ŸπŸà⁄©ŸÜÿßÿ¶ÿ≤⁄à ÿß€å⁄©Ÿà€åŸπ€åÿ≤ ÿ®ÿ≠ÿ±€åŸÜ ÿ®⁄æÿ™€åÿ¨ ÿ≥ÿßŸÜÿ≠€Å ŸÜ€åŸàÿß ⁄ÜÿßŸπ⁄ØÿßŸÖ ÿß€å⁄©ÿ≥⁄Ü€åŸÜÿ¨ ÿ≥ÿ±⁄©ÿ±ÿØ€Å ÿ∑ŸÑÿßŸÑ ÿ™€åÿ≥€í €Åÿ±ŸÜÿßÿ¶ÿß ÿ™ÿπÿµÿ®ÿßŸÜ€Å ÿ≥ÿ±Ÿà€í ÿÆ€åÿ±ÿÆŸàÿß€Å ÿ¨ŸÜÿ±ŸÑ ÿß€åÿ±Ÿàÿ≥Ÿæ€åÿ≥ ÿß€åÿ≥€í ÿØÿßÿ±ŸÑÿ≠⁄©ŸàŸÖÿ™ Ÿπ€åÿ±ŸÅ ÿØÿ≥ÿ™Ÿàÿ± ŸÅÿ±ŸÖ ŸÖŸàŸÜÿØÿ±€Å ÿ¨ÿßŸÑ€í ÿ¨ÿ≤Ÿàÿß ÿ±⁄©⁄æŸàÿßÿ¶ÿß ÿ≤€Åÿ±€åŸÑ€í ⁄à€åŸÜÿ¶ŸÑ ÿ™ÿπÿ≤€åÿ™ ŸÑ€Åÿ¨€Å ÿßŸÅÿ™ÿ™ÿßÿ≠ ÿ∏€ÅŸàÿ± ⁄Ø⁄æŸÜÿß ÿ¨ŸÜ⁄ØŸÑ ÿ≥ŸÑÿ∑ŸÜÿ™ ÿ≥ŸÜÿßÿ±⁄©ŸÑ ÿß€å⁄àŸàŸà⁄©€åŸπ ÿ±ÿßŸÜ ÿÆÿ¨ÿ≥ÿ™€Å ⁄ÜŸà€ÅÿØÿ±ÿß ÿå ÿØŸÖ⁄àŸÖ ŸπŸà€åŸπÿ≥ ÿ≠ÿßŸÑ€å€Å ŸÅÿ±€åŸÜ⁄àÿ≤ ŸÖÿ∂ÿßŸÅ Ÿæÿ±€åÿ¥ÿßŸÜ ÿ≥ÿ®ÿ® ⁄ØŸàÿ± ⁄©ÿ±ÿØ ÿß⁄Øÿ±ÿ¢Ÿæ €Å€åŸÜ⁄Øÿ± ÿ®ÿ±Ÿæÿß ÿ≥⁄©ÿß ÿ™ÿπÿ≤€åÿ± ⁄©ŸÜÿßŸÑ ÿ™ŸÜÿßÿ§ ÿ≥€åŸÅÿßÿ¶ÿ± ÿßÿπÿ™ÿØÿßŸÑ ŸàŸÅÿßŸÇ ⁄ØŸÑÿ¥ŸÜ ÿß€åÿßŸÜ ÿπÿ´ŸÖÿßŸÜÿß ÿ™ÿ∞ŸÑ€åŸÑ Ÿæ€åÿ±Ÿàÿß ÿØÿßŸÜ ÿ¨ÿßÿ±ÿ≠ÿßŸÜ€Å ŸÅÿßŸÑŸàŸàÿ±ÿ≤ ŸÜÿßŸÖÿß ⁄ÜŸÑ€å ÿ¢ÿ±ÿßÿ¶ÿß Ÿæÿ±ÿ≤€Å ÿßŸπ⁄æŸàÿßÿ¶ Ÿπÿ±ÿßŸÑÿ±ÿ≤ ÿØŸà€Åÿ±ÿß ÿßŸÑ€å⁄©Ÿπÿ±⁄© ÿ±€åŸÖŸπ€åŸÜÿ≥ ÿ¨Ÿàÿ¥€åŸÑÿß ⁄Øÿßÿ¶€í ÿ¨€åÿ™ ŸÅÿßÿµŸÑ€Å ÿßŸÜ⁄©ÿ™ ÿπŸÑÿßŸà€Å ⁄Ü⁄æŸàŸπÿß ⁄Ü⁄æŸàŸπÿß ÿß€åÿßŸÖ ÿπÿØÿßŸÑÿ™ ÿ®ÿØŸÑÿ™ÿß ⁄©Ÿàÿ±ÿ™ ŸÑÿß⁄àŸÑ€í ⁄ØŸàÿ¶ ÿ®⁄æ€åÿ¨€í ŸÖŸÜÿßÿ≤ŸÑ ÿßÿ™ŸÜ⁄ë ÿÆÿ≥ÿßÿ± €åŸàŸÜŸπÿ≥ ÿ®ÿÆÿ¥ÿ™€í ÿ¨ŸàŸÜÿ≤ ÿ®ÿ∑Ÿàÿ± ÿ¢ÿ±ÿßŸÖ ⁄©ÿ≥ ⁄©Ÿπÿßÿ§ ŸÖÿ¨ÿß€ÅÿØ ⁄ØŸÜ€í ÿ¥ÿß€åÿßŸÜ ⁄©⁄©ÿ±ÿß ÿ≠ÿßŸÖÿß ÿ±€åÿ®ŸÑ ÿ®ŸÑ€å⁄© ÿ±⁄©ÿ¥€Å ÿ®ÿØŸÇÿ≥ŸÖÿ™ÿß ÿ®€åÿß€Å ŸÑ€åÿ¨€å€í Ÿæÿ±⁄©ÿ™ŸÜÿß ŸÖÿ®ÿ¥ÿ± ÿ®⁄æÿßÿ±ÿ™€å€Å ÿ®ÿ±€åŸÜ ÿ¨ŸÖÿ¥€åÿØ ŸÅÿ∂ÿß ⁄©€åŸÜÿ≥ÿ±ÿ≤ ÿ¥ÿß€ÅÿßŸÜ€Å ÿ∫ÿ∞ÿß ⁄Ü⁄ë€å ⁄©ŸÖŸàÿ±⁄àÿ± ⁄ØŸÜÿß€Å ÿ±ŸàŸÜÿß ŸÖÿ±ÿß⁄©ÿ≤ ⁄©€åÿ≥€í ÿ≥ÿπÿßÿØÿ™ Ÿæ⁄æŸÑÿ¨⁄æ⁄ëÿß ⁄Ü⁄æŸÑŸÜÿß ÿßÿ®ÿ™ÿ≠ÿßÿ¨ ⁄Ü⁄æÿ™ÿ±ŸàŸÑ ŸÖŸÑÿ≤ Ÿæ€å ÿ±ŸÅŸÇÿß ŸÅÿßŸπÿß ⁄©ŸÜ⁄ØŸÜÿß ÿ®⁄©⁄æÿ±ÿßŸÜÿß ⁄©ŸÜÿ≥Ÿàÿ±ÿ¥€åŸÖÿ≤ ŸÖŸÜÿ™ÿ∏ŸÖ€åŸÜ ⁄©⁄æ€åŸÑ€í ÿ®ŸÑŸàÿßÿ≥ÿ∑€Å €Å€åŸπÿß ⁄©ÿßŸÖÿ≥€åŸπÿ≥ Ÿàÿ±€å⁄òŸÜ ÿ¥ŸÑŸàÿ®ÿ± ÿ≤€Åÿ±€åŸÑ€í ÿ≥⁄Ø€í Ÿπ€åŸÜ⁄àÿ± ÿ≠ŸÑŸÅ ÿ±€åŸÅÿßÿ¶ŸÜÿ±ÿß ÿ¥ŸÖÿßÿ¶ŸÑ€Å ŸæŸÜÿ¨ ⁄©€åŸÖ€åŸÑ ÿ≥Ÿπ€åŸÜ⁄àÿ±⁄à ÿ∂ÿßÿ®ÿ∑€Å ÿ≠ŸÑŸÇ€í ÿßÿµÿ±ÿßÿ± ŸÜÿßÿ≤⁄© ŸÅÿ±ŸÖÿßÿ¶€í ŸÜŸàÿ¢ÿ®ÿßÿØ€åÿßÿ™ÿß ÿ®Ÿàÿ¶ŸÜ⁄Ø ÿØÿßÿ¶ÿ± ⁄à⁄æŸàŸÑ ÿ±ŸÖÿ≤ ÿ™ŸÇÿßÿ±€åÿ± ⁄à€åÿ≤ÿßÿ¶ŸÜÿ±ÿ≤ ⁄àÿ≥⁄©ŸàÿßŸÑ€åŸÅ⁄©€åÿ¥ŸÜ ŸÇÿØÿ±ÿ™ ÿ¢ÿ≥⁄© ŸÅ€åŸÜÿ≥ŸÜ⁄Ø ÿßŸÜŸà€åÿ≥Ÿπ€å⁄Øÿ¥ŸÜ ÿ¨€í ÿ±⁄©Ÿà ÿ¢ÿ®ÿßÿØÿß ÿ®€í⁄ØŸÜÿß€Åÿß €åÿßÿ±ÿ≤ÿßÿ±€å⁄Ü ŸÑŸπ⁄©ÿß ŸÖÿ≥ÿ™ŸÇÿ®ŸÑ ÿ¨Ÿàÿ¥€åŸÑÿß ÿßŸπ€å⁄© ÿ®ÿ±€å⁄Ø€å⁄à ⁄©⁄æŸÑŸàÿßÿ¶ÿß ÿØ⁄æŸÑÿß Ÿπ⁄æÿßŸÜÿß ŸÖÿ≠⁄©ŸÖ€í €Åÿ™⁄æ€åÿß Ÿæÿß⁄ë€Å ŸÑÿßŸÜ⁄Ü €ÅŸàÿ¶ÿß ÿπÿØŸÑ€å€Å €ÅŸÑ⁄©ÿß ŸÖ€Åÿ±ÿßŸÑŸÜÿ≥ÿß ⁄Ü€åÿÆÿ™ÿß ŸÖ€åÿ≤ÿßŸÜÿßÿ¶ŸÜ ⁄Üÿßÿ±ÿØ ⁄ØŸàÿ±ŸÜŸÜÿ≥ ÿß⁄©ÿßÿ§ŸÜŸπŸÜ⁄Ø ÿ™ŸÇ€å ÿ™⁄æÿ±ŸÖŸÑ ÿ≠ŸÜÿß ÿ≥€Å€åŸÑ ÿ™ÿ®ÿµÿ±€í ÿ¨ŸÜ€åŸÜ ⁄©€åŸæ€åÿ≥Ÿπÿß Ÿæ€ÅŸÜ⁄Üÿßÿ™€í ÿ¥ÿß⁄Øÿ±ÿØÿß ŸæŸÑŸπÿß ÿ±€å⁄ØŸÜ ŸπŸæ⁄© ŸÜÿ≥ÿÆ€Å ⁄©Ÿà€åÿ™ ÿ¨Ÿà⁄ëŸÜÿß ÿßÿµŸÑÿßÿ≠ ÿØÿ±ÿ≥ÿß ÿØŸÅÿßÿ™ÿ± ⁄©ŸàŸÜ ÿßÿπÿØÿßÿØŸàÿ¥ŸÖÿßÿ± ÿ™ŸÑÿßŸÅÿß ÿ™ÿ±ŸÖ€åŸÖÿß ÿ≠ŸÑŸÇ€í €Åÿ™⁄æ€åÿß ⁄Ü⁄ë⁄æÿß ⁄ÜŸÑÿß€åÿß ÿ≥ŸàÿßŸÖÿß ŸÜ€åŸæÿß Ÿàÿßÿ¨Ÿæÿßÿ¶ÿß ÿ±ÿßÿ¶ÿ≥ ŸÖ⁄Øÿ± ÿ≥ŸàŸÑ€Å ÿ≤€åÿ±ÿ≠ÿ±ÿßÿ≥ÿ™ ÿ≥€åŸÜ⁄©⁄ë ÿ®ÿß⁄àÿß ŸÜÿ∞ÿ± €ÅÿßŸÑÿß ÿ≥ŸÖ Ÿàÿ±ÿ™⁄æ ŸÇÿ®ÿßÿ¶ŸÑÿß ŸÜŸà⁄©ŸÜ⁄àÿß ÿßŸàŸÅÿß ÿ®ÿØÿßÿπÿ™ŸÖÿßÿØÿß ÿ®⁄æ€åÿ¨ŸÜÿß ÿ™ÿ±ÿ∫€åÿ® ÿ™ŸÇÿßÿ±€åÿ® ⁄ØŸÜÿß€Å⁄Øÿßÿ± ŸÑ⁄Øÿßÿ¶€í Ÿæÿßÿ≥ŸæŸàÿ±Ÿπ ŸÅ€å⁄©Ÿπ Ÿàÿ¨ŸàÿØ ÿ≥€åÿ≤ ŸÅÿßÿ¶Ÿπÿ±ÿ≤ ÿ®ÿØŸàŸÑÿ™ Ÿæÿ≥€å ÿ™ÿ≠ŸÑ€åŸÑ €ÅŸàÿ≥⁄©€í ÿ≠ÿØÿ™ ÿß⁄ë€í ÿ∫ÿßŸÑÿ®ÿß ÿ¨ŸàŸπ⁄æÿßÿ± ŸÖÿ≠ÿ®ÿ™ ÿ¢ÿ≥ÿ™ÿßŸÜ€Å ÿ±Ÿàÿ¥ŸÜ€å ÿ®Ÿà⁄Øÿß ŸÖÿÆŸÅŸÅ ÿÆÿ±⁄Ü€Å ⁄àÿßÿ¶ÿ±€å⁄©Ÿπÿ±ÿ®€åÿ¨ŸÖŸÜ ŸÖ€åŸÜÿß ÿ±ÿ≥ŸàŸÖ ⁄©ÿßÿ±€å\n",
            "\n",
            "Words: 300 | Sentences: 0\n",
            "\n",
            "Bigram Article 3\n",
            "------------------------------------------------------------\n",
            "Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫ ŸæŸàŸÑ€åÿ≥ Ÿæÿ¥ €Åÿßÿ§ÿ≥ŸÜ⁄Ø ⁄Øÿ±€åÿ¨Ÿàÿ¶€åÿ¥ŸÜ ÿßŸÖ€åÿ¨ÿ≤ ÿ¨ÿ±⁄Ø€Å ÿß€å⁄©ÿ≥⁄Ü€åŸÜÿ¨ ÿ¢ŸÜŸπÿß ÿßŸÜŸπ€åŸÑÿß ŸÖÿ≥ÿßŸÑ⁄© ŸÖÿπÿ∑ŸÑ ÿß€åÿ¨ŸÜŸπ ⁄©ŸÖÿ≤Ÿàÿ± Ÿæÿßÿ™ÿß ŸÖŸÜÿßÿ≥ÿ®ÿ™ ÿ≥ŸæŸÑÿßÿ¶ÿ±ÿ≤ ŸÅŸàÿ™⁄Øÿß ÿßÿ≥€å ÿ≥ŸÜ⁄Ø⁄æÿßÿ±ÿßŸÖ€í ÿ∑ÿ±€åŸÇ ÿ≥⁄Ü ŸæÿßŸÜ⁄ÜŸàÿß⁄∫ €ÅŸÜÿ≥ÿßÿ™€í ÿ±ÿßÿ≥ÿ™€í ÿ®ÿ¨ÿ±ÿß ÿßŸÜŸà€åÿ≥Ÿπ€å⁄Ø€åŸÜÿ¥ ŸÇÿ∑ÿπ ⁄©Ÿà⁄ØŸÑ€åŸÖŸÜ ŸÖ⁄©€Å €åÿπŸÇŸàÿ® ÿßŸàÿ≤ ÿ∫€åŸàÿ± ⁄©ŸÑ€å⁄©ÿ¥ŸÜ ÿ¥€åŸÑŸπÿ± ⁄©€åÿ™⁄æÿ±€åŸÜ ⁄àÿ±ÿßÿ¶€åŸàŸÜ⁄Ø Ÿæ⁄æŸàŸÑÿ™ÿß ÿßÿ´ÿßÿ´€Å ⁄ØŸÑŸà⁄Øÿßÿ± ⁄àÿ≥ŸæŸÑ€åŸÜ ÿ¢ŸÅÿ±€åÿØÿß ⁄©ÿß ⁄©€ÅŸÜÿß ŸàŸÑÿß€åÿ™ €Å€åŸπ ŸÑ€å⁄àÿ±ÿ¥Ÿæ ÿπÿ®ÿØÿßŸÑÿ±ÿ§ŸÅ ÿ¨Ÿà⁄ëÿß ÿßÿπÿ¥ÿßÿ±€å€Å ÿ®⁄æ€åÿ± ŸÅŸÜ⁄©ÿßÿ± ÿ™ÿ±ÿ≥ ÿ®⁄æÿß⁄Ø ÿ±ÿ¨ÿ≠ÿßŸÜ ÿÆ€åÿ®ÿ±ŸæÿÆÿ™ŸàŸÜ ÿ®€å⁄© ÿß⁄ëÿßŸÜ ÿ¥ŸÖÿßÿ¶ŸÑ€Å ÿ™ÿ±ÿ¨ŸÖÿßŸÜÿß ÿ™ÿßÿ¨Ÿàÿ± ÿ™ÿßÿ±⁄© ⁄©€å⁄Ü ŸÖÿ¨ŸÖÿπ€Å Ÿæ€åÿßÿ±€í ÿ®⁄Üÿ™ÿß ÿ≥€í <NUM> ŸÅ€åÿµÿØ Ÿæ⁄©⁄ëÿ™ÿß ⁄Øÿ±Ÿà ÿ™€åÿ±ÿß€Å ⁄©€Åÿ±ÿßŸÖ €åÿßÿ≥ŸÖ€åŸÜ ÿµŸÜŸÅ ÿπ€åŸÜ ŸπÿßŸÑ ÿ®ŸÑÿ™ÿ≥ÿ™ÿßŸÜ ÿπÿ®ÿØÿßŸÑŸàŸÑÿß ⁄©ÿßŸπÿ™ÿß ⁄©€åŸÅ€åÿ™ ŸÖÿßÿ≥Ÿπÿ±ÿ≤ Ÿæÿ±Ÿàÿ≥ÿ≥ ÿ±€åŸæÿ± ÿ¥€å ÿ≥€åÿßÿ≠ ÿß€åŸàŸÜŸÜ⁄Ø ÿßŸÜÿ™ÿ∑ÿßŸÖ ÿÆŸàŸÅŸÜÿß⁄© ÿßÿ±Ÿà⁄ë€Å ŸÖÿ®ÿ¥ÿ± Ÿæÿ±Ÿàÿ¨€å⁄©Ÿπ ŸÖŸÑÿß⁄©ÿß ÿ±ŸàŸÖÿßŸÑ ÿ®ÿ¨⁄©ÿ± ŸÅŸÑÿßÿ¶ÿß ÿØŸà€åŸàŸÜ€åŸÜ ⁄Øÿ≤ÿ±ÿ®ÿ≥ÿ± ÿ¨ÿ™ŸÜÿß ÿØ⁄æŸÖ⁄©ÿß€åÿß ŸæŸÜÿ¥ŸÜ ⁄©ŸÖÿ≥ŸÜ ÿ¥€åŸàÿß ⁄ØŸÅŸπÿ≥ ÿ¢ÿ≤ŸÖÿß€åÿß Ÿà€å⁄© Ÿæÿ¥ÿ™ŸàŸÜÿ≥ÿ™ÿßŸÜ €Åÿ±ÿ≥Ÿπ ÿ±€åŸÅÿ±ŸÜ⁄àŸÖ ⁄©€åŸÖ€åŸÑ ÿ¨ÿßÿ¶€í ⁄Ø€åŸÜ⁄Øÿ≥Ÿπÿ±ÿ≤ ⁄Ü€åÿÆÿ™€í ⁄Ü⁄æÿßÿ™€Å ÿß€åŸÜÿØ⁄æŸÜ ŸæŸÑ⁄ØŸÖÿ®ŸπŸÑÿß⁄ë⁄©ÿßŸÜ€Å ÿ±ÿßÿ∂ÿß ŸÑŸÜ⁄©ŸÜ ŸÖÿ≠Ÿà Ÿπ⁄æÿß⁄∫ ŸÖ€åÿ±€åÿ¨ÿ≤ ÿ≥ÿßÿÆÿ™ ÿ≥€åŸÖ ÿßÿ¥ÿßÿ±€Å ÿ≥ŸÖÿßÿπÿ™ ⁄©ŸàÿØ€Åÿ¥ÿ™ ÿßŸÑŸÇÿØŸàÿ≥ ÿ≥ÿßÿ¶€å⁄à ÿ≠ŸÑ ŸÇÿ®ÿ± ⁄©⁄æÿ¨Ÿàÿ± ŸÅÿπŸÑ ÿ≤ÿßÿ±ŸàŸÇÿ∑ÿßÿ± ŸÖŸÑÿ®Ÿàÿ≥ ÿ®ŸÑ€åŸÜ ÿØ€åÿ™€í ŸÖŸàÿ± ÿ≥ŸÑÿ∑ÿßŸÜ ÿ≥ŸπŸàÿ±€åÿ≤ ⁄Ü€åÿ±ÿ™ÿß ÿßŸà⁄à€åÿ≥ÿß ÿπŸÑÿßŸÇ ÿ±ÿß€Å ŸÅÿ∂ÿßÿ§⁄∫ ÿ¨ÿßÿ¶€í ŸàŸÇŸàÿπ€Å ÿ¥ŸÜÿß⁄©ÿß ÿ≥ŸÜ€å⁄Üÿ± ÿ®ŸÑŸÇ€åÿ≥ ÿ®ÿßŸÑÿß ÿ±ÿ≥ŸÖ ÿ¨ÿ®€Å ÿ≥ÿßÿ¶Ÿπÿß€å⁄©ÿ≥Ÿæÿ± ŸÜŸàÿßÿ≤ Ÿæÿ±ÿ≥ŸÜŸÑ ÿ®ÿÆÿßÿ±ÿß €ÅŸàÿ¶ÿß ⁄©€Å Ÿæ€ÅŸÑ€í ÿßŸÑ€å⁄©Ÿπÿ±⁄© €ÅŸÜ⁄ØÿßŸÖ€Å ÿ®⁄ë⁄æÿßŸÜÿß Ÿæÿ±ŸàŸÅ€åÿ¥ŸÜŸÑÿ≤ ŸÖÿ∞ÿßŸÇ ÿ®ÿ±ÿ™⁄æ ⁄©ŸÑŸÜ⁄©ÿ± ÿ®ÿßÿ®ÿ±ÿß ÿ≥ŸàŸÖ€åŸÜ ŸÅÿßÿ±ÿß ÿ™ŸÜ⁄Ø ⁄à€åŸàÿßÿ¶€å⁄àÿ± ŸÖÿ¨ÿßÿ≤ ⁄©ŸÑ⁄Üÿ± ÿßŸÜÿ¨ŸÖÿßÿØ ŸÖŸÜÿ≤ŸÑ€Å ÿ®ÿ∫€åÿ±ŸÖÿß€Åÿ± ⁄Ü€åŸÜÿßŸÜ⁄à€åÿß ŸÅÿßÿ¶ŸÑ Ÿæÿßÿ≥ ⁄Ø⁄ëÿß ÿ≥€åŸÜ€Å €ÅŸàÿ¥ÿ±ÿ®€Å Ÿæ⁄æŸàŸπÿ™€í ŸÜŸàÿ¥⁄©ÿß €ÅŸàÿ¨ÿßÿ™ÿß ÿ≥ŸÑŸÅÿ± ÿ≥ŸÅÿ±ÿß ŸÖÿ¨ÿ≥ŸÖ€í ÿÆŸàŸÑ ŸÅÿ±€åŸÜ⁄àÿ¥Ÿæ ÿßÿ¨€åÿ±ŸÜ ÿØÿ¥Ÿàÿßÿ± ÿ∫€åÿ±ŸÖÿπŸÖŸàŸÑÿß ŸÑ€å⁄©ÿ≥ ÿßŸÑŸÅÿßÿ±ÿßŸÜ ⁄©ÿßÿ¥ŸÅ ÿ≥⁄©ÿßÿ¶ÿß ⁄Øÿ±ÿØŸàŸÜŸàÿßÿ≠ ÿÆÿØÿßÿ¶ÿß ÿ≥ÿ±Ÿæÿ±€Å ÿ¥ÿ¥€å ⁄Üÿ± ÿßŸÑŸÖ€åÿØ€Å ÿßŸÖŸÜ ÿßÿ™⁄æÿßÿ±Ÿπÿß €Åÿßÿ¶⁄àÿ±ÿß ÿ¨€Åÿ™ÿß ⁄ÜŸàŸÜ ÿ≥ŸÅÿ±ÿß Ÿπ⁄æ€Åÿ±ÿß€åÿß ⁄à⁄æŸà⁄© ŸÖŸàÿ±ÿØ ÿ¥ÿßÿØÿß ÿ¨ÿ≥ÿ™ÿ¨Ÿà ÿ¨Ÿà€Åÿ±ÿß ÿ®Ÿàÿ≥€åÿØ€Å €Åÿ≤ÿßÿ±⁄ØŸÜÿ¨ÿß ⁄Ø⁄æÿ±ÿß ŸÖŸàŸÑÿßŸÜÿß €å€Å €ÅŸÜÿØ€åŸàŸÜÿßŸÜÿß ÿ±ŸàŸπ€åŸÜ ⁄©⁄æÿß ÿµÿ≠ŸÜ Ÿæÿ±ÿßÿ≥€å⁄©ŸàŸπÿ± ŸÖÿ¥€ÅÿØ ÿ®ÿ±ÿßÿ≥ ⁄©ÿßÿ±⁄©ŸÜ ÿ¢ŸàŸπ ÿß€åŸàŸÜ⁄©ÿ≥ ŸÖŸàÿ®ŸÑÿßÿ¶ÿ≤€åÿ¥ŸÜ Ÿπ€åÿ®ŸÑ ÿ™€åŸÖÿßÿ±ÿØÿßÿ±ÿß ÿ®ÿßŸàÿ¨ŸàÿØ ÿ®⁄æÿ¨Ÿàÿßÿ¶€í ÿ®⁄Üÿ™ÿß ÿ¥ÿßŸÖ ÿßŸÖ€åÿ¨ÿ≤ ÿ∏ÿßŸÑŸÖ ⁄©ŸÜ€å⁄©Ÿπ ⁄à€åŸÜŸπŸÑ ŸÑÿßŸπ⁄æÿß ŸÅŸÑÿß⁄∫ ŸæŸÑÿßŸÜŸπ ÿ≥⁄Øÿ±€åŸπ ŸàŸÑÿØ ÿßÿ≥Ÿπ€åÿ¨ Ÿæÿ±ŸàŸÅ ⁄àŸÜ⁄à ŸÖÿ≤ÿØŸàÿ±ÿß ÿßÿ±ÿØ ÿ¥€åŸàŸÜ⁄Ø ⁄©ÿßÿ±€å Ÿπ⁄æ€Åÿ±ÿß ŸÑ⁄©€å ÿ¥€Åÿ≤ÿßÿØ€í ⁄ØÿßŸÜ⁄Ü⁄æ€í ŸàÿßŸæÿ≥ÿß ÿ≥ÿß⁄ë⁄æ€í ⁄Øÿ±ÿßÿ≥ ŸÅŸÜÿßŸÜÿ¥ŸÑ ŸÖÿßÿ±ŸπŸÖ ÿßŸÑŸÖÿπÿ±ŸàŸÅ ÿßŸà⁄ë⁄æ€í ŸÜ€åÿØÿ±ŸÑ€åŸÜ⁄àÿ≤ €ÅŸÖÿØÿ±ÿØÿß ŸÖ€å⁄©ÿ≥€å⁄©Ÿà ÿ™Ÿàÿ¨€Å ÿßŸÑÿπÿ®ÿßÿØ ÿµÿπŸÜÿ™ÿß ÿ®€åŸπÿ± ÿßŸπ⁄æÿßÿ™ÿß ŸàŸàŸπŸÜ⁄Ø ŸÅÿßÿ¶ŸÜ⁄àŸÜ⁄Ø ŸæŸàÿ¥ ŸÜÿ´ÿßÿ±ÿπŸÑÿß ÿ®ÿ¨ÿ±ÿß ⁄©Ÿàÿ®⁄æÿß ÿ®ŸÜÿØ⁄æŸÜ ŸÜŸÅÿ±ÿß ⁄©Ÿàÿßÿ∫Ÿàÿß ⁄©Ÿàÿ±€åŸà⁄©€í ÿ™ŸàÿßŸÜÿßÿ¶ ÿßŸÑ€å⁄©Ÿπÿ±ÿßŸÜ⁄© ÿ≥€åŸàŸÜ⁄Ø ŸÖÿ≠€å ÿßÿ≥ÿ™ÿØŸÑÿßŸÑ ŸÑ€åÿ≤ŸÜ⁄Ø ÿßÿ≠€åÿß ⁄©Ÿàÿ±⁄àŸÑ€åÿ≥ ⁄ÜŸà⁄©€åÿØÿßÿ± ÿ≥€åÿ™⁄æ€åŸÜ ŸÖÿ±Ÿàÿ¨€Å ⁄ÜŸÖŸÜ ÿßŸÅ€åŸàŸÜ\n",
            "\n",
            "Words: 300 | Sentences: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRIGRAM MODEL ARTICLE GENERATION WITH BACKOFF**"
      ],
      "metadata": {
        "id": "tpzh8EaayJxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def sample_trigram_backoff(w1, w2):\n",
        "\n",
        "    if (w1, w2) in trigram_model.bigram_context_counts and \\\n",
        "       trigram_model.bigram_context_counts[(w1, w2)] > 0:\n",
        "\n",
        "        dist = trigram_model.get_next_word_probabilities(w1, w2)\n",
        "        words = list(dist.keys())\n",
        "        probs = list(dist.values())\n",
        "\n",
        "        total = sum(probs)\n",
        "        if total > 0:\n",
        "            probs = [p/total for p in probs]\n",
        "            return random.choices(words, weights=probs, k=1)[0]\n",
        "\n",
        "    if w2 in bigram_model.bigram_counts:\n",
        "        dist = bigram_model.get_next_word_probabilities(w2)\n",
        "        words = list(dist.keys())\n",
        "        probs = list(dist.values())\n",
        "\n",
        "        total = sum(probs)\n",
        "        if total > 0:\n",
        "            probs = [p/total for p in probs]\n",
        "            return random.choices(words, weights=probs, k=1)[0]\n",
        "\n",
        "    return random.choice(list(unigram_model.vocabulary))\n",
        "\n",
        "\n",
        "def generate_trigram_article(seed_prompt,\n",
        "                             min_words=200,\n",
        "                             max_words=300,\n",
        "                             min_sentences=5):\n",
        "\n",
        "    seed_tokens = seed_prompt.split()\n",
        "\n",
        "    if len(seed_tokens) < 5 or len(seed_tokens) > 8:\n",
        "        print(\"Seed must contain 5‚Äì8 Urdu words.\")\n",
        "        return None\n",
        "\n",
        "    generated = seed_tokens.copy()\n",
        "    sentence_count = 0\n",
        "\n",
        "    while len(generated) < max_words:\n",
        "\n",
        "        if len(generated) < 2:\n",
        "            next_word = random.choice(list(unigram_model.vocabulary))\n",
        "        else:\n",
        "            w1, w2 = generated[-2], generated[-1]\n",
        "            next_word = sample_trigram_backoff(w1, w2)\n",
        "\n",
        "        generated.append(next_word)\n",
        "\n",
        "        if next_word == \"€î\":\n",
        "            sentence_count += 1\n",
        "\n",
        "        if len(generated) >= min_words and sentence_count >= min_sentences:\n",
        "            break\n",
        "\n",
        "    if generated[-1] != \"€î\":\n",
        "        generated.append(\"€î\")\n",
        "\n",
        "    return \" \".join(generated[:max_words])\n",
        "\n",
        "\n",
        "\n",
        "seed = \"Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫\"\n",
        "\n",
        "for i in range(1, 4):\n",
        "    article = generate_trigram_article(seed)\n",
        "    print(f\"\\nTrigram Article {i}\")\n",
        "    print(\"-\"*60)\n",
        "    print(article)\n",
        "    print(f\"\\nWords: {len(article.split())} | Sentences: {article.count('€î')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JDdmS3zx8te",
        "outputId": "a67a19ab-3840-41ab-f818-3ea95c5ec090"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trigram Article 1\n",
            "------------------------------------------------------------\n",
            "Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫ ÿØÿπŸàÿß ⁄Üÿß€å€å€í ⁄à€åŸàÿßÿ¶ÿ≥ÿ≤ ŸÖÿ±ÿßÿ≥ŸÑ€Å €ÅŸÑ⁄ÜŸÑ ⁄Øÿßÿ§⁄∫ ŸÖÿπŸÖŸàÿ± ⁄à€åŸÅÿßŸÑŸπ ŸÑŸπ ŸàŸàÿßÿ± ÿßÿØ⁄æ ŸπŸàÿ¶ŸÜŸπÿß ŸÖÿ™ÿ¥ŸÖŸÑ Ÿàÿßÿ¶ÿ± ÿµÿØ€åŸÇ€Å ÿ®ÿ≥ÿ± ÿ®⁄æÿßŸπÿß €åŸàŸÜ€åŸàÿ≥Ÿπ€å €åÿßÿØÿØÿßÿ¥ÿ™ Ÿæÿ±ŸàŸÅ€åÿ¥ŸÜŸÑÿ≤ ŸÑ⁄©⁄æŸÜÿß ŸÖ€Åÿ∞ÿ® ⁄©€åŸÜÿ≥ÿ±ÿ≤ ÿÆÿ®ÿ∑ ÿ¨ÿ®⁄ë€í ⁄©ÿ±ŸàÿßŸÜÿß ŸÇÿ±€åÿ¥€å ÿ¨⁄æŸÜ⁄Ø ŸÅÿ±ÿß⁄à ŸÅ€åŸÑŸà ÿπŸÑ€åÿ≠ÿØ€Å ÿßŸÑÿßÿØÿß Ÿæ⁄ë⁄æÿßÿ¶ÿß ÿπÿ®ÿßÿ≥ÿß ÿ®⁄©⁄æÿ±€í ⁄ÜŸàÿßÿ¶ÿ≥ ⁄ØŸπ⁄©€í ŸæŸÜ⁄©ÿß €Åÿßÿ±ÿß ÿßÿ±€í €Å€åŸπÿß ŸÜŸÅ€å ŸÖ€åÿ≥ÿ¨ ÿØ⁄©⁄æÿ™ÿß ÿßÿ≥ÿ≥€åÿ≥ŸÖŸÜŸπ ÿ®Ÿà⁄ë⁄æÿß ŸπÿßŸàŸÜÿ≤ ⁄©€ÅŸÇŸàŸÖ ⁄à€åŸÅÿßŸÑŸπ ÿ™ŸÖÿØŸÜ ÿß€å⁄©ÿ≥Ÿπ€åŸÜÿ¥ŸÜ Ÿæÿ±ŸàŸπŸàŸÜ ŸÖÿ≥ÿ™ŸàŸÜ⁄Ø ÿ®ÿ±ÿß€ÅŸÖÿØÿ∫ ŸÜ€åŸæ ⁄àÿ®ŸÑ€åŸà €Å€åŸÜ⁄Øÿ± ÿ®€åŸÑŸÜÿ≥ ŸÑŸàÿ≥€åÿß ŸÖ⁄Ü⁄æ ŸæÿßŸÑ€åÿ≥ÿß ÿßÿØŸàÿßÿ± ŸÖ€å⁄∫ Ÿàÿ∂ÿπ Ÿæÿ≤€åÿ¥⁄©€åÿßŸÜ ⁄©ÿßŸÑ€í ⁄©ŸÜŸàÿßÿ±€í Ÿæÿßÿ¥ÿß ⁄©ŸÑŸÜ⁄©ÿ± ⁄Øÿ∞ÿ¥ÿ™€Å ÿØ€å⁄Øÿ±ÿ™ŸÖÿßŸÖ ÿßÿ±ÿ™⁄©ÿßÿ≤ ⁄Øÿßÿ±⁄àÿ≤ ÿ≥ÿØŸàÿ± ⁄©ÿßÿ±Ÿàÿ®ÿßÿ± ŸÖŸÑÿßŸÇÿßÿ™€å ÿ≤€åÿ±ÿ∫Ÿàÿ± ŸÑ⁄Øÿß€åÿß ÿß€å⁄©Ÿà€åŸπÿßÿ≥ ÿÆŸÖ ⁄©€åŸπ⁄Øÿ±ÿß ⁄©€åÿ®ŸÜŸπ ŸÖÿ≠Ÿàÿ± ŸÖ€åŸÜÿß €ÅŸÜ⁄àÿßÿ¶ÿß ÿ®€åŸÖÿßÿ± ÿ™Ÿà€Å€åŸÜ ⁄©€ÅŸà ÿßÿØÿ±⁄Øÿ±ÿØ ⁄àÿß⁄©Ÿπÿ± €å€ÅŸæ€ÅŸÑÿß ŸÖ€åŸÜ⁄à⁄© ŸÖ⁄©€Å ÿ≠ÿ≥€åŸÜ€Å €ÅŸÑ ÿßŸÜÿßŸÑ€åÿ≥ÿ≤ ŸÜÿßÿ®ÿßŸÑÿ∫ ÿπÿ¨ŸÑÿ™ ÿπÿßÿ±ŸÅ ⁄©ÿß ÿ®ŸÜ€åÿßÿØÿß ŸÜÿ™€åÿ¨ÿ™ÿß ŸÅÿßÿ±ŸàŸÇÿß ŸÅÿßÿ±ŸÖŸàŸÑ€í ŸæŸà⁄Ü⁄æŸÜÿß €Åÿ±ÿ≥Ÿπ ⁄àÿß⁄©Ÿàÿ§⁄∫ ŸæŸÜŸæ ÿ¨⁄æ⁄©ÿßÿ§ ÿ¨ŸÜÿ≥ÿß ÿπÿßÿ®ÿØ€Å Ÿπÿ±ÿßŸÜÿ≤€å⁄©ÿ¥ŸÜÿ≤ ŸÅ€åŸÜ ÿ±ÿ≠ŸÖÿßŸÜ ŸæŸà⁄Ü⁄æŸÜÿß ŸÜÿßÿ¶Ÿπ ⁄Üÿß€Å€å€í ÿ¢⁄Øÿ±€Å ÿßŸÜ⁄àÿ≥Ÿπÿ±€åŸÑ ÿßŸÑ€Åÿß ÿ¨⁄æÿß⁄ë€å ŸÖŸÇÿ™ÿØÿ±€Å ŸÑ⁄ëÿ± ÿ¥ŸÅÿßŸÅ ÿ®ŸÜ⁄ØŸÑÿß ⁄Ø⁄æŸÜŸπ ÿ≥ÿ≤ÿß ÿØŸàŸÑ€Åÿß ŸÖÿ≠ŸÑ€í ÿ≤ŸÜÿØ⁄Øÿß ŸÖ€åÿ¥ ŸæŸÜ€åŸπÿß ÿßŸÖÿØŸÜÿß ÿ≥ÿ≤ÿßÿ§⁄∫ ÿ≥⁄©€åŸÜÿ± €ÅŸàŸÑ⁄àŸÜ⁄Øÿ≤ ÿ¥ÿ±ÿßÿ®€Å ŸÅÿ¥ÿßŸÜÿß ŸÖÿπÿØŸÜÿß €Åÿßÿ™⁄æ ŸÑ⁄Ü⁄© ⁄Üÿ±ÿ®€åŸÑ ÿßŸà €åŸàÿ±€åÿ¥€åŸÜ ÿ≥ÿ≥Ÿπ€åŸÜ€åÿ®ŸÑ ÿ®⁄æÿ™€åÿ¨€í ÿÆŸÅ€å€Å ÿ≥ŸÖ⁄ØŸÑÿ± ÿ¥ÿ±Ÿπ €ÅŸÜ⁄ØŸàŸÑ ŸÖŸÑÿßŸÇÿßÿ™€å €Åÿßÿ≥ŸπŸÑ ŸÜŸàÿßÿ≠ ÿ≥€åŸπÿ≥ ŸÖÿ¥⁄©ŸÑ ŸæŸàŸÑ€åŸÜ⁄à ÿØÿ±ÿ¨€Å Ÿæ€Å⁄ÜÿßŸÜÿß ŸÜ€Åÿ¨ ÿ≥ŸÜ⁄Ø⁄æÿßÿ±ÿßŸÖ€í ÿ≤ÿ¶€å <NUM>ÿØÿ≥ŸÖÿ®ÿ± ⁄©⁄©ÿ±ÿß Ÿæÿ±€åŸÖ€åÿ¶ÿ± ⁄àŸà€å⁄òŸÜŸÑ ÿ≥Ÿàÿ±⁄Ø ÿßŸà€åÿ≥ ⁄Ü€åÿ™⁄æ⁄ë€í ÿßÿ≥€åÿ≥ŸÖŸÜŸπ ⁄à⁄©€åÿ™ÿß ÿ≥Ÿàÿ¥ŸÑÿ≥Ÿπ ÿØÿßÿ™ÿß Ÿà€åŸæŸÜÿ≤ ŸÖ€åŸπÿ± ÿ≥ÿßÿ®ŸÇ€åŸÜ €ÅŸàÿ¨ÿßÿ™ÿß ÿØŸàÿ≥ÿ±€í €Åÿ±ÿ®ŸÜÿ≥ ÿ¨⁄æ⁄©ŸÜÿß ⁄ØŸÖÿ¥ÿØ⁄Øÿß ÿßŸπ⁄æŸÜÿß ÿ∏ÿßŸÑŸÖ ⁄ØŸàŸÜÿ¨ €ÅŸàÿ¶Ÿä ⁄©⁄æ€åŸæ ŸÖÿ≥ŸπŸÜ⁄àÿß ÿ®⁄àŸÜ⁄Ø ŸÜŸÖŸπÿ™€í ÿ≥€Åÿ™€í ÿß⁄Øÿ¶ ⁄àŸà€åÿ≤ŸÜ ÿ¢ÿ≥⁄©ÿ™€í ÿßÿ´ÿßÿ´ ÿßŸÜÿßŸÖ ÿßŸÑŸÖÿßÿ±ÿß ŸÅ€å⁄©⁄ÜŸàÿ¶ŸÑ ÿ¥ÿßŸÖ ÿØÿ™ÿß Ÿæ€åÿ™⁄æ⁄© Ÿæ⁄æ€åŸÑÿßÿ¶ÿß ÿ¥€Åÿ±ÿ™ Ÿæÿ±ÿßÿ¶ŸÖ ÿØÿ®€í ÿ®ÿ±ÿ™ŸÜ Ÿæ€åŸπÿ±ÿ≥ŸÜ ÿ®ÿ±€åŸπÿßŸÜ€å⁄©ÿß ÿØÿ±ŸÖ€åÿß ÿ±ŸÇ€åÿ® €Å€åŸÑÿß ÿØ⁄©ÿß ÿ¨⁄Øÿ±ÿß ŸÖŸàŸπÿ±ÿ≥ÿßÿ¶€å⁄©ŸÑ ⁄©ŸÜ⁄ØŸÜ ÿ≥ÿßÿ™⁄æ ŸÅŸàÿ¨ÿß ÿßÿÆÿ™€åÿßÿ± ÿß€åŸàŸÜŸÜ⁄Ø ŸàŸÇÿßÿ±ÿß ÿ®ÿß€Åÿ± ÿÆÿ≥ÿßÿ±€Å ÿßŸÜ⁄àÿ≥Ÿπÿ±ÿß ÿ®⁄æ⁄©ÿßÿ±ÿß ⁄©€Åÿß⁄∫ ÿ®ÿπÿ∂ ÿ¨ÿ®⁄ë€í ÿ¨Ÿàÿßÿ®ÿØ€Åÿß ÿ≥Ÿπÿ±€åŸπÿ¨ÿß ⁄©ÿßÿ±ÿ¢ŸÖÿØ Ÿæÿ±€å⁄©Ÿπ€åÿ¥ŸÜÿ±ÿ≤ €Åÿ™⁄æ⁄©ŸÜ⁄à€í ÿ®⁄æÿßÿ¥ÿß Ÿæ⁄æÿß⁄ëÿß ÿØÿ±ÿ≥ÿ™⁄Øÿß ÿ≥Ÿπ€åÿ®ŸÑ Ÿà€Åÿßÿ® ŸÖÿßÿ±ŸπŸÖ ŸÖŸÑÿ≠ŸÇ€Å ÿßÿ®ŸàÿßŸÑÿ≠ÿ≥ŸÜ ŸÖ€åŸà⁄ÜŸÑ ÿØ⁄æŸà ÿØÿ™Ÿàÿß ÿ±ÿÆÿ≥ÿßŸÜ€Å ⁄Üÿß€Å€åÿ¶€í ÿ™ÿßŸÜ⁄Ø ÿ≥ŸÜ€åÿßÿ±Ÿπÿß ÿ®ÿØÿ™ÿ±€åŸÜ ⁄©ÿßÿ±ŸæŸàÿ±€åŸπ ÿ™ÿπŸÑ€åŸÖÿß ÿπÿ≤ŸÖ ⁄à⁄æÿß⁄©ÿß ŸÖÿß€Åÿ±€Å ÿ¨⁄æŸÜ⁄à€í ⁄©ÿßÿ±ÿ≥€åŸà⁄© ÿ¨€åÿ¶ŸÜÿØ ÿ¨ÿßŸÜŸÜÿß ÿ®ÿØŸÑÿ™ÿß ÿ¢⁄Øÿß€Å ŸÖÿÆÿ™ÿµ ŸÅÿßÿ±ŸÖ €ÅÿßŸàÿ± €Åÿßÿ±Ÿπÿ≤ŸÖ ⁄Ü€Åÿ±€í ÿßŸÜŸà€åÿ≥Ÿπÿß ŸÅÿ±ÿß€ÅŸÖ Ÿæÿ±ŸàŸæ€å⁄ØŸÜ⁄à€í ⁄ÜŸà⁄ëÿß ŸÑŸæŸπ ŸàŸÜ⁄àÿ≥ÿ± ⁄àÿß⁄© ÿ®⁄Ü⁄æÿß ÿß€åŸÑÿ≥ ÿØ⁄æ⁄Ü⁄©€í ÿ∑ŸÑŸàÿπ ŸÅÿß⁄©ÿ≥ ⁄©€íÿß€å⁄© ÿ¨ÿßÿ¶ÿ≤€í ÿ¨ŸÜ€åŸàÿß ⁄Üÿßÿ¶ŸÑ⁄à ŸæŸàŸÜ ÿ¥ÿß€ÅŸàÿßŸÜÿß ⁄Ø⁄ë⁄æ€í ÿßÿ±ÿßÿ∂ÿß ÿ±Ÿàÿ∂€Å ÿ¨Ÿàÿßÿ≤ ŸÅ €Åÿ±ÿß ÿ®ÿß⁄©ÿ≥ŸÜ⁄Ø ÿØ€Åÿ¥ÿ™⁄Øÿ±ÿØÿß ÿßÿ≥Ÿπ€åŸÜÿ≤ŸÑ Ÿπ⁄æŸà⁄©ÿ± ÿ≥Ÿàÿ¥ŸÑ ÿ≥ÿ±ÿß ŸÜŸÖÿßÿ¶ŸÜÿØ⁄ØÿßŸÜ Ÿæ⁄æÿ≥ŸÑÿ™ÿß\n",
            "\n",
            "Words: 300 | Sentences: 0\n",
            "\n",
            "Trigram Article 2\n",
            "------------------------------------------------------------\n",
            "Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫ ÿ¢€åÿß ÿ™⁄æ€åŸÑ€í ŸÜŸÜ⁄Ø ÿß€åÿ¨ÿßÿ® ÿÆÿØ€åÿ¨€Å ⁄Øÿßÿ¶€å⁄à ÿÆ€åÿ≥Ÿà ÿ≥⁄©ÿ±ÿØŸà €Åÿ±⁄Øÿ≤ ŸÖ€åÿ±ÿπŸÑÿß ⁄Ü⁄æÿßÿ™ÿß ÿ≥ÿ±⁄Üÿ¥ŸÖ€Å ŸÑ€åÿß ŸÑÿß⁄ØŸà Ÿæÿ±ÿ≥ŸÜ ÿ¥ŸÇÿßŸàÿ™ ÿ¨ŸÑ€åÿ®ÿß ÿ¨ŸÜ⁄àÿßŸÜŸàÿßŸÑ€Å ŸæÿßŸÑÿ≥€å€å ŸÑÿßÿ¶ÿ®ÿ±€åÿ±ÿß ŸÇÿ≥ŸÖÿ™ ÿßÿ≠ÿØ ÿßŸÜÿßŸÑ€åÿ≥ÿ≤ ÿ≤ÿÆŸÖÿß ⁄àÿßŸÑ <NUM>⁄©ÿß Ÿà€åŸÜ⁄Üÿ±ÿ≤ ⁄Ü€åÿÆÿ™ÿß ÿßŸÖ€åŸÜ€Å ŸÇÿ≥ŸÖÿ™ÿß ÿ®ŸÑ⁄æ€í Ÿæÿßÿ±⁄© ÿ¨ÿßŸÑ ÿ±ŸÇÿ®€Å ÿß€åŸæ⁄©ÿ≥ ÿßŸÜŸà€åÿ≥Ÿπ ŸÅŸÑÿßÿ¶Ÿπ ⁄©ŸÜŸÜÿØ⁄ØÿßŸÜ ŸÖÿ∫ŸÅÿ±ÿ™ ÿ™⁄æÿß ÿ≠ÿ±ÿ® €Åÿ±ÿ¨ÿßŸÜ€Å ÿÆÿ∑Ÿàÿ∑ ⁄àÿ±ÿ™ÿß ÿ®ÿßŸÜŸà ŸÑÿ±ŸÜŸÜ⁄Ø ÿ∑ÿßŸÑÿ®€Å Ÿæÿ±ÿßÿ¶ŸÖ ŸÖÿµÿ±€åÿßŸÑ ŸÜ€åÿ≥ŸÑ€í ŸÖŸÑÿ≠ŸÇ€Å ÿ≥ÿØŸàÿ≤ÿ¶ÿß ÿ≥Ÿà⁄Üÿß ÿ™ÿ¨ÿ±ÿ® ŸÑŸæ€åŸπ ÿ≥ŸÑŸà⁄© ÿ¥Ÿà ÿ±€åŸπÿ±€åŸπ ÿß€å⁄Øÿ≤€åŸàÿ≥ ÿÆŸÅÿß ŸÜ⁄©ÿßŸÑÿß Ÿæÿ±Ÿàÿß€Å ÿ≥ÿ™⁄æÿ±€í ŸÖ€åÿØÿßŸÜ ÿ≥Ÿπ€í ÿß⁄©Ÿπ⁄æ€í ÿ≥ÿ®⁄àÿß ÿ±ÿßÿ≥ÿ™€í €åŸàŸπ€åŸàÿ®ÿ± ŸÖÿ∞ŸÖÿ™ÿß Ÿàÿßÿ±ŸÅ€åÿ¶ÿ± ÿ´ÿßŸÑÿ´ÿß ŸÜ€í ÿπÿØÿßŸÑÿ™ ÿßÿ±ÿ® ÿ¨Ÿà⁄à€åÿ¥ŸÑ ŸÇÿ±ÿßÿ±ÿØÿßÿØ ŸÜÿßŸπ€å⁄©ŸÑ ÿ™ÿ±ÿ™€åÿ® ŸÜÿßÿß€ÅŸÑÿß ŸÖŸÖÿØŸàŸπ ŸÖÿÆÿ™ÿµÿ±ÿß ŸÖÿ¥ÿ±ÿß ŸÑ€åÿ™€í ⁄Ü⁄æÿßŸÑ€åÿß ÿ¥€Åÿ±€åÿ™ ÿ®ÿ±ÿ¢ŸÖÿØÿ™ ⁄ØŸÜ€åÿ≤ ⁄©ÿ¥ŸÖ€åÿ± ⁄©Ÿæ⁄ë ÿØŸÑÿßÿ≥€Å ⁄©ÿ™€í ÿß⁄©€åÿßŸæ ŸÖÿßŸÜÿß ÿ®⁄ØŸÑ ŸÖÿµŸÜŸàÿπÿß ŸÑŸàŸæ Ÿæ€Åÿß⁄ëÿß ÿ®€åŸπ⁄æ€í ÿØ€ÅÿßŸÜÿß €ÅŸÜÿ± ÿ®Ÿàÿ±ŸÜ ÿ≥ŸÅÿßÿ±ÿ™ÿÆÿßŸÜ ÿ≥ÿ∑ÿ≠ ŸÑÿ≥ÿ®€åŸÑ€Å Ÿπ€å⁄©ÿ≥Ÿπÿßÿ¶ŸÑ ÿ≠ÿßŸÅÿ∏ ÿßŸæŸÜÿßŸÜÿß ÿ≥Ÿæÿ±ÿØ ÿØ⁄©⁄æÿßŸàÿß ÿ∫ÿ±€åÿ® ÿßÿ¶ŸÜÿØ€Å ŸÖÿπÿ∏ŸÖ ŸÖÿ™ÿµŸÑ €Å€å⁄©ÿ±ÿ≤ €ÅŸπÿß€åÿß ÿØ⁄æÿß⁄Ø ÿ≥ÿ±ŸÖ⁄Üÿßÿ± ⁄©ÿßÿ±ÿ®€åŸÜ Ÿæÿ±ŸÜÿØ€í €åŸàŸÜÿß ŸÅÿßÿ±ŸÑ€åŸÜ⁄à ŸÖŸÜÿ™ Ÿπ€å⁄©ÿ≥ÿ≤ ÿ¨ÿ±ŸÜ€åŸÑ ŸÜŸÖÿßÿ¶ŸÜÿØ€í ÿ±⁄©ŸÜÿß ÿ±Ÿà⁄à ⁄ØÿØ⁄æ€í ÿ≤ŸÜÿ¨€åÿ± Ÿπ€å⁄©ÿß ⁄à⁄æ⁄©€í ÿßŸÑÿ≠ŸÖÿØŸÑŸÑ€Å ÿØÿßÿ≥ ÿ®⁄à ŸÑ⁄ë€å ŸÖŸÜ⁄Ø€åÿ™ÿ± ŸπŸàŸπÿß ÿ±Ÿàÿßÿ®ÿ∑ ÿÆÿ∑ÿ±€Å ŸÖÿ∞ŸÖÿ™ ÿ±€å⁄à€å⁄©ŸÑÿßÿ¶€åÿ≤€åÿ¥ŸÜ ÿ®Ÿàÿ¶€í ÿ¨ÿ∞ÿ®ÿßÿ™ÿß ÿ¢ÿ¶ ÿ±€åŸæ€åÿ≥Ÿπ Ÿæ€åŸπŸÜÿß ÿ®⁄æŸπÿß ŸÑ⁄ëÿßÿ¶ÿß ⁄àÿ±ÿßÿ¶€åŸàÿ±ÿ≤ ÿ¨ŸÜ⁄ØŸÑÿß ÿÆÿ∑ ÿßŸÖÿ™€åÿßÿ≤ÿß ÿ≥Ÿàÿ≥ÿßÿ¶Ÿπ ÿ®⁄æ€åÿ¨ÿß ÿßÿ™ÿ±ÿß ⁄©ÿßŸÑÿ≤ ÿ±€å⁄©ÿßÿ±⁄àŸÜ⁄Øÿ≤ ŸÑŸæŸπ ŸÅ€å⁄©Ÿπÿ±€å ŸÜŸàÿßÿ≤ÿ¥ÿ±€åŸÅ ÿßŸÜŸπÿ±Ÿπ€åŸÜ Ÿæ€åÿØÿßŸàÿ±ÿß ⁄Ü€åÿ¶ÿ± ÿ®€íÿ±Ÿàÿ≤⁄Øÿßÿ± ÿ±€í ⁄àÿ±ÿßÿ¶€åŸàÿ±ÿ≤ ÿÆÿ®ÿ± ÿ™Ÿàÿ≠€åÿØ ŸàŸÑ€åŸÖ€í Ÿàÿ≤Ÿπÿ±ÿ≤ Ÿæÿ±ÿßÿ¶ÿ≥ŸÜ⁄Ø ÿßŸÖÿØ €Åÿßÿ¶⁄àÿ±ÿß ÿßÿ∂ÿßŸÅ€Å ŸÑÿßŸÜ⁄Üÿ±ÿ≤ ÿπÿØŸÖ ÿ≥ŸàŸÜ⁄© ⁄©€åÿ±ÿß ÿ®⁄æÿ±ÿ™ÿß ÿ¥€ÅŸÑÿß ⁄Üÿß€Åÿ™ ⁄©ŸÖÿ®⁄æ ÿ¢Ÿæÿ±€åÿ¥ŸÜÿ≤ ÿ±ÿßŸàŸÜ⁄àÿ± ÿ≥€å⁄©Ÿπÿ±ÿ≤ ÿ≥Ÿà€åŸÑ€åŸÜ ÿ¢ÿ¶€å ŸæŸÑŸπÿß ⁄©⁄æÿßŸÑ ÿ∫€åÿ±ŸÖŸÇÿßŸÖÿß ŸÜÿßŸÖ€Å ÿ≥Ÿàÿ±ÿß ⁄©ŸÑÿßŸà⁄à ŸÖŸÜ€Å Ÿà⁄©ÿßŸÑÿ™ ŸÖÿÆÿßŸÑŸÅ€åŸÜ ⁄©ÿßÿ±ÿß ⁄àÿß⁄©Ÿà ÿπÿ®ÿßÿ≥ÿß ÿßÿ±ÿßÿ¶ÿß ÿ®ÿ≠ÿ±ÿßŸÜ ŸÖÿß€Å ⁄©€åŸÖŸàŸÜ€å⁄©ÿ¥ŸÜ ŸÖ⁄ØŸàÿ¶ ÿ≥Ÿæÿ±ŸÜ⁄Øÿ≥ ŸÖÿÆÿßŸÑŸÅÿ™ ÿ¢⁄Ü⁄©€í ÿ™ÿßÿ±⁄ë ÿ¥ÿÆÿµ€åÿ™ ŸæŸàÿ±ŸπŸÑ Ÿæ€å⁄Ü⁄æŸÑ€í ÿ®ÿ±ÿßÿ≤€åŸÑ ÿØÿ≥ÿ™⁄© ŸÑÿß⁄àŸÑÿß Ÿàÿß⁄©ÿß ⁄Øÿ≥ÿ™ÿßÿÆÿß ŸÅŸàŸÑÿßÿØÿß ⁄©ŸÜŸàŸÑÿß ÿ≥€åÿØ€Å ÿßŸπ⁄æÿß€åÿß ÿ≥Ÿπÿ±€åŸπ ŸÖ€åŸπÿ±Ÿà Ÿàÿßÿ∂ÿ≠ Ÿπ€åŸÑŸÜŸπ ⁄àÿßÿ±ŸÖ€í ÿß⁄ÜŸÜ ⁄©Ÿàÿßÿ¶ŸÜÿ≤ ÿßŸà⁄Ü ÿ™ÿ±ÿßŸÜ⁄à€åÿß ŸÜ€íŸæÿß⁄©ÿ≥ÿ™ÿßŸÜ Ÿàÿ≤ŸÜ ÿßŸÜÿØ⁄æ€í ŸÅ€åŸπÿ≥ ÿ¨€åÿ≥€í ŸÅÿßÿ±ŸÜÿ±ÿ≤ ÿµŸÅÿ≠ ŸÑ€åÿ® ÿÆÿßŸÖ€å ÿßŸÑŸÇÿØ€åŸÖ ⁄Ü⁄æŸæÿßÿ¶ÿß ⁄©ÿ±ÿß⁄Üÿß ÿ≥ÿ±ÿÆ€å ŸÖÿ∫ŸÑ ÿÆÿµŸàÿµÿß ŸÜŸÖÿßÿ≤ ŸÑ⁄ØŸÜÿß Ÿàÿ∏ÿßÿ¶ŸÅ ÿ™ÿßÿ´€åÿ± ŸÖÿßŸÜŸà ÿ≤€åŸÜÿ® ÿÆÿ±⁄Ü€í ÿ±€å⁄à€å⁄©ŸÑÿßÿ¶€åÿ≤€åÿ¥ŸÜ ŸÑŸÜ⁄Øÿ± ŸÜÿ¥ÿ± ÿ∂ÿßÿ®ÿ∑ ⁄Ü⁄æŸπÿß ⁄©ÿßÿ¥ŸÅ ⁄©ÿßŸàÿ®ÿßÿ±ÿß ⁄©ŸàÿßŸÑ€åŸÅÿßÿ¶ÿß ÿß€åŸÖ€å⁄Øÿ±€åÿ¥ŸÜ Ÿπ⁄æŸà⁄©ÿ± ÿ¥ÿ±ŸÖÿß ⁄Üÿ±ÿ®€åŸÑÿß ÿ¨⁄æŸÑÿ≥€í ÿ≥ÿßÿ¶ŸÜÿ≥ ÿ™€åÿ±ÿß⁄©ÿß ÿ≥Ÿπÿ±ÿ≥ ÿ≥ÿßÿØ⁄æÿß ÿ≥ÿ±⁄©Ÿπ ŸÖÿ≠ÿ≥ŸÜ ⁄Øÿ≤ÿ¥ÿ™€Å ⁄©€íÿØŸàÿ±ÿßŸÜ ŸÖÿ™Ÿàÿßÿ≤ŸÜ ⁄©ŸàŸÑŸÖÿ®€åÿß ŸÖÿ≥⁄©ŸÑŸàÿ≥⁄©€åŸÑ€åŸπŸÑ ÿ®⁄ëÿß ÿßŸàÿ≥ÿ∑ÿß ÿ®ÿß€ÅŸÜÿß ⁄©ÿß ŸÜÿßŸÖ ÿ≥ÿ±Ÿàÿ±€í ÿπŸÖŸÑ €Å€í ⁄Øÿ≤ÿßÿ±€í ÿ±ŸàŸÜ⁄Ø€åÿß ŸÜÿßÿ±Ÿà €ÅŸàÿ¥ ÿ¢⁄©ÿ≥ÿßÿ¶€å⁄à ÿ≥€å⁄©ÿ±Ÿπÿ±€åŸπ ⁄àÿ®€Å ŸÜ⁄òÿßÿØ ÿ®ÿßŸàÿ¨ŸàÿØ ŸÖŸÜ€åŸÖŸÖ\n",
            "\n",
            "Words: 300 | Sentences: 0\n",
            "\n",
            "Trigram Article 3\n",
            "------------------------------------------------------------\n",
            "Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ŸÖ€ÅŸÜ⁄Øÿßÿ¶€å ⁄©€å ÿ¥ÿ±ÿ≠ ŸÖ€å⁄∫ ÿπÿßŸÅ€å€Å ŸæÿßŸÑ€åÿ≥€å ÿØ⁄æÿß⁄Ø ŸÜŸÖŸàÿØÿßÿ± ŸÖÿ∑€åÿπ ÿ¶€å ŸàÿßŸÇÿπÿ™ÿß ÿßŸπÿßŸÖ⁄© ÿ™ÿ±€åÿÆ ⁄ØŸàŸÑÿß ŸÜŸÇÿØ ⁄àÿßÿ¶€åŸÑ€åÿ≥ÿ≥ ÿ®€åŸπÿ±ÿß ÿ∑ÿ®€å Ÿàÿ∫€åÿ±€Å ŸÜŸàÿÆ€åÿ≤ ÿ®€Åÿßÿ¶€í ŸÅŸàÿ±ÿ™⁄æ ÿßÿπÿ™ÿ±ÿßŸÅ ÿ∫€åŸàÿ± ŸÖ€åÿ≤ ÿ¨ŸÜÿ±€åŸπÿ±ÿ≤ ÿ±ŸàÿßŸÜ⁄àÿß ŸÖÿπ€åÿ¥ÿ™ ÿ®⁄ë⁄æÿ™ÿß ÿ≥€åŸÜ⁄Øÿ± ÿ™ÿßŸÇ€åÿßŸÖÿ™ ÿ±ÿ¥Ÿàÿ™ ÿ®ÿßÿ±⁄Øÿß€Å ÿ∞ŸàÿßŸÑŸÅŸÇÿßÿ± ÿ®ÿßÿ≤ÿß ÿßŸÅÿ∫ÿßŸÜÿß ⁄ØŸÑŸà⁄Øÿßÿ± ⁄©ÿ±ÿØÿß ÿ±€åÿ¥Ÿà ÿ¥ÿßÿ±ÿ¨€Å ÿ∞ÿ±€åÿπ€í ŸÑ€åÿ≥⁄©Ÿà ÿ™Ÿà⁄ë ⁄©ÿ±ŸàÿßŸÜÿß ÿßŸÖÿØÿßÿØ ⁄©€å⁄∫ ŸÅÿßŸÑŸàÿ±ÿ≤ ÿØ€å⁄Øÿ±ÿ™ŸÖÿßŸÖ €Å€å⁄à⁄©Ÿàÿßÿ±Ÿπÿ±ÿ≤ ŸæŸÑ€åÿ≥ Ÿæ€åŸπÿß ÿ≥⁄àŸÜÿß ÿ´ŸÇÿßŸÅÿ™ÿß ÿßÿ¥ÿ±ŸÅ ŸÖ€å⁄©ÿ≥Ÿà€åŸÑ ÿ±€åŸæÿ≥Ÿπ ÿπÿ¨€åÿ® ŸÑÿßÿ∑€åŸÜÿß Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ€åÿ≤ ÿ™ÿ≠ÿ±€åÿ±ÿß ÿ¥⁄©ÿß€åÿ™ ⁄©ŸÖÿ±€í ÿ≥€í ÿßŸæÿ±€åÿ¥ŸÜ ⁄à€åŸà ŸÜŸàŸπÿ≥ ŸÖÿ∑ÿßÿ®ŸÇÿ≥ÿßŸÑ ÿ≥⁄Øÿ±€åŸπ ŸÜÿ≥€åŸÖ ⁄©ŸÖŸπŸÖŸÜŸπ Ÿà⁄©ŸÑÿßÿ° ÿ±€åÿßÿ∂ ⁄Øÿ±ÿß€åÿß ÿ∫€åÿ±ÿßÿπŸÑÿßŸÜ€å€Å €ÅŸàÿßÿ§⁄∫ ÿ¨ŸàŸÜÿ≤ ŸæŸÑÿßŸÜŸÜ⁄Ø ŸÜÿßÿ™ŸÖÿßŸÖ €Å€åŸæ€åŸÖŸàŸÜ ŸæŸÑÿß⁄© ŸÖŸÑÿß⁄©ŸÜ⁄à ÿß⁄©ÿ´ÿ± ŸÖŸÖÿßŸÜÿπÿ™ €åŸàÿ±€åŸÜ ŸÖÿ®€åŸÜ ŸÖŸÇÿßŸÖÿß ÿß€å⁄àŸÖÿ±ŸÑ ÿ™ÿØÿßÿ±⁄© ÿßÿ∑ŸÑÿßŸÇ ⁄©ÿßŸÑÿ± ⁄Øÿ±⁄à ŸÖ€åŸÖŸàÿ±€åŸÜ⁄àŸÖ ÿ¨ÿßÿ± ŸÇÿ®ŸàŸÑ€åÿ™ ŸÖÿ®ÿ™ŸÑÿß ÿ≥ŸÑŸàÿß ŸÜ⁄©ÿßÿ™ÿß ÿ¨ÿßÿ§ ÿ™€åÿ±ÿßŸæ€åŸàŸπ⁄© ÿ≥ÿ®Ÿàÿ™ÿß⁄ò ÿ≥ŸæÿßŸÜÿ≥ÿ± €åŸÇ€åŸÜŸÜÿß ŸÖÿµŸÜŸÅ€åŸÜ Ÿæ⁄©€í ÿ≥ŸÅÿß⁄©€åÿ™ Ÿàÿßÿ±ŸÜŸÜ⁄Ø ŸÅŸÜÿßŸÜÿ¥ŸÑ ⁄©ÿßŸÜŸπÿ±€å⁄©Ÿπ ⁄©ŸÑŸæÿ≥ ÿßÿ®ÿßÿ¨ÿß ÿ¥ÿßŸæŸÜ⁄Ø ÿß€å⁄©ÿ≥ÿ±ÿ≥ÿßÿ¶ÿ≤ ÿß€å€å ÿ≥ŸÜÿ™ÿß ÿØÿ™ÿß ŸÖÿ∑ÿßŸÑÿ®€í ÿ≤€å⁄à ÿßŸàŸÜ⁄Ü€í ŸÖÿ¥€ÅÿØ ŸÖ⁄©€åŸÜ€å⁄©ŸÑ ÿ∂€åÿßÿßŸÑÿ≠ŸÇ ÿßŸÜ⁄©ŸÖ ÿ®ÿßŸÑÿÆÿµŸàÿµ ŸÖÿÆÿ™ÿµÿ±ÿß ŸÜŸÇŸàŸÑ ÿß€å⁄©⁄ë ŸÖ⁄©ÿ≥ŸÜ⁄Ø ÿßÿ±⁄àÿ±ÿ≤ ÿØŸÑŸàÿßÿ¶ÿß €åÿ≠€åÿß ⁄©ÿ¥ŸÖ⁄©ÿ¥ ÿ≠ÿ≥ ÿ≥ÿ±ÿ¨ŸÜ ŸÑÿ≥Ÿπ ŸÅŸàŸÜ ÿ±€åÿßÿ∂ÿß ÿ™⁄æÿ±ŸÑ⁄©ŸÑ⁄à ÿ≠ÿµÿßÿ± ÿ®€åŸÜ⁄à Ÿàÿ±ÿØÿß Ÿπ€åŸÜŸπ ÿ®€åŸÖ ⁄à€åŸÖŸæÿßÿ±ŸπŸÖŸÜŸπŸÑ ŸÅŸàŸπ€åÿ¨ ÿßŸÜÿßŸÑ€åÿ≥ÿ≤ ÿ¢⁄Ø ÿ¥ÿØ ŸÜÿ∑ŸÜÿ≤ Ÿæÿ±ŸàŸπŸàŸÜ ÿßŸÜÿ≥ÿßŸÜ€åÿ™ ⁄©ÿ±€å ÿßÿ≠ÿ™ÿ¨ÿßÿ¨ÿß ÿ≥€åŸπ⁄æ €ÅŸÜÿ≥ ŸÜÿ¥ÿ™ ÿ®ÿØŸÜÿßŸÖÿß ÿ¢ŸÜ€å ÿßŸà⁄à€åÿ≥ÿß ÿ≤ÿ± ⁄©€í Ÿæÿ±€å⁄àÿß ÿ∂ŸÖ€åŸÖ€Å ÿ∫€åÿ±ÿ≥ÿ±⁄©ÿßÿ±ÿß ÿßÿØÿßÿ¶€å⁄Øÿß ÿ≥Ÿæÿ±Ÿàÿßÿ¶ÿ≤ÿ± ÿ™ŸÇÿ≥€åŸÖ ŸÖŸÅÿ™ ÿØ€åÿ®€Å ÿ∑ŸÑÿ®ÿß ÿ™ÿßÿ®ÿπ ÿ®ÿ¨Ÿπ ÿ∑Ÿà ⁄©ÿßŸÜÿ≥€åŸæŸπ ÿ®€å⁄©ŸÜ⁄Ø ÿßŸÜŸπÿ±⁄Ü€åŸÜÿ¨ÿ≤ ÿ≥€åŸÜ€åŸπÿ±ÿ≤ ÿÆŸÜÿ≥ÿß ŸÖŸÑÿ®€í ÿ¥ŸÜÿßÿ≥ÿßÿ¶ÿß ÿ≥ŸÑŸÅÿ± ⁄à€åŸÖ Ÿæÿ±ÿ≥ ÿ®ÿ±ÿßŸÜ⁄Ü ÿ±€åŸæŸÑ€åÿ≥ ÿ´ŸÜÿß ŸÇÿ±ÿßÿ±ÿØ€åÿß Ÿàÿßÿ≠ÿØ ⁄Üÿßÿ±€Å ⁄©⁄Ü⁄æÿß ÿ≥ŸÖÿ¨⁄æŸÜÿß ÿßŸπ⁄æÿßÿ™€í ÿ®ŸàŸÑ ÿ∞ŸÖ€Å ÿ®Ÿπ⁄æÿß ⁄ØŸÑÿßÿ® ⁄ÜŸÖ⁄ë€í Ÿàÿ≥ÿßŸÜ €ÅŸàÿßÿ¶ÿß ŸÅÿßÿ¶ÿ±ŸÜ⁄Ø ⁄©€Å<NUM> Ÿàÿ±€å⁄òŸÜÿ≤ ÿ¥ŸÖÿ¥€åÿ± ÿ®ÿØŸÖÿ≤⁄Øÿß ŸÅÿ±ŸÖ ÿ™ÿßŸÇ€åÿßŸÖÿ™ ŸàŸÑ €åŸàŸπÿ±ŸÜ ŸÖÿπŸÑŸÖ ÿØ⁄æŸàÿß⁄∫ Ÿæ€åÿßÿ≥€í ÿ¨ŸÖÿßŸÑ€å ÿ¨⁄æŸÜ⁄à€í ŸπÿßŸæÿ≥ ⁄©⁄æÿßÿ¶ÿß ŸÖÿ¨ÿ≥Ÿπÿ±€åŸπ ŸÖÿ¥ÿ™ÿ±⁄© ⁄©⁄æŸÑÿß ÿπÿßŸÇŸÑ ⁄Ø⁄æŸàŸÖ ŸÖ€åÿ≤ÿ®ÿßŸÜ ÿ®ÿ±€åÿ≥Ÿπ ÿ≥ŸÇŸÖ ŸÖŸÑÿß€åÿß ÿ±ÿ≥ŸàŸÑÿß ŸÜÿßÿ≥ÿßÿ≤⁄Øÿßÿ± ÿßÿ≤⁄©ŸÖ ⁄Üÿßÿ¶ŸÑ⁄àÿ≥ ÿßÿÆŸÑÿßŸÇÿß ÿ¥€åŸÑŸÅ ŸÖÿ≤ÿß€ÅŸÖÿ™ÿß ŸÇŸàŸÖ€åÿ™ ÿ≥ÿßŸÜ⁄Ü€í ÿ®⁄©ÿ™ÿß ÿØŸÅÿπ ÿ±ÿÆÿµÿ™ ŸÖŸÑÿßŸÇÿßÿ™€å ÿßŸÜŸπÿ± ÿ¢Ÿπ⁄æŸàÿß⁄∫ ÿ®ŸÜŸàÿßÿ™€í ÿ≥ŸπŸàÿ±€åÿ¨ ÿ¥€Åÿ±ÿ™ ⁄©€åŸπŸÑ ÿ®ÿ∫€åÿ± ÿ≥Ÿàÿßÿ±ÿß ÿØÿ±ÿßÿµŸÑ ÿ®⁄æ⁄Ø⁄àÿ± ÿßŸà⁄à€åÿ≥ÿß ÿπÿ¥ŸÇ ÿßŸÖÿ™ÿ≤ÿßÿ¨ ÿµÿπŸÜÿ™ÿß ÿ≥€å€åŸÜÿ¶ÿ± ŸÖÿπÿ±⁄©€í ŸÜ€åŸàÿ≥ ÿß€åÿ±ÿßŸÜÿß ŸÅÿ±€åÿ® ŸÅÿ±ŸÖÿßŸÜ ÿ±ÿ∂ÿß⁄©ÿßÿ± ÿ¥ÿßŸÜÿØÿßÿ± Ÿπÿ±ÿßŸÜÿ≤€å⁄©ÿ¥ŸÜ Ÿæ€Åÿ± ÿØŸàŸÑÿ™ÿßŸÜ€Å ⁄Ü⁄ë⁄æ Ÿæ⁄æ€åŸÑÿßÿ¶€í ⁄©Ÿà⁄ØŸÑ€åŸÖŸÜ ÿØŸà⁄©ÿßŸÜ ÿ¢ÿ±Ÿπ ÿØŸÜ⁄Ø ÿ≠ÿßŸÖŸÑ ⁄©ÿßÿ≥ÿ± ÿ¨€åÿ≥ ÿ≥ŸÜ€Åÿ±ÿß ŸÑ⁄©⁄æÿ™€í ÿ≥ÿ®ŸÇ ÿß€å⁄©ÿ≥Ÿæÿßÿ¶ÿ±⁄à ⁄©€åŸÜÿ≥ÿ± ÿπÿßÿ¥ÿ± ŸÖÿßŸÑÿß ⁄©€åŸÜ€å⁄àÿß ÿ±ÿßŸà€åÿ™ÿß ÿ¨ŸÖÿßÿπÿ™ ŸÅ€åŸÑ ⁄©ÿßŸæÿß ŸÖ€åÿ¨ÿ± ÿ≥Ÿæ€åÿ¥ŸÑ ÿ¨⁄ë€å Ÿæ⁄æŸàŸÑÿ™ÿß ÿ∫€åÿ±ŸÖÿπŸÖŸàŸÑÿß ⁄©€ÅŸÜ ÿ®ÿ±€åŸÅ€åŸÜ⁄Ø Ÿπ⁄©ÿ±ÿßÿ™€í ÿ±€åÿ¥ ÿßÿ®ÿ±ÿß€Å€åŸÖ ÿµÿßÿ≠ÿ® ŸÖÿπÿß€ÅÿØ€í ÿ¢⁄©ŸÑŸà⁄òŸÜ ⁄©ÿ±ÿ≥⁄©€í ⁄©ÿßÿ±ŸÜÿßÿ±ŸàŸÜ ÿ≥ŸÖÿ¨⁄æŸÜÿß\n",
            "\n",
            "Words: 300 | Sentences: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HEADLINE GENERATION**"
      ],
      "metadata": {
        "id": "IohWxi5jybbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_headline(model_type, seed_prompt, max_words=12):\n",
        "\n",
        "    seed_tokens = seed_prompt.split()\n",
        "\n",
        "    if len(seed_tokens) < 2:\n",
        "        print(\"Headline seed must contain at least 2 words.\")\n",
        "        return None\n",
        "\n",
        "    generated = seed_tokens.copy()\n",
        "\n",
        "    while len(generated) < max_words:\n",
        "\n",
        "        if model_type == \"bigram\":\n",
        "\n",
        "            last_word = generated[-1]\n",
        "\n",
        "            if last_word not in bigram_model.bigram_counts:\n",
        "                break\n",
        "\n",
        "            dist = bigram_model.get_next_word_probabilities(last_word)\n",
        "            words = list(dist.keys())\n",
        "            probs = list(dist.values())\n",
        "\n",
        "            total = sum(probs)\n",
        "            if total > 0:\n",
        "                probs = [p/total for p in probs]\n",
        "                next_word = random.choices(words, weights=probs, k=1)[0]\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        elif model_type == \"trigram\":\n",
        "\n",
        "            if len(generated) < 2:\n",
        "                break\n",
        "\n",
        "            w1, w2 = generated[-2], generated[-1]\n",
        "            next_word = sample_trigram_backoff(w1, w2)\n",
        "\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        if next_word == \"€î\":\n",
        "            break\n",
        "\n",
        "        generated.append(next_word)\n",
        "\n",
        "    return \" \".join(generated)\n",
        "\n",
        "\n",
        "\n",
        "headline_seeds = [\n",
        "    \"Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫\",\n",
        "    \"ÿ≠⁄©ŸàŸÖÿ™ ŸÜ€í\",\n",
        "    \"Ÿàÿ≤€åÿ± ÿßÿπÿ∏ŸÖ\",\n",
        "    \"ŸÖŸÑ⁄© ŸÖ€å⁄∫\",\n",
        "    \"ÿπŸàÿßŸÖ ⁄©Ÿà\"\n",
        "]\n",
        "\n",
        "for i in range(5):\n",
        "    model = \"bigram\" if i % 2 == 0 else \"trigram\"\n",
        "    headline = generate_headline(model, headline_seeds[i])\n",
        "    print(f\"{i+1}. {headline}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_c1OFJfx8qL",
        "outputId": "bf307f65-8e0e-45dd-ea60-ac5f9a0a1495"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ⁄Ü⁄æŸπÿß ŸÅÿßÿµŸÑ€Å Ÿæ€åÿ¨ÿ±ÿ≤ ⁄©⁄æŸÑŸàŸÜÿß ÿ®€ÅŸπ ŸÖŸÜÿµŸà ÿπŸÜÿßÿµÿ± ÿ®€å⁄æŸπÿ™€í ŸÖŸÖÿ®ÿ¶ÿß ÿ≥€å€ÅÿßŸÜ\n",
            "2. ÿ≠⁄©ŸàŸÖÿ™ ŸÜ€í ÿ±ÿßŸàŸÜ€Å ŸÖÿπ€åÿ≤ ⁄©ÿßŸàÿ®ÿßÿ± ŸÜ€åŸÖ ŸÑŸà⁄©€åÿ¥ŸÜÿ≤ ÿ±€å⁄àÿßŸπ ÿ™ÿ± Ÿàÿ≥ÿ∑ÿß ÿßŸÜÿ¥Ÿàÿ±⁄à ÿ®ÿ±ÿ™ŸÜ\n",
            "3. Ÿàÿ≤€åÿ± ÿßÿπÿ∏ŸÖ ⁄©⁄æ€åŸÑ ⁄à€åÿ≤€å⁄ØŸÜ€åÿ¥ŸÜ ÿ∫€åÿ±ŸÖŸÇÿßŸÖÿß ŸÇŸÖÿπ ÿØÿßÿ¶ÿ±€Å ÿßÿ∏€Åÿßÿ± ÿßÿ¨ŸÜÿ®ÿß ÿ±€åÿßÿ≥ÿ™ ŸÅÿßÿ¶ŸÜ⁄àŸÜ⁄Ø ŸÖ⁄©ŸÜ⁄à\n",
            "4. ŸÖŸÑ⁄© ŸÖ€å⁄∫ ÿ¢ŸÅÿ±ÿ≤ ÿÆÿßŸàŸÜÿØ ÿØÿßÿÆŸÑ ÿ®€ÅÿßŸÜ€Å ŸÖŸÇÿ±ÿ±€åŸÜ ÿ¨ŸàŸÅÿ±ŸÜŸπ€åÿ¶ÿ± ÿß⁄ëÿß€åÿß ÿ¨€Åÿßÿ≤⁄©€í ŸÇÿ∑ÿ±€í ÿ®ŸÑ⁄©€Å\n",
            "5. ÿπŸàÿßŸÖ ⁄©Ÿà ÿ≥ÿ≤ÿßÿ¶€í ÿ¥ÿßŸÜÿß⁄∫ ÿ≤€åÿ±ÿßŸÜÿ™ÿ∑ÿßŸÖ ÿ®ÿØÿ± ÿ®ÿßŸæŸÜÿØÿß ÿ≥ŸÖŸàÿ¶€í ÿ¨ÿπ ŸÖÿßÿ¶€å⁄©ŸÑ ŸÇŸàŸÖ€åÿ™ ŸÖÿßŸÜÿß\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iL5fF5cLeoSA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1ZhqMQJeoPE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tOzlV2VFyYt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ei0Ns93syYrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R5eWdjmtyYn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gDXz1GTXvHwE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}