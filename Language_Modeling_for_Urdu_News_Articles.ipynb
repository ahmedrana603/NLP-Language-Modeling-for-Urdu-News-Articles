{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqQmmyxTjSXowWCZ5dBJ3b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedrana603/NLP-Language-Modeling-for-Urdu-News-Articles/blob/main/Language_Modeling_for_Urdu_News_Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "LKY6QmWwxi6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n"
      ],
      "metadata": {
        "id": "f8aPS6w1xiCF"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Base URL**"
      ],
      "metadata": {
        "id": "Ib8OnIRH8ovK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.bbc.com/urdu/topics/cjgn7n9zzq7t\"\n",
        "\n",
        "article_links = set()\n",
        "raw_articles = []\n",
        "metadata_list = []"
      ],
      "metadata": {
        "id": "ItRSE0fP8oCK"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Collecting Article Links**"
      ],
      "metadata": {
        "id": "oFVHarxoxp6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for page in range(1, 50):\n",
        "    url = f\"{base_url}?page={page}\"\n",
        "    res = requests.get(url)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    for a in soup.select(\"h2 a[href*='/urdu/articles/']\"):\n",
        "        href = a[\"href\"]\n",
        "        if href.startswith(\"/\"):\n",
        "            href = \"https://www.bbc.com\" + href\n",
        "        article_links.add(href)\n",
        "\n",
        "    if len(article_links) >= 270:\n",
        "        break\n",
        "\n",
        "article_links = list(article_links)[:270]\n"
      ],
      "metadata": {
        "id": "oJSYj0Xwxh9p"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scrapping Articles**"
      ],
      "metadata": {
        "id": "j54tlC3Bx2Bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, link in enumerate(article_links, 1):\n",
        "    res = requests.get(link)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    title_tag = soup.find(\"h1\", class_=\"article-heading\")\n",
        "    title = title_tag.get_text(strip=True) if title_tag else \"No title found\"\n",
        "\n",
        "    date_tag = soup.find(\"time\")\n",
        "    date = date_tag.get_text(strip=True) if date_tag else \"No date found\"\n",
        "\n",
        "    author_tag = soup.find(\"span\", class_=\"byline__name\")\n",
        "    author = author_tag.get_text(strip=True) if author_tag else \"BBC Urdu\"\n",
        "\n",
        "    category_tag = soup.find(\"a\", class_=\"bbc-1f2hn8h e1hk9ate4\")\n",
        "    category = category_tag.get_text(strip=True) if category_tag else \"Unknown\"\n",
        "\n",
        "    body_paragraphs = []\n",
        "\n",
        "    article_tag = soup.find(\"article\")\n",
        "    if article_tag:\n",
        "        for p in article_tag.find_all(\"p\"):\n",
        "            text = p.get_text(strip=True)\n",
        "            if text.startswith(\"©\") or \"،تصویر کا ذریعہ\" in text:\n",
        "                continue\n",
        "            body_paragraphs.append(text)\n",
        "\n",
        "    if not body_paragraphs:\n",
        "        for div in soup.find_all(\"div\", class_=lambda x: x and \"RichTextComponentWrapper\" in x):\n",
        "            for p in div.find_all(\"p\"):\n",
        "                text = p.get_text(strip=True)\n",
        "                if text.startswith(\"©\") or \"،تصویر کا ذریعہ\" in text:\n",
        "                    continue\n",
        "                body_paragraphs.append(text)\n",
        "\n",
        "    if not body_paragraphs:\n",
        "        for div in soup.find_all(\"div\", {\"dir\": \"rtl\"}):\n",
        "            for p in div.find_all(\"p\"):\n",
        "                text = p.get_text(strip=True)\n",
        "                if len(text) > 5:\n",
        "                    body_paragraphs.append(text)\n",
        "\n",
        "    body = \"\\n\".join(body_paragraphs).strip()\n",
        "\n",
        "\n",
        "    raw_articles.append((idx, body))\n",
        "    metadata_list.append({\n",
        "        \"article_id\": idx,\n",
        "        \"title\": title,\n",
        "        \"url\": link,\n",
        "        \"category\": category,\n",
        "        \"date\": date,\n",
        "        \"author\": author\n",
        "    })\n",
        "\n",
        "    time.sleep(0.5)\n"
      ],
      "metadata": {
        "id": "UfVxEGFlxh6N"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Txt File**"
      ],
      "metadata": {
        "id": "XFhu3tpc_esj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"raw.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for idx, body in raw_articles:\n",
        "        f.write(f\"### Article {idx} ###\\n\")\n",
        "        f.write(body + \"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "a9hFqzvgxX2k"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metadata JSON file**"
      ],
      "metadata": {
        "id": "umpA9PGK7Oyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metadata_list, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "RSRi_2SyyvOr"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_UhTTN3kzJxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4YbXZU6-zJrH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}