{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedrana603/NLP-Language-Modeling-for-Urdu-News-Articles/blob/main/Language_Modeling_for_Urdu_News_Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 1 - BBC Urdu Dataset Collection and Preprocessing**"
      ],
      "metadata": {
        "id": "BE0chAlhB9z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "LKY6QmWwxi6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import re"
      ],
      "metadata": {
        "id": "f8aPS6w1xiCF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Base URL**"
      ],
      "metadata": {
        "id": "Ib8OnIRH8ovK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.bbc.com/urdu/topics/cjgn7n9zzq7t\"\n",
        "\n",
        "article_links = set()\n",
        "raw_articles = []\n",
        "metadata_list = []"
      ],
      "metadata": {
        "id": "ItRSE0fP8oCK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Collecting Article Links**"
      ],
      "metadata": {
        "id": "oFVHarxoxp6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for page in range(1, 50):\n",
        "    url = f\"{base_url}?page={page}\"\n",
        "    res = requests.get(url)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    for a in soup.select(\"h2 a[href*='/urdu/articles/']\"):\n",
        "        href = a[\"href\"]\n",
        "        if href.startswith(\"/\"):\n",
        "            href = \"https://www.bbc.com\" + href\n",
        "        article_links.add(href)\n",
        "\n",
        "    if len(article_links) >= 270:\n",
        "        break\n",
        "\n",
        "article_links = list(article_links)[:270]\n"
      ],
      "metadata": {
        "id": "oJSYj0Xwxh9p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scrapping Articles**"
      ],
      "metadata": {
        "id": "j54tlC3Bx2Bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, link in enumerate(article_links, 1):\n",
        "    res = requests.get(link)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    title_tag = soup.find(\"h1\", class_=\"article-heading\")\n",
        "    title = title_tag.get_text(strip=True) if title_tag else \"No title found\"\n",
        "\n",
        "    date_tag = soup.find(\"time\")\n",
        "    date = date_tag.get_text(strip=True) if date_tag else \"No date found\"\n",
        "\n",
        "    author_tag = soup.find(\"span\", class_=\"byline__name\")\n",
        "    author = author_tag.get_text(strip=True) if author_tag else \"BBC Urdu\"\n",
        "\n",
        "    category_tag = soup.find(\"a\", class_=\"bbc-1f2hn8h e1hk9ate4\")\n",
        "    category = category_tag.get_text(strip=True) if category_tag else \"Unknown\"\n",
        "\n",
        "    body_paragraphs = []\n",
        "\n",
        "    article_tag = soup.find(\"article\")\n",
        "    if article_tag:\n",
        "        for p in article_tag.find_all(\"p\"):\n",
        "            text = p.get_text(strip=True)\n",
        "            if text.startswith(\"©\") or \"،تصویر کا ذریعہ\" in text:\n",
        "                continue\n",
        "            body_paragraphs.append(text)\n",
        "\n",
        "    if not body_paragraphs:\n",
        "        for div in soup.find_all(\"div\", class_=lambda x: x and \"RichTextComponentWrapper\" in x):\n",
        "            for p in div.find_all(\"p\"):\n",
        "                text = p.get_text(strip=True)\n",
        "                if text.startswith(\"©\") or \"،تصویر کا ذریعہ\" in text:\n",
        "                    continue\n",
        "                body_paragraphs.append(text)\n",
        "\n",
        "    if not body_paragraphs:\n",
        "        for div in soup.find_all(\"div\", {\"dir\": \"rtl\"}):\n",
        "            for p in div.find_all(\"p\"):\n",
        "                text = p.get_text(strip=True)\n",
        "                if len(text) > 5:\n",
        "                    body_paragraphs.append(text)\n",
        "\n",
        "    body = \"\\n\".join(body_paragraphs).strip()\n",
        "\n",
        "\n",
        "    raw_articles.append((idx, body))\n",
        "    metadata_list.append({\n",
        "        \"article_id\": idx,\n",
        "        \"title\": title,\n",
        "        \"url\": link,\n",
        "        \"category\": category,\n",
        "        \"date\": date,\n",
        "        \"author\": author\n",
        "    })\n",
        "\n",
        "    time.sleep(0.5)\n"
      ],
      "metadata": {
        "id": "UfVxEGFlxh6N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Txt File**"
      ],
      "metadata": {
        "id": "XFhu3tpc_esj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"raw.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for idx, body in raw_articles:\n",
        "        f.write(f\"### Article {idx} ###\\n\")\n",
        "        f.write(body + \"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "a9hFqzvgxX2k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metadata JSON file**"
      ],
      "metadata": {
        "id": "umpA9PGK7Oyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metadata_list, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "RSRi_2SyyvOr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diacritics Removal**"
      ],
      "metadata": {
        "id": "mboHaibYuXvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_diacritics(text):\n",
        "    \"\"\"\n",
        "    Removes Urdu diacritics (Aarabs) from text.\n",
        "    Unicode ranges:\n",
        "    064B–065F\n",
        "    0670\n",
        "    06D6–06ED\n",
        "    \"\"\"\n",
        "    diacritics_pattern = r'[\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]'\n",
        "    return re.sub(diacritics_pattern, '', text)\n",
        "\n",
        "\n",
        "with open(\"raw.txt\", \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
        "    raw_content = f.read()\n",
        "\n",
        "\n",
        "cleaned_content = remove_diacritics(raw_content)\n",
        "\n",
        "\n",
        "with open(\"no_diacritics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(cleaned_content)\n",
        "\n",
        "\n",
        "print(\"Diacritics removed successfully.\")"
      ],
      "metadata": {
        "id": "_UhTTN3kzJxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e894b0-2623-43cd-80c0-e622383d73e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diacritics removed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Noise Removal**"
      ],
      "metadata": {
        "id": "uyYznqBXvwhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removal of Non-Urdu Text**"
      ],
      "metadata": {
        "id": "PzOz1zoH1Dsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    \"\"\"Remove URLs like http://... or www...\"\"\"\n",
        "    url_pattern = r'http\\S+|www\\S+'\n",
        "    return re.sub(url_pattern, '', text)\n",
        "\n",
        "def remove_emojis(text):\n",
        "    \"\"\"Remove emojis\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub('', text)\n",
        "\n",
        "def remove_english(text):\n",
        "    \"\"\"Remove English letters\"\"\"\n",
        "    english_pattern = r'[A-Za-z]+'\n",
        "    return re.sub(english_pattern, '', text)\n",
        "\n",
        "def remove_navigation_text(text):\n",
        "    \"\"\"Remove common web/navigation phrases\"\"\"\n",
        "    unwanted_phrases = [\n",
        "        \"مواد پر جائیں\",\n",
        "        \"سبسکرائب کرنے کے لیے کلک کریں\",\n",
        "        \"بی بی سی اردو کی خبروں اور فیچرز کو اپنے فون پر حاصل کریں\",\n",
        "        \"اپنے فون پر حاصل کریں\",\n",
        "        \"کلک کریں\"\n",
        "    ]\n",
        "    for phrase in unwanted_phrases:\n",
        "        text = text.replace(phrase, '')\n",
        "    return text\n",
        "\n",
        "def remove_noise(text):\n",
        "    \"\"\"Apply all noise removal rules\"\"\"\n",
        "    text = remove_urls(text)\n",
        "    text = remove_emojis(text)\n",
        "    text = remove_english(text)\n",
        "    text = remove_navigation_text(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_non_urdu(text):\n",
        "    \"\"\"Keep only Urdu letters, digits, spaces, Urdu punctuation\"\"\"\n",
        "    return re.sub(r'[^\\u0600-\\u06FF\\s۔؟!،0-9]', '', text)\n",
        "\n",
        "\n",
        "with open(\"no_diacritics.txt\", \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
        "    content = f.read()\n",
        "\n",
        "content = remove_noise(content)\n",
        "\n",
        "split_articles = content.split(\"### Article \")\n",
        "filtered_articles = []\n",
        "\n",
        "for part in split_articles:\n",
        "    if not part.strip():\n",
        "        continue\n",
        "\n",
        "    lines = part.split(\"\\n\", 1)\n",
        "    header_num = lines[0].strip()\n",
        "    header = f\"### Article {header_num} ###\"\n",
        "    body = lines[1] if len(lines) > 1 else \"\"\n",
        "\n",
        "    body = remove_non_urdu(body)\n",
        "\n",
        "    filtered_articles.append(header + \"\\n\" + body.strip() + \"\\n\\n\")\n",
        "\n",
        "with open(\"urdu_only_filtered.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(filtered_articles)\n",
        "\n",
        "print(\"Noise removed and non-Urdu text filtered. Article headers preserved. File ready: urdu_only_filtered.txt\")"
      ],
      "metadata": {
        "id": "4YbXZU6-zJrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89aa5a3-70de-4ce7-e34a-9977f1f94621"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noise removed and non-Urdu text filtered. Article headers preserved. File ready: urdu_only_filtered.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence Segmentation**"
      ],
      "metadata": {
        "id": "kWIcxCqw1J25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"urdu_only_filtered.txt\"\n",
        "output_file = \"segmented.txt\"\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = re.split(r'(?=### Article \\d+ ###)', content)\n",
        "\n",
        "segmented_articles = []\n",
        "\n",
        "for article in articles:\n",
        "    article = article.strip()\n",
        "    if not article:\n",
        "        continue\n",
        "\n",
        "    lines = article.split(\"\\n\", 1)\n",
        "    header = lines[0].strip()\n",
        "    body = lines[1] if len(lines) > 1 else \"\"\n",
        "\n",
        "\n",
        "    body = re.sub(r'([۔؟!])\\s*', r'\\1\\n', body)\n",
        "\n",
        "    body = re.sub(r'\\n+', '\\n', body)\n",
        "\n",
        "    body = body.strip()\n",
        "\n",
        "    segmented_articles.append(header + \"\\n\" + body + \"\\n\\n\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(segmented_articles)\n",
        "\n",
        "print(\"Sentence segmentation complete. File saved as segmented.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B8nA58p1LXC",
        "outputId": "04ca75b9-029a-4d99-e6be-9f27cda028d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence segmentation complete. File saved as segmented.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Whitespace and Formatting Normalization**"
      ],
      "metadata": {
        "id": "11c8i3bO1MY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "input_file = \"segmented.txt\"\n",
        "output_file = \"normalized.txt\"\n",
        "\n",
        "def normalize_whitespace(text):\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        line = re.sub(r'\\s+', ' ', line)\n",
        "\n",
        "        line = line.strip()\n",
        "\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    cleaned_text = '\\n'.join([l for l in cleaned_lines if l])\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = re.split(r'(?=### Article \\d+ ###)', content)\n",
        "\n",
        "normalized_articles = []\n",
        "\n",
        "for article in articles:\n",
        "    article = article.strip()\n",
        "    if not article:\n",
        "        continue\n",
        "\n",
        "    parts = article.split(\"\\n\", 1)\n",
        "    header = parts[0].strip()\n",
        "    body = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "    body = normalize_whitespace(body)\n",
        "\n",
        "    normalized_articles.append(header + \"\\n\" + body + \"\\n\\n\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(normalized_articles)\n",
        "\n",
        "print(\"Whitespace and formatting normalization complete.\")\n",
        "print(\"File saved as normalized.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdhlHnlA1Pao",
        "outputId": "a1ceb8f8-5c1c-457f-b6d3-afe21614d69b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace and formatting normalization complete.\n",
            "File saved as normalized.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Linguistic Processing**"
      ],
      "metadata": {
        "id": "pMmexb-Y9MTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "input_file = \"normalized.txt\"\n",
        "output_file = \"cleaned.txt\"\n",
        "\n",
        "\n",
        "def urdu_tokenizer(text, is_header=False):\n",
        "    \"\"\"\n",
        "    Tokenizes Urdu text:\n",
        "    - Replaces numbers with <NUM> only for body text\n",
        "    - Separates punctuation\n",
        "    \"\"\"\n",
        "    if not is_header:\n",
        "        text = re.sub(r'\\d+', '<NUM>', text)\n",
        "\n",
        "    text = re.sub(r'([۔،؟!])', r' \\1 ', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    tokens = text.split(\" \")\n",
        "    return tokens\n",
        "\n",
        "lemmatizer_map = {\n",
        "    \"ہیں\": \"ہے\",\n",
        "    \"تھیں\": \"تھا\",\n",
        "    \"گئیں\": \"گیا\",\n",
        "    \"کرتی\": \"کرتا\"\n",
        "}\n",
        "\n",
        "def urdu_lemmatizer(word):\n",
        "    \"\"\"\n",
        "    Rule-based lemmatizer for Urdu:\n",
        "    - Handles plurals (وں, یں, ات)\n",
        "    - Feminine → Masculine (ی → ا)\n",
        "    - Handles irregular forms via dictionary\n",
        "    \"\"\"\n",
        "    if word in lemmatizer_map:\n",
        "        return lemmatizer_map[word]\n",
        "\n",
        "    if word.endswith(\"وں\") and len(word) > 3:\n",
        "        return word[:-2]\n",
        "\n",
        "    if word.endswith(\"یں\") and len(word) > 3:\n",
        "        return word[:-2]\n",
        "\n",
        "    if word.endswith(\"ات\") and len(word) > 3:\n",
        "        return word[:-2]\n",
        "\n",
        "    if word.endswith(\"ی\") and len(word) > 3:\n",
        "        return word[:-1] + \"ا\"\n",
        "\n",
        "    return word\n",
        "\n",
        "\n",
        "suffixes = [\n",
        "    \"وں\", \"یں\", \"ات\", \"یاں\",\n",
        "    \"نے\", \"ہے\", \"ہوں\"\n",
        "]\n",
        "\n",
        "def urdu_stemmer(word):\n",
        "    \"\"\"\n",
        "    Light stemmer to reduce vocabulary without destroying sentence structure.\n",
        "    \"\"\"\n",
        "    for suffix in sorted(suffixes, key=len, reverse=True):\n",
        "        if word.endswith(suffix) and len(word) > len(suffix) + 1:\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = re.split(r'(?=### Article \\d+ ###)', content)\n",
        "\n",
        "processed_articles = []\n",
        "\n",
        "for article in articles:\n",
        "    article = article.strip()\n",
        "    if not article:\n",
        "        continue\n",
        "\n",
        "    parts = article.split(\"\\n\", 1)\n",
        "    header = parts[0].strip()\n",
        "    body = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "    sentences = body.split(\"\\n\")\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if not sentence:\n",
        "            continue\n",
        "\n",
        "        tokens = urdu_tokenizer(sentence, is_header=False)\n",
        "        tokens = [urdu_lemmatizer(tok) for tok in tokens]\n",
        "\n",
        "        tokens = [urdu_stemmer(tok) for tok in tokens]\n",
        "\n",
        "        processed_sentences.append(\" \".join(tokens))\n",
        "\n",
        "    processed_body = \"\\n\".join(processed_sentences)\n",
        "\n",
        "    processed_articles.append(header + \"\\n\" + processed_body + \"\\n\\n\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(processed_articles)\n",
        "\n",
        "print(\"Custom Tokenization, Lemmatization, and Light Stemming complete.\")\n",
        "print(f\"File saved as {output_file}\")\n"
      ],
      "metadata": {
        "id": "cRMIWamf5LYJ",
        "outputId": "96ca8ec6-8cf6-42fc-858c-0847d7594a8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tokenization, Lemmatization, and Light Stemming complete.\n",
            "File saved as cleaned.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2 - BBC Style Urdu News Article Generation**"
      ],
      "metadata": {
        "id": "vvnr6bm2CEwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Language Model Training**"
      ],
      "metadata": {
        "id": "_ESeGmwYCJYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "print(\"Loading preprocessed dataset from cleaned.txt...\")\n",
        "\n",
        "with open(\"cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "articles = content.split(\"### Article \")\n",
        "all_tokens = []\n",
        "\n",
        "for article in articles:\n",
        "    if not article.strip():\n",
        "        continue\n",
        "\n",
        "    lines = article.split(\"\\n\", 1)\n",
        "    if len(lines) > 1:\n",
        "        body = lines[1].strip()\n",
        "        tokens = body.split()\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "print(f\"Total tokens: {len(all_tokens)}\")\n",
        "print(f\"Vocabulary size: {len(set(all_tokens))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptwdRejqDi2N",
        "outputId": "b6325d62-c223-4cef-f38f-4be597736bec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed dataset from cleaned.txt...\n",
            "Total tokens: 427754\n",
            "Vocabulary size: 12683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UNIGRAM Model**"
      ],
      "metadata": {
        "id": "uupcIq9ADoij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnigramModel:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.unigram_counts = Counter()\n",
        "        self.total_words = 0\n",
        "        self.vocabulary = set()\n",
        "\n",
        "    def train(self, tokens):\n",
        "        self.unigram_counts = Counter(tokens)\n",
        "        self.total_words = len(tokens)\n",
        "        self.vocabulary = set(tokens)\n",
        "\n",
        "        print(\"[UNIGRAM MODEL TRAINED]\")\n",
        "        print(\"Vocabulary size:\", len(self.vocabulary))\n",
        "\n",
        "    def get_probability(self, word):\n",
        "        if self.total_words == 0:\n",
        "            return 0\n",
        "        return self.unigram_counts[word] / self.total_words\n",
        "\n",
        "    def get_most_common(self, n=10):\n",
        "        return self.unigram_counts.most_common(n)\n",
        "\n",
        "unigram_model = UnigramModel()\n",
        "unigram_model.train(all_tokens)\n",
        "\n",
        "print(\"\\nTop 10 words:\")\n",
        "(unigram_model.get_most_common(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuvHxSEkDixo",
        "outputId": "fc30415c-843c-456c-8780-90cd7ff89060"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIGRAM MODEL TRAINED]\n",
            "Vocabulary size: 12683\n",
            "\n",
            "Top 10 words:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('کے', 18513),\n",
              " ('۔', 14023),\n",
              " ('ہے', 13463),\n",
              " ('میں', 12533),\n",
              " ('کی', 11908),\n",
              " ('اور', 8424),\n",
              " ('سے', 8246),\n",
              " ('کہ', 8100),\n",
              " ('نے', 6394),\n",
              " ('کا', 6073)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIGRAM Model**"
      ],
      "metadata": {
        "id": "wwPJCurIDzt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "class BigramModel:\n",
        "\n",
        "    def __init__(self, smoothing='add-k', k=0.1):\n",
        "        self.bigram_counts = defaultdict(Counter)\n",
        "        self.unigram_counts = Counter()\n",
        "        self.vocabulary = set()\n",
        "        self.vocab_size = 0\n",
        "\n",
        "        self.smoothing = smoothing\n",
        "        self.k = 1.0 if smoothing == 'laplace' else k\n",
        "\n",
        "    def train(self, tokens):\n",
        "        tokens = ['<START>'] + tokens + ['<END>']\n",
        "\n",
        "        self.unigram_counts = Counter(tokens)\n",
        "        self.vocabulary = set(tokens)\n",
        "        self.vocab_size = len(self.vocabulary)\n",
        "\n",
        "        for i in range(len(tokens) - 1):\n",
        "            w1 = tokens[i]\n",
        "            w2 = tokens[i + 1]\n",
        "            self.bigram_counts[w1][w2] += 1\n",
        "\n",
        "    def get_probability(self, w1, w2):\n",
        "        bigram_count = self.bigram_counts[w1][w2]\n",
        "        unigram_count = self.unigram_counts[w1]\n",
        "\n",
        "        numerator = bigram_count + self.k\n",
        "        denominator = unigram_count + (self.k * self.vocab_size)\n",
        "\n",
        "        if denominator == 0:\n",
        "            return 1.0 / self.vocab_size\n",
        "\n",
        "        return numerator / denominator\n",
        "\n",
        "\n",
        "\n",
        "bigram_model = BigramModel(smoothing='add-k', k=0.1)\n",
        "bigram_model.train(all_tokens)\n",
        "\n",
        "print(\"[BIGRAM MODEL TRAINED]\")\n",
        "print(\"Vocabulary size:\", bigram_model.vocab_size)\n",
        "\n",
        "\n",
        "bigram_list = []\n",
        "\n",
        "for w1 in bigram_model.bigram_counts:\n",
        "    for w2 in bigram_model.bigram_counts[w1]:\n",
        "        count = bigram_model.bigram_counts[w1][w2]\n",
        "        bigram_list.append(((w1, w2), count))\n",
        "\n",
        "bigram_list = sorted(bigram_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 bigrams:\")\n",
        "for bigram, count in bigram_list[:10]:\n",
        "    print(f\"{bigram}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiOEz2ciDiu6",
        "outputId": "cf3e1149-f312-4d0f-efae-f6cfbd932666"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BIGRAM MODEL TRAINED]\n",
            "Vocabulary size: 12685\n",
            "\n",
            "Top 10 bigrams:\n",
            "('ہے', '۔'): 5499\n",
            "('ہے', 'کہ'): 2948\n",
            "('کے', 'لیے'): 2004\n",
            "('کے', 'مطابق'): 1347\n",
            "('انھ', 'نے'): 1269\n",
            "('تھا', 'کہ'): 1215\n",
            "('ہے', 'اور'): 1156\n",
            "('ان', 'کے'): 1149\n",
            "('تھا', '۔'): 1146\n",
            "('کے', 'بعد'): 1114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRIGRAM Model**"
      ],
      "metadata": {
        "id": "xapSrhFDEv-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "class TrigramModel:\n",
        "\n",
        "    def __init__(self, bigram_model, smoothing='add-k', k=0.1):\n",
        "        self.trigram_counts = defaultdict(lambda: defaultdict(Counter))\n",
        "        self.bigram_context_counts = defaultdict(int)\n",
        "        self.vocabulary = set()\n",
        "        self.vocab_size = 0\n",
        "        self.bigram_model = bigram_model\n",
        "\n",
        "        self.smoothing = smoothing\n",
        "        self.k = 1.0 if smoothing == 'laplace' else k\n",
        "\n",
        "    def train(self, tokens):\n",
        "        tokens = ['<START>', '<START>'] + tokens + ['<END>']\n",
        "\n",
        "        self.vocabulary = set(tokens)\n",
        "        self.vocab_size = len(self.vocabulary)\n",
        "\n",
        "        for i in range(len(tokens) - 2):\n",
        "            w1, w2, w3 = tokens[i], tokens[i+1], tokens[i+2]\n",
        "\n",
        "            self.trigram_counts[w1][w2][w3] += 1\n",
        "            self.bigram_context_counts[(w1, w2)] += 1\n",
        "\n",
        "    def get_probability(self, w1, w2, w3):\n",
        "        trigram_count = self.trigram_counts[w1][w2][w3]\n",
        "        context_count = self.bigram_context_counts[(w1, w2)]\n",
        "\n",
        "        if context_count < 2:\n",
        "            return self.bigram_model.get_probability(w2, w3)\n",
        "\n",
        "        numerator = trigram_count + self.k\n",
        "        denominator = context_count + (self.k * self.vocab_size)\n",
        "\n",
        "        if denominator == 0:\n",
        "            return self.bigram_model.get_probability(w2, w3)\n",
        "\n",
        "        return numerator / denominator\n",
        "\n",
        "\n",
        "\n",
        "trigram_model = TrigramModel(bigram_model, smoothing='add-k', k=0.1)\n",
        "trigram_model.train(all_tokens)\n",
        "\n",
        "print(\"[TRIGRAM MODEL TRAINED]\")\n",
        "print(\"Vocabulary size:\", trigram_model.vocab_size)\n",
        "\n",
        "\n",
        "trigram_list = []\n",
        "\n",
        "for w1 in trigram_model.trigram_counts:\n",
        "    for w2 in trigram_model.trigram_counts[w1]:\n",
        "        for w3 in trigram_model.trigram_counts[w1][w2]:\n",
        "            count = trigram_model.trigram_counts[w1][w2][w3]\n",
        "            trigram_list.append(((w1, w2, w3), count))\n",
        "\n",
        "trigram_list = sorted(trigram_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 trigrams:\")\n",
        "for trigram, count in trigram_list[:10]:\n",
        "    print(f\"{trigram}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOsVBNlIDish",
        "outputId": "d6fbe932-f423-4049-da8b-ea2d29021b3b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRIGRAM MODEL TRAINED]\n",
            "Vocabulary size: 12685\n",
            "\n",
            "Top 10 trigrams:\n",
            "('۔', 'انھ', 'نے'): 699\n",
            "('کہنا', 'تھا', 'کہ'): 684\n",
            "('کا', 'کہنا', 'تھا'): 647\n",
            "('کی', 'جانب', 'سے'): 572\n",
            "('بی', 'بی', 'سی'): 566\n",
            "('نے', 'کہا', 'کہ'): 529\n",
            "('کہتے', 'ہے', 'کہ'): 485\n",
            "('کے', 'بارے', 'میں'): 478\n",
            "('کہنا', 'ہے', 'کہ'): 466\n",
            "('کا', 'کہنا', 'ہے'): 454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9hHYzmM7DiqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hP-NjHnbDinI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fE28MF1BDika"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}